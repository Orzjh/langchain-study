# LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆå…«ï¼‰ï¼šç»¼åˆå®æˆ˜

## ä»‹ç»

è¯¥ç¯‡æ–‡ç« å°†ç»™å‡ºä¸€ä¸ªåˆ©ç”¨LangChainå®ç°RAGçš„ç¤ºä¾‹ç¨‹åºã€‚è¿™æ®µç¨‹åºå°†ä¼šä½“ç°å‡ºLangChainå…­ä¸ªæ ¸å¿ƒæ¨¡å—çš„åº”ç”¨ã€‚

RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ–‡æ¡£æ£€ç´¢ä¸è¯­è¨€ç”Ÿæˆçš„æŠ€æœ¯æ¡†æ¶ï¼Œèƒ½å¤Ÿè®©å¤§æ¨¡å‹åœ¨ç”Ÿæˆå›ç­”å‰å…ˆä»å¤–éƒ¨çŸ¥è¯†ä¸­æ£€ç´¢ç›¸å…³å†…å®¹ï¼Œä»è€Œæå‡å‡†ç¡®æ€§ä¸ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚

## ä»£ç 

### ç¯å¢ƒé…ç½®

```python
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé…ç½® API åœ°å€ä¸å¯†é’¥ï¼ˆä½¿ç”¨ç¬¬ä¸‰æ–¹ OpenAI é•œåƒï¼‰
import os
os.environ["OPENAI_API_BASE"] = "https://api.chatanywhere.tech"
os.environ["OPENAI_API_KEY"] = 'sk-xxx'
```

### åŠ è½½æ–‡æ¡£å¹¶ä¿å­˜è‡³å‘é‡æ•°æ®åº“ï¼Œä»¥å®ç°Retrieval

```python
# -------------------- æ–‡æ¡£åŠ è½½ä¸åˆ‡åˆ† --------------------
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader

base_dir = 'Docs'  # æ–‡æ¡£æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„
documents = []     # å­˜æ”¾åŠ è½½åçš„ Document å¯¹è±¡

# éå†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œæ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ä¸åŒçš„åŠ è½½å™¨
for file in os.listdir(base_dir):
    file_path = os.path.join(base_dir, file)
    if file.endswith('.pdf'):
        loader = PyPDFLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.docx'):
        loader = Docx2txtLoader(file_path)
        documents.extend(loader.load())
    elif file.endswith('.txt'):
        loader = TextLoader(file_path)
        documents.extend(loader.load())

# æ–‡æœ¬åˆ‡åˆ†å™¨ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªæ®µè½å—ï¼Œchunk_size ä¸ºæ¯æ®µæœ€å¤§é•¿åº¦
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)
chunked_documents = text_splitter.split_documents(documents)
```


```python
# -------------------- åµŒå…¥ + å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ --------------------
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings

# å°†åˆ‡åˆ†åçš„æ–‡æ¡£åµŒå…¥ä¸ºå‘é‡ï¼Œå¹¶ä¸´æ—¶å­˜å‚¨åœ¨ Qdrant çš„å†…å­˜æ•°æ®åº“ä¸­
vectorstore = Qdrant.from_documents(
    documents=chunked_documents,
    embedding=OpenAIEmbeddings(),
    location=":memory:",  # ä»…å­˜äºå†…å­˜ï¼Œé€‚åˆæ•™å­¦/æµ‹è¯•
    collection_name="my_documents"
)
```

### æ„å»ºChains

è¿™è¾¹æ„å»ºäº†ä¸¤æ¡é“¾ï¼Œä¸€æ¡RAGé—®ç­”é“¾ã€ä¸€æ¡æ‘˜è¦é“¾ã€‚


```python
# -------------------- æ„å»º RAG é—®ç­”é“¾ --------------------
import logging
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# æ‰“å¼€å¤šæŸ¥è¯¢æ£€ç´¢æ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
logging.basicConfig()
logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹ï¼ˆOpenAI GPT-3.5ï¼‰
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# ä½¿ç”¨ MultiQueryRetrieverï¼Œè®©æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢ï¼Œæé«˜å¬å›ç‡
retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)

# åˆå§‹åŒ–å¯¹è¯è®°å¿†
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
memory.clear()

# åˆ›å»º ConversationalRetrievalChainï¼Œå®ç° RAG é—®ç­”ï¼ˆå¸¦ä¸Šä¸‹æ–‡è®°å¿†ï¼‰
qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever_from_llm, memory=memory, verbose=True)

# æé—®ç¤ºä¾‹ï¼šé¦–è½®æé—®
result = qa_chain({"question": "ä»€ä¹ˆæ˜¯MCPï¼Ÿ"})
print(result["answer"])

# è¿½é—®ä¸Šä¸‹æ–‡é—®é¢˜ï¼šæµ‹è¯•è®°å¿†èƒ½åŠ›
print(qa_chain({"question": "å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ"})["answer"])
```


```python
# -------------------- æ–‡æ¡£æ‘˜è¦é“¾ï¼ˆMap-Reduceï¼‰ --------------------
from langchain.chains.summarize import load_summarize_chain

summ_chain = load_summarize_chain(llm=llm, chain_type="map_reduce", verbose=True)

def qa_with_summary(question: str):
    docs = retriever_from_llm.get_relevant_documents(question)
    summary = summ_chain.run(docs)
    return summary

print(qa_with_summary("è¯·ç”¨ 100 å­—æ€»ç»“ MCP çš„æ ¸å¿ƒæ¦‚å¿µ"))
```

### å®šä¹‰Agent

agentèƒ½è°ƒç”¨ä¸‰ä¸ªå·¥å…·ï¼Œé™¤äº†ä¸Šè¿°çš„ä¸¤æ¡é“¾ä¹‹å¤–è¿˜èƒ½è°ƒç”¨pythonè§£é‡Šå™¨ã€‚

```python
# -------------------- Agent å¤šå·¥å…·ç»„åˆ --------------------
from langchain.tools import Tool
from langchain_experimental.tools.python.tool import PythonREPLTool
from langchain.agents import initialize_agent, AgentType

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé€šè¿‡Retrieval QAé“¾å›ç­”ç”¨æˆ·çš„é—®é¢˜
def retriever_tool_func(q):
    return qa_chain({"question": q})["answer"]  # è°ƒç”¨qa_chainå¤„ç†é—®é¢˜ï¼Œå¹¶æå–ç­”æ¡ˆéƒ¨åˆ†

# åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æ¡£æ£€ç´¢é—®ç­”çš„å·¥å…·
retrieval_tool = Tool(name="RAG_QA", func=retriever_tool_func, description="å¯¹ä¸€èˆ¬çŸ¥è¯†ç±»é—®é¢˜ï¼Œå…ˆæ£€ç´¢æ–‡æ¡£å†å›ç­”")

# åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æ¡£æ‘˜è¦çš„å·¥å…·
summary_tool = Tool(name="RAG_Summary", func=qa_with_summary, description="å½“ç”¨æˆ·è¦æ±‚æ¦‚æ‹¬/æ€»ç»“æ—¶ä½¿ç”¨")

# åˆ›å»ºä¸€ä¸ªç”¨äºæ‰§è¡ŒPythonä»£ç çš„å·¥å…·
python_tool = PythonREPLTool()

# å°†ä¸Šè¿°å·¥å…·ç»„åˆæˆä¸€ä¸ªå·¥å…·åˆ—è¡¨ï¼Œä¾›Agentä½¿ç”¨
tools = [retrieval_tool, summary_tool, python_tool]

# åˆå§‹åŒ–ä¸€ä¸ªæ™ºèƒ½ä½“ï¼ˆAgentï¼‰ï¼Œé…ç½®å¦‚ä¸‹ï¼š
# - ä½¿ç”¨OpenAIçš„GPT-3.5æ¨¡å‹ï¼ˆllmï¼‰
# - æä¾›çš„å·¥å…·åˆ—è¡¨ï¼ˆtoolsï¼‰
# - ä½¿ç”¨ZERO_SHOT_REACT_DESCRIPTIONç±»å‹çš„Agentï¼ˆåŸºäºReActæ¡†æ¶ï¼‰
# - å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆverbose=Trueï¼‰
# - ä½¿ç”¨ä¼šè¯è®°å¿†ï¼ˆmemoryï¼‰ä»¥ä¿æŒä¸Šä¸‹æ–‡
agent = initialize_agent(
    tools=tools,  # æä¾›çš„å·¥å…·åˆ—è¡¨
    llm=llm,  # ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agentçš„ç±»å‹
    verbose=True,  # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯
    memory=memory  # ä¼šè¯è®°å¿†ï¼Œç”¨äºä¿æŒå¯¹è¯çš„ä¸Šä¸‹æ–‡
)

# ç¤ºä¾‹ï¼šè§¦å‘æ€»ç»“å·¥å…·
print(agent.run("MCP çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ"))

# ç¤ºä¾‹ï¼šè§¦å‘ Python è®¡ç®—å·¥å…·
print(agent.run("è®¡ç®— (27*13)+9 ç­‰äºå¤šå°‘ï¼Ÿ"))

# ç¤ºä¾‹ï¼šè§¦å‘æ–‡æ¡£æ£€ç´¢é—®ç­”å·¥å…·
print(agent.run("MCP å’Œ LangChain Output Parser æœ‰ä½•åŒºåˆ«ï¼Ÿ"))
```

### å¢åŠ Callbacks

```python
# -------------------- å›è°ƒæ‰“å°è¾“å‡ºäº‹ä»¶ï¼ˆå¯ç”¨äºè°ƒè¯•ã€æ‰“å­—æœºæ•ˆæœï¼‰ --------------------
from langchain_core.callbacks import BaseCallbackHandler

class SimpleCallbackHandler(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("\nğŸ”µ LLM started")

    def on_llm_new_token(self, token, **kwargs):
        print(token, end="", flush=True)

    def on_llm_end(self, response, **kwargs):
        print("\nğŸŸ¢ LLM finished")

    def on_chain_end(self, outputs, **kwargs):
        print(f"\nğŸŸ¡ Chain outputs: {list(outputs.keys())}")

callback_handler = SimpleCallbackHandler()

# Agent æ‰§è¡Œå¸¦å›è°ƒï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
agent.run("LangChain çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ", callbacks=[callback_handler])
```