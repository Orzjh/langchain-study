{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置环境变量，配置 API 地址与密钥（使用第三方 OpenAI 镜像）\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.chatanywhere.tech\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 文档加载与切分 --------------------\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "\n",
    "base_dir = 'Docs'  # 文档所在文件夹路径\n",
    "documents = []     # 存放加载后的 Document 对象\n",
    "\n",
    "# 遍历文件夹下所有文件，根据文件类型调用不同的加载器\n",
    "for file in os.listdir(base_dir):\n",
    "    file_path = os.path.join(base_dir, file)\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# 文本切分器：将长文档切分为多个段落块，chunk_size 为每段最大长度\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 嵌入 + 存储到向量数据库 --------------------\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 将切分后的文档嵌入为向量，并临时存储在 Qdrant 的内存数据库中\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    location=\":memory:\",  # 仅存于内存，适合教学/测试\n",
    "    collection_name=\"my_documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP的定义是什么？  ', 'MCP指的是什么？  ', 'MCP的全称和解释是什么？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\n",
      "\n",
      "MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n",
      "\n",
      "MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\n",
      "\n",
      "这也就意味着，如果我们要发开一个 Function Call 工具，需要对不同的模型进行适配，比如参数格式、触发逻辑、返回结构等等，这个成本是非常高的。\n",
      "\n",
      "阶段三、MCP（ Anthropic  在 2024 .11 提出）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "核心贡献：统一规范\n",
      "\n",
      "Command 对应命令，args 对应参数。对于公开的 MCP 工具，我们通常可以这样找到相应的设置方法，后面我还会介绍如何查找更多公开的 MCP 服务器。\n",
      "\n",
      "4.2.4 使用方法\n",
      "\n",
      "安装完服务器后，在对话界面点击启用 MCP 服务器。\n",
      "\n",
      "选择支持 MCP 的模型（带有扳手 🔧 标志）。\n",
      "\n",
      "就可以开始对话了！\n",
      "\n",
      "\n",
      "\n",
      "使用mcp功能\n",
      "\n",
      "\n",
      "\n",
      "选择支持mcp的模型\n",
      "\n",
      "对话效果：\n",
      "\n",
      "\n",
      "\n",
      "对话效果\n",
      "Human: 什么是MCP？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCP（Model Context Protocol，模型上下文协议）是由Anthropic于2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP的主要目的是解决当前AI模型因数据孤岛限制而无法充分发挥潜力的问题，使AI应用能够安全地访问和操作本地及远程数据，并为AI应用提供连接万物的接口。\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 什么是MCP？\n",
      "Assistant: MCP（Model Context Protocol，模型上下文协议）是由Anthropic于2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP的主要目的是解决当前AI模型因数据孤岛限制而无法充分发挥潜力的问题，使AI应用能够安全地访问和操作本地及远程数据，并为AI应用提供连接万物的接口。\n",
      "Follow Up Input: 它的使用方法是什么？\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['它是如何使用的？', '请问这个产品的具体使用步骤是什么？', '如何正确使用它？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "做什么。老人笑着回答说，他在做一只小熊玩具，准备送给自己的孙子。\\n\\n小女孩感动地看着',\n",
      "\n",
      "哈哈，看来小明的“数学”逻辑真是与众不同呀！\n",
      "Predict method took 2.9873 seconds to execute.\n",
      "有一天，小明去上学，老师问他：“小明，如果你有十个苹果，给了别人四个，你还剩几个苹果？”  \n",
      "小明想了想，回答：“老师，我还剩下六个！”  \n",
      "老师说：“不对啊，应该是六个。你为什么会那么确定？”\n",
      "\n",
      "代码调用\n",
      "\n",
      "智能体平台（如dify、阿里百炼）\n",
      "\n",
      "4.2 使用示例\n",
      "\n",
      "以CherryStudio为例子\n",
      "\n",
      "4.2.1 为什么选择 Cherry Studio？\n",
      "\n",
      "Command 对应命令，args 对应参数。对于公开的 MCP 工具，我们通常可以这样找到相应的设置方法，后面我还会介绍如何查找更多公开的 MCP 服务器。\n",
      "\n",
      "4.2.4 使用方法\n",
      "\n",
      "安装完服务器后，在对话界面点击启用 MCP 服务器。\n",
      "\n",
      "选择支持 MCP 的模型（带有扳手 🔧 标志）。\n",
      "\n",
      "就可以开始对话了！\n",
      "\n",
      "\n",
      "\n",
      "使用mcp功能\n",
      "\n",
      "\n",
      "\n",
      "选择支持mcp的模型\n",
      "\n",
      "对话效果：\n",
      "\n",
      "\n",
      "\n",
      "对话效果\n",
      "\n",
      "[1m> Finished chain.[0m\n",
      "We were just exchanging greetings and asking each other how we are. Is there something specific you would like to talk about?\n",
      "\n",
      "集成外部工具\n",
      "Human: 它的使用方法是什么？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "我不知道。\n"
     ]
    }
   ],
   "source": [
    "# 准备模型和Retrieval链\n",
    "import logging # 导入Logging工具\n",
    "from langchain_openai import ChatOpenAI # ChatOpenAI模型\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetriever工具\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# 设置Logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "# 实例化一个大模型工具 - OpenAI的GPT-3.5\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 实例化一个MultiQueryRetriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "\n",
    "# 实例化一个memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.clear()\n",
    "\n",
    "# 实例化一个ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever_from_llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['请用 100 字描述 MCP 的主要思想和原则。  ', 'MCP 的核心概念是什么？请用 100 字概括一下。  ', '能否用 100 字总结 MCP 的基本概念和关键点？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"提供哪些服务：\n",
      "\n",
      "Tools：提供给LLM应用特别是Agent使用的工具。\n",
      "\n",
      "Resoures：提供给LLM应用一些额外的结构化数据。\n",
      "\n",
      "Prompts：提供给LLM应用的一些Prompt模板。比如你的应用是一个Chatbot，可以从MCP Server中取出这些模板，让使用者选择使用。\n",
      "\n",
      "使用什么语言创建？\n",
      "\n",
      "目前支持TS/Python/Java SDK来编写MCP Server。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"这也就意味着，如果我们要发开一个 Function Call 工具，需要对不同的模型进行适配，比如参数格式、触发逻辑、返回结构等等，这个成本是非常高的。\n",
      "\n",
      "阶段三、MCP（ Anthropic  在 2024 .11 提出）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "核心贡献：统一规范\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"新的AI能力共享生态：通过MCP Server的共享，新的LLM应用可以快速获得各种工具，形成了一种新的合作体系，提高整体效用。\n",
      "\n",
      "总结：FunctionCall和MCP的区别\n",
      "\n",
      "\n",
      "\n",
      "3. 核心架构\n",
      "\n",
      "3.1 概念解析：\n",
      "\n",
      "\n",
      "\n",
      "MCP 主机（MCP Hosts）：发起请求的 LLM 应用程序（例如 Claude Desktop、IDE 或 AI 工具）。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP (Model Context Protocol), launched by Anthropic in late November 2024, is an open standard designed to unify communication between large language models (LLMs) and external data sources and tools. Its primary goal is to address data silo issues that limit AI models' potential, enabling secure access and operation of local and remote data, thereby providing a comprehensive connectivity interface for AI applications.\n",
      "\n",
      "The MCP (Model Context Protocol) research report introduces the concept of MCP, outlining its purpose and significance in managing and interpreting contextual information within models.\n",
      "\n",
      "MCP Clients are server sessions created and maintained by client LLM applications using a Client SDK. MCP Servers provide context, tools, and prompts to clients and can be deployed either locally or remotely.\n",
      "\n",
      "The MCP Server offers tools for LLM applications, particularly for agents, along with structured data resources and prompt templates for customization, such as those used in chatbots. It supports development in TypeScript, Python, and Java SDKs.\n",
      "\n",
      "为了开发一个 Function Call 工具，需要适配不同模型的参数格式、触发逻辑和返回结构，这样的成本非常高。2024年11月，Anthropic 提出了 MCP，其核心贡献是提供统一规范。\n",
      "\n",
      "The text discusses a new AI capability-sharing ecosystem facilitated by MCP Server, which allows LLM applications to quickly access various tools, thereby forming a collaborative system that enhances overall efficiency. It also outlines the distinction between FunctionCall and MCP, and introduces MCP Hosts as LLM applications (e.g., Claude Desktop, IDE, or AI tools) that initiate requests.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Launched by Anthropic in November 2024, the Model Context Protocol (MCP) is an open standard aimed at unifying communication between large language models (LLMs) and external data sources. It addresses data silo challenges, allowing secure access to local and remote data and enhancing AI applications' connectivity. The MCP framework comprises MCP Clients, which are server sessions managed by LLM applications, and MCP Servers, which provide necessary context, tools, and prompts for LLM applications. It supports development in TypeScript, Python, and Java SDKs, and facilitates a collaborative ecosystem for efficient AI capability sharing. MCP distinguishes itself from FunctionCall and introduces MCP Hosts, which are LLM applications that initiate requests.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- 文档摘要链（Map-Reduce） --------------------\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summ_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "def qa_with_summary(question: str):\n",
    "    docs = retriever_from_llm.get_relevant_documents(question)\n",
    "    summary = summ_chain.run(docs)\n",
    "    return summary\n",
    "\n",
    "print(qa_with_summary(\"请用 100 字总结 MCP 的核心概念\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我需要总结 MCP（可能指代“主控处理器”或其他含义）的作用。为此，我将检索相关文档来获取 MCP 的功能和作用方面的信息。  \n",
      "Action: RAG_Summary  \n",
      "Action Input: \"MCP 的作用\"  \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP 在什么领域中发挥作用？  ', 'MCP 是什么，其主要功能是什么？  ', 'MCP 的具体用途和应用示例有哪些？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"提供哪些服务：\n",
      "\n",
      "Tools：提供给LLM应用特别是Agent使用的工具。\n",
      "\n",
      "Resoures：提供给LLM应用一些额外的结构化数据。\n",
      "\n",
      "Prompts：提供给LLM应用的一些Prompt模板。比如你的应用是一个Chatbot，可以从MCP Server中取出这些模板，让使用者选择使用。\n",
      "\n",
      "使用什么语言创建？\n",
      "\n",
      "目前支持TS/Python/Java SDK来编写MCP Server。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The MCP (Model Context Protocol) is a framework designed to standardize the way models are contextualized and managed within various systems. It facilitates effective communication and interaction between different models by defining protocols for context sharing and management.\n",
      "\n",
      "MCP (Model Context Protocol) is an open standard introduced by Anthropic in late November 2024, designed to unify communication between large language models (LLMs) and external data sources and tools. Its primary goal is to overcome data silos that limit AI model potential, enabling secure access and operation of local and remote data, thereby providing a comprehensive interface for AI applications.\n",
      "\n",
      "MCP Clients are server sessions created and maintained by client LLM applications using a Client SDK. MCP Servers provide essential context, tools, and prompts to these clients, which can be deployed locally or remotely.\n",
      "\n",
      "The MCP Server offers tools for LLM applications, particularly for agents, along with structured data resources and prompt templates for chatbot applications. It supports development in TypeScript, Python, and Java SDK.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe Model Context Protocol (MCP) is an open standard introduced by Anthropic in November 2024 to standardize the contextualization and management of models across systems. It enhances communication between large language models (LLMs) and external data sources, aiming to eliminate data silos and facilitate secure access to local and remote data. MCP consists of client-server architecture, where MCP Clients are created by LLM applications and MCP Servers provide essential tools, context, and prompts. It supports development in TypeScript, Python, and Java SDK.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在已经知道 MCP 的作用，它是一个开放标准，旨在标准化模型的上下文管理和系统间的交互。  \n",
      "Final Answer: MCP（Model Context Protocol）是由 Anthropic 在 2024 年 11 月提出的开放标准，用于标准化模型的上下文管理，增强大型语言模型（LLM）和外部数据源之间的通信，消除数据孤岛并促进对本地和远程数据的安全访问。MCP 采用客户端-服务器架构，LLM 应用生成 MCP 客户端，而 MCP 服务器提供必要的工具、上下文和提示。它支持 TypeScript、Python 和 Java SDK 的开发。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCP（Model Context Protocol）是由 Anthropic 在 2024 年 11 月提出的开放标准，用于标准化模型的上下文管理，增强大型语言模型（LLM）和外部数据源之间的通信，消除数据孤岛并促进对本地和远程数据的安全访问。MCP 采用客户端-服务器架构，LLM 应用生成 MCP 客户端，而 MCP 服务器提供必要的工具、上下文和提示。它支持 TypeScript、Python 和 Java SDK 的开发。\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我需要计算表达式 (27*13)+9 的值。  \n",
      "Action: Python_REPL  \n",
      "Action Input: (27*13)+9  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道了计算的结果。  \n",
      "Final Answer: 360  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "360\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo address the question about the differences between MCP (Model Configuration Protocol) and LangChain Output Parser, I need to gather relevant information. I'll begin by searching for general knowledge on both MCP and LangChain Output Parser to identify their distinctions.\n",
      "\n",
      "Action: RAG_QA\n",
      "Action Input: \"MCP Model Configuration Protocol vs LangChain Output Parser difference\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 什么是MCP？\n",
      "Assistant: MCP（Model Context Protocol，模型上下文协议）是由Anthropic于2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP的主要目的是解决当前AI模型因数据孤岛限制而无法充分发挥潜力的问题，使AI应用能够安全地访问和操作本地及远程数据，并为AI应用提供连接万物的接口。\n",
      "Human: 它的使用方法是什么？\n",
      "Assistant: 我不知道。\n",
      "Human: MCP 的作用能一步总结给我吗？\n",
      "Assistant: MCP（Model Context Protocol）是由 Anthropic 在 2024 年 11 月提出的开放标准，用于标准化模型的上下文管理，增强大型语言模型（LLM）和外部数据源之间的通信，消除数据孤岛并促进对本地和远程数据的安全访问。MCP 采用客户端-服务器架构，LLM 应用生成 MCP 客户端，而 MCP 服务器提供必要的工具、上下文和提示。它支持 TypeScript、Python 和 Java SDK 的开发。\n",
      "Human: 计算 (27*13)+9 等于多少？\n",
      "Assistant: 360\n",
      "Follow Up Input: MCP Model Configuration Protocol vs LangChain Output Parser difference\"\n",
      "\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. MCP 模型配置协议与 LangChain 输出解析器之间存在哪些主要区别？', '2. 在 MCP 模型配置协议和 LangChain 输出解析器的功能上，有哪些不同之处？', '3. 可以详细说明一下 MCP 模型配置协议及 LangChain 输出解析器的不同之处吗？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\n",
      "\n",
      "Model返回的内容通常都是字符串的模式，但在实际开发过程中，往往希望model可以返回更直观的内容，Langchain提供的输出解析器则将派上用场。\n",
      "\n",
      "在实现一个输出解析器的过程中，需要实现两种方法：\n",
      "\n",
      "获取格式指令：返回一个字符串的方法，其中包含有关如何格式化语言模型输出的说明。\n",
      "\n",
      "Parse：一种接收字符串（假设是来自语言模型的响应）并将其解析为某种结构的方法。\n",
      "\n",
      "MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n",
      "\n",
      "新的AI能力共享生态：通过MCP Server的共享，新的LLM应用可以快速获得各种工具，形成了一种新的合作体系，提高整体效用。\n",
      "\n",
      "总结：FunctionCall和MCP的区别\n",
      "\n",
      "\n",
      "\n",
      "3. 核心架构\n",
      "\n",
      "3.1 概念解析：\n",
      "\n",
      "\n",
      "\n",
      "MCP 主机（MCP Hosts）：发起请求的 LLM 应用程序（例如 Claude Desktop、IDE 或 AI 工具）。\n",
      "\n",
      "这也就意味着，如果我们要发开一个 Function Call 工具，需要对不同的模型进行适配，比如参数格式、触发逻辑、返回结构等等，这个成本是非常高的。\n",
      "\n",
      "阶段三、MCP（ Anthropic  在 2024 .11 提出）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "核心贡献：统一规范\n",
      "Human: MCP 模型配置协议与 LangChain 输出解析器有什么区别？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mMCP（Model Context Protocol）和LangChain输出解析器的区别在于它们的主要目的和功能。\n",
      "\n",
      "MCP是一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。它的主要目的是解决AI模型因数据孤岛限制而无法发挥潜力的问题，使AI应用能够安全地访问和操作本地及远程数据，从而提高整体效用。\n",
      "\n",
      "而LangChain输出解析器则关注于如何格式化和解析语言模型的输出，主要提供两种功能：获取格式指令和解析模型的响应。它是为了在实际开发中使模型的返回内容更加直观和结构化。\n",
      "\n",
      "总结而言，MCP侧重于规范化通信协议，而LangChain输出解析器侧重于处理模型输出的格式和解析。\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: MCP（Model Context Protocol）和LangChain输出解析器的主要区别在于功能和目的。MCP是一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信，解决数据孤岛问题。而LangChain输出解析器则用于格式化和解析语言模型的输出，使得模型的返回内容更加直观和结构化。总结而言，MCP侧重于通信规范，而LangChain输出解析器则专注于格式化和解析输出。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCP（Model Context Protocol）和LangChain输出解析器的主要区别在于功能和目的。MCP是一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信，解决数据孤岛问题。而LangChain输出解析器则用于格式化和解析语言模型的输出，使得模型的返回内容更加直观和结构化。总结而言，MCP侧重于通信规范，而LangChain输出解析器则专注于格式化和解析输出。\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Agent 多工具组合 --------------------\n",
    "from langchain.tools import Tool\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# 定义一个函数，用于通过Retrieval QA链回答用户的问题\n",
    "def retriever_tool_func(q):\n",
    "    return qa_chain({\"question\": q})[\"answer\"]  # 调用qa_chain处理问题，并提取答案部分\n",
    "\n",
    "# 创建一个用于文档检索问答的工具\n",
    "retrieval_tool = Tool(name=\"RAG_QA\", func=retriever_tool_func, description=\"对一般知识类问题，先检索文档再回答\")\n",
    "\n",
    "# 创建一个用于文档摘要的工具\n",
    "summary_tool = Tool(name=\"RAG_Summary\", func=qa_with_summary, description=\"当用户要求概括/总结时使用\")\n",
    "\n",
    "# 创建一个用于执行Python代码的工具\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# 将上述工具组合成一个工具列表，供Agent使用\n",
    "tools = [retrieval_tool, summary_tool, python_tool]\n",
    "\n",
    "# 初始化一个智能体（Agent），配置如下：\n",
    "# - 使用OpenAI的GPT-3.5模型（llm）\n",
    "# - 提供的工具列表（tools）\n",
    "# - 使用ZERO_SHOT_REACT_DESCRIPTION类型的Agent（基于ReAct框架）\n",
    "# - 启用详细日志输出（verbose=True）\n",
    "# - 使用会话记忆（memory）以保持上下文\n",
    "agent = initialize_agent(\n",
    "    tools=tools,  # 提供的工具列表\n",
    "    llm=llm,  # 使用的语言模型\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agent的类型\n",
    "    verbose=True,  # 是否输出详细的日志信息\n",
    "    memory=memory  # 会话记忆，用于保持对话的上下文\n",
    ")\n",
    "\n",
    "# 示例：触发总结工具\n",
    "print(agent.run(\"MCP 的作用能一步总结给我吗？\"))\n",
    "\n",
    "# 示例：触发 Python 计算工具\n",
    "print(agent.run(\"计算 (27*13)+9 等于多少？\"))\n",
    "\n",
    "# 示例：触发文档检索问答工具\n",
    "print(agent.run(\"MCP 和 LangChain Output Parser 有何区别？\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'StringPromptValue' object has no attribute 'keys'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟡 Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mLangChain 是一个用于构建与大型语言模型（LLMs）交互应用程序的框架。它的主要作用是使开发者能够更容易地创建文本生成、对话代理、总结和问答等功能，通过将不同的数据源和处理模块结合起来，帮助开发者管理复杂的应用逻辑。\n",
      "\n",
      "Action: RAG_Summary  \n",
      "Action Input: \"LangChain 的作用能一步总结给我吗？\"  \u001b[0m\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['LangChain 的主要功能是什么？请简要概述一下。  ', '能否帮我概括一下 LangChain 的应用和目的？  ', '请简述一下 LangChain 的关键功能是什么？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 系列教程（一）：入门\n",
      "\n",
      "介绍\n",
      "\n",
      "LangChain 是一个用于开发由语言模型驱动的应用程序的框架。它使得应用程序能够：\n",
      "\n",
      "具有上下文感知能力：将语言模型连接到上下文来源（提示指令，少量的示例，需要回应的内容等）\n",
      "\n",
      "具有推理能力：依赖语言模型进行推理（根据提供的上下文如何回答，采取什么行动等）\n",
      "\n",
      "这个框架由几个部分组成。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 库：Python 和 JavaScript 库。包含了各种组件的接口和集成，一个基本的运行时，用于将这些组件组合成链和代理，以及现成的链和代理的实现。\n",
      "\n",
      "LangChain 模板：一系列易于部署的参考架构，用于各种任务。\n",
      "\n",
      "LangServe：一个用于将 LangChain 链部署为 REST API 的库。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 系列教程（二）：Model I/O\n",
      "\n",
      "介绍\n",
      "\n",
      "LangChain 提供的 LangChain Expression Language(LCEL) 让开发可以很方便地将多个组件连接成 AI 工作流。如下是一个简单的工作流：\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangSmith：一个开发者平台，让你可以调试、测试、评估和监控基于任何 LLM 框架构建的链，并且与 LangChain 无缝集成。 \n",
      "\n",
      "核心模块\n",
      "\n",
      "LangChain由6个module组成，分别是Model IO、Retrieval、Chains、Memory、Agents和Callbacks。\n",
      "\n",
      "Model I/O：AI应用的核心部分，其中包括输入、Model和输出。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟡 Chain outputs: ['outputs']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a framework for developing applications driven by language models, enabling context-aware capabilities and reasoning. It integrates language models with contextual sources and facilitates decision-making based on provided input. The framework consists of several components.\n",
      "\n",
      "LangChain is a library in Python and JavaScript that provides interfaces and integrations for various components, enabling users to create chains and agents. It includes ready-made implementations and templates for easy deployment across different tasks. LangServe is an additional library that allows users to deploy LangChain chains as REST APIs.\n",
      "\n",
      "The second tutorial in the LangChain series, titled \"Model I/O,\" introduces the LangChain Expression Language (LCEL), which enables developers to easily connect multiple components to create AI workflows. A simple workflow example is provided.\n",
      "\n",
      "LangSmith is a developer platform for debugging, testing, evaluating, and monitoring chains built on any LLM framework, seamlessly integrating with LangChain. LangChain consists of six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being central to AI applications, encompassing input, model, and output.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟡 Chain outputs: ['text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "🟡 Chain outputs: ['output_text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for developing language model-driven applications with context-aware capabilities and reasoning. It supports Python and JavaScript, offering interfaces to create chains and agents, along with templates for easy deployment. LangServe extends LangChain by enabling the deployment of chains as REST APIs. The \"Model I/O\" tutorial introduces the LangChain Expression Language (LCEL) for connecting components in AI workflows. Additionally, LangSmith is a platform for debugging and evaluating chains. LangChain's architecture comprises six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being central to AI functionality.\u001b[0m\n",
      "Thought:\n",
      "🔵 LLM started\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟡 Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: LangChain 是一个用于开发与大型语言模型（LLMs）互动的框架，支持创建上下文感知能力和推理的应用程序。它为 Python 和 JavaScript 提供接口，允许用户创建链和代理，并具备便于部署的模板。LangServe 可以将链部署为 REST API。LangChain 的架构包含六个核心模块：模型输入输出、检索、链、内存、代理和回调，其中心模块是模型输入输出，支持 AI 功能的实现。\u001b[0m\n",
      "\n",
      "🟡 Chain outputs: ['output']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain 是一个用于开发与大型语言模型（LLMs）互动的框架，支持创建上下文感知能力和推理的应用程序。它为 Python 和 JavaScript 提供接口，允许用户创建链和代理，并具备便于部署的模板。LangServe 可以将链部署为 REST API。LangChain 的架构包含六个核心模块：模型输入输出、检索、链、内存、代理和回调，其中心模块是模型输入输出，支持 AI 功能的实现。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- 回调打印输出事件（可用于调试、打字机效果） --------------------\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class SimpleCallbackHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(\"\\n🔵 LLM started\")\n",
    "\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"\\n🟢 LLM finished\")\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        print(f\"\\n🟡 Chain outputs: {list(outputs.keys())}\")\n",
    "\n",
    "callback_handler = SimpleCallbackHandler()\n",
    "\n",
    "# Agent 执行带回调（打字机效果）\n",
    "agent.run(\"LangChain 的作用能一步总结给我吗？\", callbacks=[callback_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'StringPromptValue' object has no attribute 'keys'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mLangChain 是一个用于构建基于语言模型的应用程序的框架。它提供了一系列工具和组件，帮助开发者更容易地集成和使用语言模型，支持多种功能，如文本生成、问答、对话系统等。为了更好地总结，我将使用 RAG_Summary 工具来获取更详细的信息。\n",
      "\n",
      "Action: RAG_Summary\n",
      "Action Input: \"LangChain 的作用\"\u001b[0m\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['LangChain 的功能是什么？  ', 'LangChain 可以做些什么？  ', 'LangChain 的主要用途有哪些？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 系列教程（一）：入门\n",
      "\n",
      "介绍\n",
      "\n",
      "LangChain 是一个用于开发由语言模型驱动的应用程序的框架。它使得应用程序能够：\n",
      "\n",
      "具有上下文感知能力：将语言模型连接到上下文来源（提示指令，少量的示例，需要回应的内容等）\n",
      "\n",
      "具有推理能力：依赖语言模型进行推理（根据提供的上下文如何回答，采取什么行动等）\n",
      "\n",
      "这个框架由几个部分组成。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 库：Python 和 JavaScript 库。包含了各种组件的接口和集成，一个基本的运行时，用于将这些组件组合成链和代理，以及现成的链和代理的实现。\n",
      "\n",
      "LangChain 模板：一系列易于部署的参考架构，用于各种任务。\n",
      "\n",
      "LangServe：一个用于将 LangChain 链部署为 REST API 的库。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 系列教程（二）：Model I/O\n",
      "\n",
      "介绍\n",
      "\n",
      "LangChain 提供的 LangChain Expression Language(LCEL) 让开发可以很方便地将多个组件连接成 AI 工作流。如下是一个简单的工作流：\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangSmith：一个开发者平台，让你可以调试、测试、评估和监控基于任何 LLM 框架构建的链，并且与 LangChain 无缝集成。 \n",
      "\n",
      "核心模块\n",
      "\n",
      "LangChain由6个module组成，分别是Model IO、Retrieval、Chains、Memory、Agents和Callbacks。\n",
      "\n",
      "Model I/O：AI应用的核心部分，其中包括输入、Model和输出。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Chains：虽然通过Model IO、Retrieval和Memory这三大模块可以初步完成应用搭建，但是若想实现一个强大且复杂的应用，还是需要将各模块组合起来，这时就可以利用Chains将其连接起来，从而丰富功能。\n",
      "\n",
      "Agents：它可以通过用户的输入，理解用户的意图，返回一个特定的动作类型和参数，从而自主调用相关的工具来满足用户的需求，将应用更加智能化。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['outputs']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a framework designed for developing applications powered by language models. It enables applications to have context awareness by connecting language models to various context sources and to perform reasoning based on the provided context. The framework consists of several components.\n",
      "\n",
      "LangChain is a library for Python and JavaScript that provides interfaces and integrations for various components, a runtime for combining them into chains and agents, and ready-made implementations. It includes templates for easily deployable reference architectures for different tasks and LangServe, a library for deploying LangChain chains as REST APIs.\n",
      "\n",
      "The second tutorial in the LangChain series, titled \"Model I/O,\" introduces the LangChain Expression Language (LCEL), which enables developers to easily connect multiple components to create AI workflows. A simple workflow example is provided.\n",
      "\n",
      "LangSmith is a developer platform that enables debugging, testing, evaluation, and monitoring of chains built on any LLM framework, seamlessly integrating with LangChain. LangChain consists of six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being the central component that handles input, models, and output for AI applications.\n",
      "\n",
      "Chains enable the integration of Model IO, Retrieval, and Memory modules to create complex applications with enhanced functionality. Agents enhance this by interpreting user input to autonomously execute specific actions and utilize relevant tools, making applications more intelligent.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "🟡 Chain outputs: ['output_text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for developing applications powered by language models, allowing for context awareness and reasoning through various components. It offers a library for Python and JavaScript, enabling integration and deployment of AI workflows via LangServe as REST APIs. The second tutorial introduces the LangChain Expression Language (LCEL) for connecting components. LangSmith is a platform for debugging and monitoring chains built on any LLM framework. LangChain comprises six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Chains integrating these modules to create complex applications, while Agents enhance functionality by autonomously executing actions based on user input.\u001b[0m\n",
      "Thought:\n",
      "🔵 LLM started\n",
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: LangChain 是一个用于开发基于语言模型的应用程序的框架，提供了多种组件和工具，支持上下文感知和推理。它包括六个核心模块：模型输入输出、检索、链、记忆、代理和回调，帮助开发者集成和部署 AI 工作流，并通过 LangServe 提供 REST API。\u001b[0m\n",
      "🟡 Chain outputs: ['output']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain 是一个用于开发基于语言模型的应用程序的框架，提供了多种组件和工具，支持上下文感知和推理。它包括六个核心模块：模型输入输出、检索、链、记忆、代理和回调，帮助开发者集成和部署 AI 工作流，并通过 LangServe 提供 REST API。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_handler = SimpleCallbackHandler()\n",
    "\n",
    "# callbacks 可以同时传入多个 handler 的列表\n",
    "agent.run(\"LangChain 的作用能一步总结给我吗？\", callbacks=[callback_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
