{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.chatanywhere.tech\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-mvzdD655Ttsdn1C7IFKvdPN3XKtY5QI64ScWtiYmM4Le0K7i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Document Loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "\n",
    "# 加载Documents\n",
    "base_dir = 'Docs' # 文档的存放目录\n",
    "documents = []\n",
    "for file in os.listdir(base_dir): \n",
    "    # 构建完整的文件路径\n",
    "    file_path = os.path.join(base_dir, file)\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# 将Documents切分成块以便后续进行嵌入和向量存储\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 将分割嵌入并存储在矢量数据库Qdrant中\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=chunked_documents, # 以分块的文档\n",
    "    embedding=OpenAIEmbeddings(), # 用OpenAI的Embedding Model做嵌入\n",
    "    location=\":memory:\",  # in-memory 存储\n",
    "    collection_name=\"my_documents\" # 指定collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/3197553982.py:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# 准备模型和Retrieval链\n",
    "import logging # 导入Logging工具\n",
    "from langchain_openai import ChatOpenAI # ChatOpenAI模型\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetriever工具\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# 设置Logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "# 实例化一个大模型工具 - OpenAI的GPT-3.5\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 实例化一个MultiQueryRetriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "\n",
    "# 实例化一个memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.clear()\n",
    "\n",
    "# 实例化一个ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever_from_llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/688091923.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": \"什么是MCP？\"})\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP是什么？  ', 'MCP的定义和概念是什么？  ', 'MCP具体指的是什么内容？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\n",
      "\n",
      "MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n",
      "\n",
      "MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\n",
      "\n",
      "提供哪些服务：\n",
      "\n",
      "Tools：提供给LLM应用特别是Agent使用的工具。\n",
      "\n",
      "Resoures：提供给LLM应用一些额外的结构化数据。\n",
      "\n",
      "Prompts：提供给LLM应用的一些Prompt模板。比如你的应用是一个Chatbot，可以从MCP Server中取出这些模板，让使用者选择使用。\n",
      "\n",
      "使用什么语言创建？\n",
      "\n",
      "目前支持TS/Python/Java SDK来编写MCP Server。\n",
      "\n",
      "这也就意味着，如果我们要发开一个 Function Call 工具，需要对不同的模型进行适配，比如参数格式、触发逻辑、返回结构等等，这个成本是非常高的。\n",
      "\n",
      "阶段三、MCP（ Anthropic  在 2024 .11 提出）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "核心贡献：统一规范\n",
      "Human: 什么是MCP？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': '什么是MCP？', 'chat_history': [HumanMessage(content='什么是MCP？', additional_kwargs={}, response_metadata={}), AIMessage(content='MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。', additional_kwargs={}, response_metadata={})], 'answer': 'MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。'}\n",
      "MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"question\": \"什么是MCP？\"})\n",
    "print(result)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 什么是MCP？\n",
      "Assistant: MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。\n",
      "Follow Up Input: 它的使用方法是什么？\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP的具体使用步骤有哪些？  ', '如何正确使用MCP？  ', 'MCP的操作指南是什么？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Command 对应命令，args 对应参数。对于公开的 MCP 工具，我们通常可以这样找到相应的设置方法，后面我还会介绍如何查找更多公开的 MCP 服务器。\n",
      "\n",
      "4.2.4 使用方法\n",
      "\n",
      "安装完服务器后，在对话界面点击启用 MCP 服务器。\n",
      "\n",
      "选择支持 MCP 的模型（带有扳手 🔧 标志）。\n",
      "\n",
      "就可以开始对话了！\n",
      "\n",
      "\n",
      "\n",
      "使用mcp功能\n",
      "\n",
      "\n",
      "\n",
      "选择支持mcp的模型\n",
      "\n",
      "对话效果：\n",
      "\n",
      "\n",
      "\n",
      "对话效果\n",
      "\n",
      "MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\n",
      "\n",
      "MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\n",
      "\n",
      "MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n",
      "\n",
      "工具操作文件夹，实现通过对话控制桌面文件。通过这些示例，旨在帮助用户了解 MCP 强大的扩展性。\n",
      "Human: MCP的使用方法是什么？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': '它的使用方法是什么？', 'chat_history': [HumanMessage(content='什么是MCP？', additional_kwargs={}, response_metadata={}), AIMessage(content='MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。', additional_kwargs={}, response_metadata={}), HumanMessage(content='它的使用方法是什么？', additional_kwargs={}, response_metadata={}), AIMessage(content='MCP的使用方法如下：\\n\\n1. 安装完服务器后，在对话界面点击启用MCP服务器。\\n2. 选择支持MCP的模型（带有扳手 🔧 标志）。\\n3. 就可以开始对话了！ \\n\\n通过这些步骤，您可以使用MCP功能与支持的模型进行交互。', additional_kwargs={}, response_metadata={})], 'answer': 'MCP的使用方法如下：\\n\\n1. 安装完服务器后，在对话界面点击启用MCP服务器。\\n2. 选择支持MCP的模型（带有扳手 🔧 标志）。\\n3. 就可以开始对话了！ \\n\\n通过这些步骤，您可以使用MCP功能与支持的模型进行交互。'}\n",
      "{'question': '什么是MCP？', 'chat_history': [HumanMessage(content='什么是MCP？', additional_kwargs={}, response_metadata={}), AIMessage(content='MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。', additional_kwargs={}, response_metadata={}), HumanMessage(content='它的使用方法是什么？', additional_kwargs={}, response_metadata={}), AIMessage(content='MCP的使用方法如下：\\n\\n1. 安装完服务器后，在对话界面点击启用MCP服务器。\\n2. 选择支持MCP的模型（带有扳手 🔧 标志）。\\n3. 就可以开始对话了！ \\n\\n通过这些步骤，您可以使用MCP功能与支持的模型进行交互。', additional_kwargs={}, response_metadata={})], 'answer': 'MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。'}\n",
      "MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。\n"
     ]
    }
   ],
   "source": [
    "# 具有记忆功能，根据上下文推断“它”为MCP\n",
    "print(qa_chain({\"question\": \"它的使用方法是什么？\"}))\n",
    "print(result)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/2417023693.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever_from_llm.get_relevant_documents(question)\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['请简要概述 MCP 的主要思想，控制在 100 字以内。  ', '能否用 100 字解释一下 MCP 的基本概念？  ', '请用不超过 100 字的方式描述 MCP 的核心内容。']\n",
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/2417023693.py:12: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = summ_chain.run(docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"工具操作文件夹，实现通过对话控制桌面文件。通过这些示例，旨在帮助用户了解 MCP 强大的扩展性。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"提供哪些服务：\n",
      "\n",
      "Tools：提供给LLM应用特别是Agent使用的工具。\n",
      "\n",
      "Resoures：提供给LLM应用一些额外的结构化数据。\n",
      "\n",
      "Prompts：提供给LLM应用的一些Prompt模板。比如你的应用是一个Chatbot，可以从MCP Server中取出这些模板，让使用者选择使用。\n",
      "\n",
      "使用什么语言创建？\n",
      "\n",
      "目前支持TS/Python/Java SDK来编写MCP Server。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"就像是一个 “通用插头” 或者 “USB 接口”，制定了统一的规范，不管是连接数据库、第三方 API，还是本地文件等各种外部资源，都可以通过这个 “通用接口” 来完成，让 AI 模型与外部工具或数据源之间的交互更加标准化、可复用。\n",
      "\n",
      "带来的优势：\n",
      "\n",
      "LLM应用的简化：不用适配各种私有协议，你只需要知道怎么连接MCP server\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP (Model Context Protocol) is an open standard launched by Anthropic in late November 2024, designed to unify communication between large language models (LLMs) and external data sources and tools. Its primary goal is to address the limitations of data silos that hinder AI models' potential, enabling secure access and operation of local and remote data, thereby providing a comprehensive interface for AI applications.\n",
      "\n",
      "The document demonstrates how to use a tool to manage desktop files through dialogue, showcasing the powerful extensibility of MCP.\n",
      "\n",
      "The MCP (Model Context Protocol) is a framework designed to facilitate the interaction and communication between different models in a system, ensuring that they operate effectively within their respective contexts.\n",
      "\n",
      "The service offers tools for LLM applications, structured data resources, and prompt templates for applications like chatbots. It supports development in TypeScript, Python, and Java SDK for creating the MCP Server.\n",
      "\n",
      "MCP Clients are server sessions created and maintained by client LLM applications using a Client SDK. MCP Servers provide context, tools, and prompts to clients and can be deployed locally or remotely.\n",
      "\n",
      "The concept of a \"universal interface,\" akin to a \"universal plug\" or \"USB port,\" establishes standardized protocols for connecting various external resources like databases, APIs, and local files. This standardization simplifies interactions between AI models and external tools, enhancing reusability and reducing the need to adapt to different proprietary protocols, as users only need to know how to connect to the MCP server.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Model Context Protocol (MCP), launched by Anthropic in November 2024, is an open standard aimed at improving communication between large language models (LLMs) and external data sources. It addresses data silos by enabling secure access to local and remote data, providing a unified interface for AI applications. MCP facilitates interaction among models, offering tools for LLM applications, structured data resources, and prompt templates, with support for TypeScript, Python, and Java. MCP Clients maintain server sessions, while MCP Servers provide context and tools, deployable locally or remotely. The protocol establishes a \"universal interface\" for connecting various external resources, simplifying interactions and enhancing reusability across AI models.\n"
     ]
    }
   ],
   "source": [
    "# 创建 Map-Reduce 摘要链\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# 使用 OpenAI 的 GPT-3.5 模型实例化摘要链，采用 map_reduce 策略\n",
    "summ_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# 定义一个函数，接收用户问题，检索相关文档并生成摘要\n",
    "def qa_with_summary(question: str):\n",
    "    # 使用多查询检索器获取与问题相关的文档\n",
    "    docs = retriever_from_llm.get_relevant_documents(question)\n",
    "    # 对检索到的文档进行摘要\n",
    "    summary = summ_chain.run(docs)\n",
    "    return summary\n",
    "\n",
    "# 示例：生成关于 MCP 核心概念的摘要\n",
    "print(qa_with_summary(\"请用 100 字总结 MCP 的核心概念\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/431357988.py:28: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的工具类和Agent初始化方法\n",
    "from langchain.tools import Tool\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# 定义一个函数，用于通过Retrieval QA链回答用户的问题\n",
    "def retriever_tool_func(q):\n",
    "    return qa_chain({\"question\": q})[\"answer\"]  # 调用qa_chain处理问题，并提取答案部分\n",
    "\n",
    "# 创建一个用于文档检索问答的工具\n",
    "retrieval_tool = Tool(name=\"RAG_QA\", func=retriever_tool_func, description=\"对一般知识类问题，先检索文档再回答\")\n",
    "\n",
    "# 创建一个用于文档摘要的工具\n",
    "summary_tool = Tool(name=\"RAG_Summary\", func=qa_with_summary, description=\"当用户要求概括/总结时使用\")\n",
    "\n",
    "# 创建一个用于执行Python代码的工具\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# 将上述工具组合成一个工具列表，供Agent使用\n",
    "tools = [retrieval_tool, summary_tool, python_tool]\n",
    "\n",
    "# 初始化一个智能体（Agent），配置如下：\n",
    "# - 使用OpenAI的GPT-3.5模型（llm）\n",
    "# - 提供的工具列表（tools）\n",
    "# - 使用ZERO_SHOT_REACT_DESCRIPTION类型的Agent（基于ReAct框架）\n",
    "# - 启用详细日志输出（verbose=True）\n",
    "# - 使用会话记忆（memory）以保持上下文\n",
    "agent = initialize_agent(\n",
    "    tools=tools,  # 提供的工具列表\n",
    "    llm=llm,  # 使用的语言模型\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agent的类型\n",
    "    verbose=True,  # 是否输出详细的日志信息\n",
    "    memory=memory  # 会话记忆，用于保持对话的上下文\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mMCP 可能指代多种事物，但通常在技术和计算机科学中，它指的是“微控制器平台”（Microcontroller Platform）或“多通道处理”（Multi-Channel Processing）。为了准确总结其作用，我需要确认具体的上下文。由于问题没有提供具体的背景，我将进行文档检索以获取更准确的信息。\n",
      "\n",
      "Action: RAG_QA\n",
      "Action Input: \"MCP 的作用\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 什么是MCP？\n",
      "Assistant: MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。\n",
      "Human: 它的使用方法是什么？\n",
      "Assistant: MCP的使用方法如下：\n",
      "\n",
      "1. 安装完服务器后，在对话界面点击启用MCP服务器。\n",
      "2. 选择支持MCP的模型（带有扳手 🔧 标志）。\n",
      "3. 就可以开始对话了！ \n",
      "\n",
      "通过这些步骤，您可以使用MCP功能与支持的模型进行交互。\n",
      "Follow Up Input: MCP 的作用\"\n",
      "\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP 的功能是什么？  ', 'MCP 在什么方面发挥作用？  ', 'MCP 的主要用途是什么？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\n",
      "\n",
      "MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n",
      "\n",
      "MCP 客户端（MCP Clients）：由客户端LLM应用使用Client SDK创建并维护的一个Server会话。\n",
      "\n",
      "MCP 服务器（MCP Servers）：为Client  提供上下文、工具和 prompt 等信息的服务。可以部署在LLM应用本机，也可以远程部署（Remote）。\n",
      "\n",
      "\n",
      "\n",
      "3.2 工作流程：\n",
      "\n",
      "[该类型的内容暂不支持下载]\n",
      "\n",
      "MCP 服务器（MCP Servers）\n",
      "\n",
      "Command 对应命令，args 对应参数。对于公开的 MCP 工具，我们通常可以这样找到相应的设置方法，后面我还会介绍如何查找更多公开的 MCP 服务器。\n",
      "\n",
      "4.2.4 使用方法\n",
      "\n",
      "安装完服务器后，在对话界面点击启用 MCP 服务器。\n",
      "\n",
      "选择支持 MCP 的模型（带有扳手 🔧 标志）。\n",
      "\n",
      "就可以开始对话了！\n",
      "\n",
      "\n",
      "\n",
      "使用mcp功能\n",
      "\n",
      "\n",
      "\n",
      "选择支持mcp的模型\n",
      "\n",
      "对话效果：\n",
      "\n",
      "\n",
      "\n",
      "对话效果\n",
      "\n",
      "这也就意味着，如果我们要发开一个 Function Call 工具，需要对不同的模型进行适配，比如参数格式、触发逻辑、返回结构等等，这个成本是非常高的。\n",
      "\n",
      "阶段三、MCP（ Anthropic  在 2024 .11 提出）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "核心贡献：统一规范\n",
      "\n",
      "提供哪些服务：\n",
      "\n",
      "Tools：提供给LLM应用特别是Agent使用的工具。\n",
      "\n",
      "Resoures：提供给LLM应用一些额外的结构化数据。\n",
      "\n",
      "Prompts：提供给LLM应用的一些Prompt模板。比如你的应用是一个Chatbot，可以从MCP Server中取出这些模板，让使用者选择使用。\n",
      "\n",
      "使用什么语言创建？\n",
      "\n",
      "目前支持TS/Python/Java SDK来编写MCP Server。\n",
      "Human: MCP 的作用是什么？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mMCP（Model Context Protocol）的作用是统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。它旨在解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的问题，使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供连接万物的接口。具体来说，MCP 提供了工具、结构化数据和提示模板等服务，帮助 LLM 应用更好地进行交互和处理信息。\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道了 MCP 的作用，它是一个统一的通信协议，旨在增强大型语言模型与外部数据源和工具之间的交互能力。 \n",
      "\n",
      "Final Answer: MCP（Model Context Protocol）的作用是统一大型语言模型（LLM）与外部数据源和工具之间的通信协议，解决数据孤岛问题，使 AI 应用能够安全地访问和操作本地及远程数据。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCP（Model Context Protocol）的作用是统一大型语言模型（LLM）与外部数据源和工具之间的通信协议，解决数据孤岛问题，使 AI 应用能够安全地访问和操作本地及远程数据。\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(\"MCP 的作用能一步总结给我吗？\"))    # ➜ 触发 RAG_Summary 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m我需要计算表达式 (27*13)+9 的值。  \n",
      "Action: Python_REPL  \n",
      "Action Input: (27 * 13) + 9  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道了最终答案。  \n",
      "Final Answer: 360\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(\"计算 (27*13)+9 等于多少？\"))   # ➜ 触发 Python 解释器工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我需要查找有关 MCP 和 LangChain Output Parser 的信息，以便了解它们之间的区别。  \n",
      "Action: RAG_QA  \n",
      "Action Input: \"MCP 和 LangChain Output Parser 有何区别？\"  \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: 什么是MCP？\n",
      "Assistant: MCP是Model Context Protocol（模型上下文协议）的简称，是由Anthropic在2024年11月底推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。其主要目的在于解决当前AI模型由于数据孤岛限制而无法充分发挥潜力的难题，使得AI应用能够安全地访问和操作本地及远程数据，为AI应用提供连接各种资源的接口。\n",
      "Human: 它的使用方法是什么？\n",
      "Assistant: MCP的使用方法如下：\n",
      "\n",
      "1. 安装完服务器后，在对话界面点击启用MCP服务器。\n",
      "2. 选择支持MCP的模型（带有扳手 🔧 标志）。\n",
      "3. 就可以开始对话了！ \n",
      "\n",
      "通过这些步骤，您可以使用MCP功能与支持的模型进行交互。\n",
      "Human: MCP 的作用\"\n",
      "\n",
      "Assistant: MCP（Model Context Protocol）的作用是统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。它旨在解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的问题，使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供连接万物的接口。具体来说，MCP 提供了工具、结构化数据和提示模板等服务，帮助 LLM 应用更好地进行交互和处理信息。\n",
      "Human: MCP 的作用能一步总结给我吗？\n",
      "Assistant: MCP（Model Context Protocol）的作用是统一大型语言模型（LLM）与外部数据源和工具之间的通信协议，解决数据孤岛问题，使 AI 应用能够安全地访问和操作本地及远程数据。\n",
      "Human: 计算 (27*13)+9 等于多少？\n",
      "Assistant: 360\n",
      "Follow Up Input: MCP 和 LangChain Output Parser 有何区别？\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP 和 LangChain 的输出解析器之间有什么不同之处？  ', 'MCP 与 LangChain 的输出解析器相比，主要的区别是什么？  ', '请问 MCP 和 LangChain 的输出解析器各自的特点和差异是什么？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Model返回的内容通常都是字符串的模式，但在实际开发过程中，往往希望model可以返回更直观的内容，Langchain提供的输出解析器则将派上用场。\n",
      "\n",
      "在实现一个输出解析器的过程中，需要实现两种方法：\n",
      "\n",
      "获取格式指令：返回一个字符串的方法，其中包含有关如何格式化语言模型输出的说明。\n",
      "\n",
      "Parse：一种接收字符串（假设是来自语言模型的响应）并将其解析为某种结构的方法。\n",
      "\n",
      "这也就意味着，如果我们要发开一个 Function Call 工具，需要对不同的模型进行适配，比如参数格式、触发逻辑、返回结构等等，这个成本是非常高的。\n",
      "\n",
      "阶段三、MCP（ Anthropic  在 2024 .11 提出）\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "核心贡献：统一规范\n",
      "\n",
      "新的AI能力共享生态：通过MCP Server的共享，新的LLM应用可以快速获得各种工具，形成了一种新的合作体系，提高整体效用。\n",
      "\n",
      "总结：FunctionCall和MCP的区别\n",
      "\n",
      "\n",
      "\n",
      "3. 核心架构\n",
      "\n",
      "3.1 概念解析：\n",
      "\n",
      "\n",
      "\n",
      "MCP 主机（MCP Hosts）：发起请求的 LLM 应用程序（例如 Claude Desktop、IDE 或 AI 工具）。\n",
      "\n",
      "MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n",
      "\n",
      "MCP调研报告\n",
      "\n",
      "1. 什么是MCP（Model Context Protocol）\n",
      "\n",
      "提供哪些服务：\n",
      "\n",
      "Tools：提供给LLM应用特别是Agent使用的工具。\n",
      "\n",
      "Resoures：提供给LLM应用一些额外的结构化数据。\n",
      "\n",
      "Prompts：提供给LLM应用的一些Prompt模板。比如你的应用是一个Chatbot，可以从MCP Server中取出这些模板，让使用者选择使用。\n",
      "\n",
      "使用什么语言创建？\n",
      "\n",
      "目前支持TS/Python/Java SDK来编写MCP Server。\n",
      "Human: MCP 和 LangChain Output Parser 有何区别？\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mMCP（Model Context Protocol）和LangChain Output Parser的区别在于它们的核心功能和目的。\n",
      "\n",
      "1. **MCP**：\n",
      "   - 是一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。\n",
      "   - 主要目的是解决AI模型因数据孤岛限制而无法充分发挥潜力的问题，提供安全访问和操作本地及远程数据的能力。\n",
      "   - 提供工具、资源和Prompt模板，支持LLM应用的开发。\n",
      "\n",
      "2. **LangChain Output Parser**：\n",
      "   - 主要用于解析和格式化语言模型的输出，使其更直观和易于使用。\n",
      "   - 需要实现获取格式指令和解析字符串的方法，专注于如何处理和转换模型返回的内容。\n",
      "\n",
      "总结来说，MCP是一个更广泛的协议，旨在促进LLM与外部工具和数据的交互，而LangChain Output Parser则是一个具体的工具，用于处理和解析模型的输出。\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道了最终答案。  \n",
      "Final Answer: MCP（Model Context Protocol）和LangChain Output Parser的区别在于它们的核心功能和目的。MCP是一个开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议，主要解决AI模型因数据孤岛限制而无法充分发挥潜力的问题。而LangChain Output Parser则主要用于解析和格式化语言模型的输出，使其更直观和易于使用。总结来说，MCP是一个更广泛的协议，促进LLM与外部工具和数据的交互，而LangChain Output Parser是一个具体的工具，用于处理和解析模型的输出。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCP（Model Context Protocol）和LangChain Output Parser的区别在于它们的核心功能和目的。MCP是一个开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议，主要解决AI模型因数据孤岛限制而无法充分发挥潜力的问题。而LangChain Output Parser则主要用于解析和格式化语言模型的输出，使其更直观和易于使用。总结来说，MCP是一个更广泛的协议，促进LLM与外部工具和数据的交互，而LangChain Output Parser是一个具体的工具，用于处理和解析模型的输出。\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(\"MCP 和 LangChain Output Parser 有何区别？\"))   # ➜ 触发 RAG_QA 工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class SimpleCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"一个最小可用的回调示例：打印 LLM / Chain 的关键阶段\"\"\"\n",
    "\n",
    "    # LLM 开始生成时触发\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(\"\\n🔵 LLM started\")  # serialized 是模型信息；prompts 是提示词\n",
    "\n",
    "    # LLM 流式生成新 token 时触发（若模型/链支持流式）\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)  # 实时打印每个 token，可做打字机效果\n",
    "\n",
    "    # LLM 结束时触发\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"\\n🟢 LLM finished\")  # response 包含完整文本及 token 使用量\n",
    "\n",
    "    # 整个链（Chain / Agent）结束时触发\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        # outputs 是一个 dict，键值由具体链决定\n",
    "        print(f\"🟡 Chain outputs: {list(outputs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'StringPromptValue' object has no attribute 'keys'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mLangChain 是一个用于构建基于语言模型的应用程序的框架。它提供了一系列工具和组件，帮助开发者更容易地集成和使用语言模型，支持多种功能，如文本生成、问答、对话系统等。为了更好地总结，我将使用 RAG_Summary 工具来获取更详细的信息。\n",
      "\n",
      "Action: RAG_Summary\n",
      "Action Input: \"LangChain 的作用\"\u001b[0m\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['LangChain 的功能是什么？  ', 'LangChain 可以做些什么？  ', 'LangChain 的主要用途有哪些？']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 系列教程（一）：入门\n",
      "\n",
      "介绍\n",
      "\n",
      "LangChain 是一个用于开发由语言模型驱动的应用程序的框架。它使得应用程序能够：\n",
      "\n",
      "具有上下文感知能力：将语言模型连接到上下文来源（提示指令，少量的示例，需要回应的内容等）\n",
      "\n",
      "具有推理能力：依赖语言模型进行推理（根据提供的上下文如何回答，采取什么行动等）\n",
      "\n",
      "这个框架由几个部分组成。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 库：Python 和 JavaScript 库。包含了各种组件的接口和集成，一个基本的运行时，用于将这些组件组合成链和代理，以及现成的链和代理的实现。\n",
      "\n",
      "LangChain 模板：一系列易于部署的参考架构，用于各种任务。\n",
      "\n",
      "LangServe：一个用于将 LangChain 链部署为 REST API 的库。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain 系列教程（二）：Model I/O\n",
      "\n",
      "介绍\n",
      "\n",
      "LangChain 提供的 LangChain Expression Language(LCEL) 让开发可以很方便地将多个组件连接成 AI 工作流。如下是一个简单的工作流：\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangSmith：一个开发者平台，让你可以调试、测试、评估和监控基于任何 LLM 框架构建的链，并且与 LangChain 无缝集成。 \n",
      "\n",
      "核心模块\n",
      "\n",
      "LangChain由6个module组成，分别是Model IO、Retrieval、Chains、Memory、Agents和Callbacks。\n",
      "\n",
      "Model I/O：AI应用的核心部分，其中包括输入、Model和输出。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Chains：虽然通过Model IO、Retrieval和Memory这三大模块可以初步完成应用搭建，但是若想实现一个强大且复杂的应用，还是需要将各模块组合起来，这时就可以利用Chains将其连接起来，从而丰富功能。\n",
      "\n",
      "Agents：它可以通过用户的输入，理解用户的意图，返回一个特定的动作类型和参数，从而自主调用相关的工具来满足用户的需求，将应用更加智能化。\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🔵 LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['outputs']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a framework designed for developing applications powered by language models. It enables applications to have context awareness by connecting language models to various context sources and to perform reasoning based on the provided context. The framework consists of several components.\n",
      "\n",
      "LangChain is a library for Python and JavaScript that provides interfaces and integrations for various components, a runtime for combining them into chains and agents, and ready-made implementations. It includes templates for easily deployable reference architectures for different tasks and LangServe, a library for deploying LangChain chains as REST APIs.\n",
      "\n",
      "The second tutorial in the LangChain series, titled \"Model I/O,\" introduces the LangChain Expression Language (LCEL), which enables developers to easily connect multiple components to create AI workflows. A simple workflow example is provided.\n",
      "\n",
      "LangSmith is a developer platform that enables debugging, testing, evaluation, and monitoring of chains built on any LLM framework, seamlessly integrating with LangChain. LangChain consists of six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being the central component that handles input, models, and output for AI applications.\n",
      "\n",
      "Chains enable the integration of Model IO, Retrieval, and Memory modules to create complex applications with enhanced functionality. Agents enhance this by interpreting user input to autonomously execute specific actions and utilize relevant tools, making applications more intelligent.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "🔵 LLM started\n",
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "🟡 Chain outputs: ['output_text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for developing applications powered by language models, allowing for context awareness and reasoning through various components. It offers a library for Python and JavaScript, enabling integration and deployment of AI workflows via LangServe as REST APIs. The second tutorial introduces the LangChain Expression Language (LCEL) for connecting components. LangSmith is a platform for debugging and monitoring chains built on any LLM framework. LangChain comprises six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Chains integrating these modules to create complex applications, while Agents enhance functionality by autonomously executing actions based on user input.\u001b[0m\n",
      "Thought:\n",
      "🔵 LLM started\n",
      "\n",
      "🟢 LLM finished\n",
      "🟡 Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: LangChain 是一个用于开发基于语言模型的应用程序的框架，提供了多种组件和工具，支持上下文感知和推理。它包括六个核心模块：模型输入输出、检索、链、记忆、代理和回调，帮助开发者集成和部署 AI 工作流，并通过 LangServe 提供 REST API。\u001b[0m\n",
      "🟡 Chain outputs: ['output']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain 是一个用于开发基于语言模型的应用程序的框架，提供了多种组件和工具，支持上下文感知和推理。它包括六个核心模块：模型输入输出、检索、链、记忆、代理和回调，帮助开发者集成和部署 AI 工作流，并通过 LangServe 提供 REST API。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_handler = SimpleCallbackHandler()\n",
    "\n",
    "# callbacks 可以同时传入多个 handler 的列表\n",
    "agent.run(\"LangChain 的作用能一步总结给我吗？\", callbacks=[callback_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
