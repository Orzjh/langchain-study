{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.chatanywhere.tech\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-mvzdD655Ttsdn1C7IFKvdPN3XKtY5QI64ScWtiYmM4Le0K7i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥Document Loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "\n",
    "# åŠ è½½Documents\n",
    "base_dir = 'Docs' # æ–‡æ¡£çš„å­˜æ”¾ç›®å½•\n",
    "documents = []\n",
    "for file in os.listdir(base_dir): \n",
    "    # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„\n",
    "    file_path = os.path.join(base_dir, file)\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# å°†Documentsåˆ‡åˆ†æˆå—ä»¥ä¾¿åç»­è¿›è¡ŒåµŒå…¥å’Œå‘é‡å­˜å‚¨\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# å°†åˆ†å‰²åµŒå…¥å¹¶å­˜å‚¨åœ¨çŸ¢é‡æ•°æ®åº“Qdrantä¸­\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=chunked_documents, # ä»¥åˆ†å—çš„æ–‡æ¡£\n",
    "    embedding=OpenAIEmbeddings(), # ç”¨OpenAIçš„Embedding ModelåšåµŒå…¥\n",
    "    location=\":memory:\",  # in-memory å­˜å‚¨\n",
    "    collection_name=\"my_documents\" # æŒ‡å®šcollection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/3197553982.py:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡æ¨¡å‹å’ŒRetrievalé“¾\n",
    "import logging # å¯¼å…¥Loggingå·¥å…·\n",
    "from langchain_openai import ChatOpenAI # ChatOpenAIæ¨¡å‹\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetrieverå·¥å…·\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# è®¾ç½®Logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªå¤§æ¨¡å‹å·¥å…· - OpenAIçš„GPT-3.5\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªMultiQueryRetriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªmemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.clear()\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever_from_llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/688091923.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": \"ä»€ä¹ˆæ˜¯MCPï¼Ÿ\"})\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCPæ˜¯ä»€ä¹ˆï¼Ÿ  ', 'MCPçš„å®šä¹‰å’Œæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ  ', 'MCPå…·ä½“æŒ‡çš„æ˜¯ä»€ä¹ˆå†…å®¹ï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\n",
      "\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\n",
      "\n",
      "æä¾›å“ªäº›æœåŠ¡ï¼š\n",
      "\n",
      "Toolsï¼šæä¾›ç»™LLMåº”ç”¨ç‰¹åˆ«æ˜¯Agentä½¿ç”¨çš„å·¥å…·ã€‚\n",
      "\n",
      "Resouresï¼šæä¾›ç»™LLMåº”ç”¨ä¸€äº›é¢å¤–çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "Promptsï¼šæä¾›ç»™LLMåº”ç”¨çš„ä¸€äº›Promptæ¨¡æ¿ã€‚æ¯”å¦‚ä½ çš„åº”ç”¨æ˜¯ä¸€ä¸ªChatbotï¼Œå¯ä»¥ä»MCP Serverä¸­å–å‡ºè¿™äº›æ¨¡æ¿ï¼Œè®©ä½¿ç”¨è€…é€‰æ‹©ä½¿ç”¨ã€‚\n",
      "\n",
      "ä½¿ç”¨ä»€ä¹ˆè¯­è¨€åˆ›å»ºï¼Ÿ\n",
      "\n",
      "ç›®å‰æ”¯æŒTS/Python/Java SDKæ¥ç¼–å†™MCP Serverã€‚\n",
      "\n",
      "è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è¦å‘å¼€ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œæ¯”å¦‚å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘ã€è¿”å›ç»“æ„ç­‰ç­‰ï¼Œè¿™ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„ã€‚\n",
      "\n",
      "é˜¶æ®µä¸‰ã€MCPï¼ˆ Anthropic  åœ¨ 2024 .11 æå‡ºï¼‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "æ ¸å¿ƒè´¡çŒ®ï¼šç»Ÿä¸€è§„èŒƒ\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'ä»€ä¹ˆæ˜¯MCPï¼Ÿ', 'chat_history': [HumanMessage(content='ä»€ä¹ˆæ˜¯MCPï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚', additional_kwargs={}, response_metadata={})], 'answer': 'MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚'}\n",
      "MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"question\": \"ä»€ä¹ˆæ˜¯MCPï¼Ÿ\"})\n",
    "print(result)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\n",
      "Assistant: MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚\n",
      "Follow Up Input: å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCPçš„å…·ä½“ä½¿ç”¨æ­¥éª¤æœ‰å“ªäº›ï¼Ÿ  ', 'å¦‚ä½•æ­£ç¡®ä½¿ç”¨MCPï¼Ÿ  ', 'MCPçš„æ“ä½œæŒ‡å—æ˜¯ä»€ä¹ˆï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Command å¯¹åº”å‘½ä»¤ï¼Œargs å¯¹åº”å‚æ•°ã€‚å¯¹äºå…¬å¼€çš„ MCP å·¥å…·ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è¿™æ ·æ‰¾åˆ°ç›¸åº”çš„è®¾ç½®æ–¹æ³•ï¼Œåé¢æˆ‘è¿˜ä¼šä»‹ç»å¦‚ä½•æŸ¥æ‰¾æ›´å¤šå…¬å¼€çš„ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "4.2.4 ä½¿ç”¨æ–¹æ³•\n",
      "\n",
      "å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "é€‰æ‹©æ”¯æŒ MCP çš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\n",
      "\n",
      "å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼\n",
      "\n",
      "\n",
      "\n",
      "ä½¿ç”¨mcpåŠŸèƒ½\n",
      "\n",
      "\n",
      "\n",
      "é€‰æ‹©æ”¯æŒmcpçš„æ¨¡å‹\n",
      "\n",
      "å¯¹è¯æ•ˆæœï¼š\n",
      "\n",
      "\n",
      "\n",
      "å¯¹è¯æ•ˆæœ\n",
      "\n",
      "MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\n",
      "\n",
      "MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\n",
      "\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "å·¥å…·æ“ä½œæ–‡ä»¶å¤¹ï¼Œå®ç°é€šè¿‡å¯¹è¯æ§åˆ¶æ¡Œé¢æ–‡ä»¶ã€‚é€šè¿‡è¿™äº›ç¤ºä¾‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·äº†è§£ MCP å¼ºå¤§çš„æ‰©å±•æ€§ã€‚\n",
      "Human: MCPçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'chat_history': [HumanMessage(content='ä»€ä¹ˆæ˜¯MCPï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚', additional_kwargs={}, response_metadata={}), HumanMessage(content='å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='MCPçš„ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\\n\\n1. å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨MCPæœåŠ¡å™¨ã€‚\\n2. é€‰æ‹©æ”¯æŒMCPçš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\\n3. å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼ \\n\\né€šè¿‡è¿™äº›æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨MCPåŠŸèƒ½ä¸æ”¯æŒçš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚', additional_kwargs={}, response_metadata={})], 'answer': 'MCPçš„ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\\n\\n1. å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨MCPæœåŠ¡å™¨ã€‚\\n2. é€‰æ‹©æ”¯æŒMCPçš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\\n3. å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼ \\n\\né€šè¿‡è¿™äº›æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨MCPåŠŸèƒ½ä¸æ”¯æŒçš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚'}\n",
      "{'question': 'ä»€ä¹ˆæ˜¯MCPï¼Ÿ', 'chat_history': [HumanMessage(content='ä»€ä¹ˆæ˜¯MCPï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚', additional_kwargs={}, response_metadata={}), HumanMessage(content='å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', additional_kwargs={}, response_metadata={}), AIMessage(content='MCPçš„ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\\n\\n1. å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨MCPæœåŠ¡å™¨ã€‚\\n2. é€‰æ‹©æ”¯æŒMCPçš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\\n3. å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼ \\n\\né€šè¿‡è¿™äº›æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨MCPåŠŸèƒ½ä¸æ”¯æŒçš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚', additional_kwargs={}, response_metadata={})], 'answer': 'MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚'}\n",
      "MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚\n"
     ]
    }
   ],
   "source": [
    "# å…·æœ‰è®°å¿†åŠŸèƒ½ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­â€œå®ƒâ€ä¸ºMCP\n",
    "print(qa_chain({\"question\": \"å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\"}))\n",
    "print(result)\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/2417023693.py:10: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever_from_llm.get_relevant_documents(question)\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['è¯·ç®€è¦æ¦‚è¿° MCP çš„ä¸»è¦æ€æƒ³ï¼Œæ§åˆ¶åœ¨ 100 å­—ä»¥å†…ã€‚  ', 'èƒ½å¦ç”¨ 100 å­—è§£é‡Šä¸€ä¸‹ MCP çš„åŸºæœ¬æ¦‚å¿µï¼Ÿ  ', 'è¯·ç”¨ä¸è¶…è¿‡ 100 å­—çš„æ–¹å¼æè¿° MCP çš„æ ¸å¿ƒå†…å®¹ã€‚']\n",
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/2417023693.py:12: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = summ_chain.run(docs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"å·¥å…·æ“ä½œæ–‡ä»¶å¤¹ï¼Œå®ç°é€šè¿‡å¯¹è¯æ§åˆ¶æ¡Œé¢æ–‡ä»¶ã€‚é€šè¿‡è¿™äº›ç¤ºä¾‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·äº†è§£ MCP å¼ºå¤§çš„æ‰©å±•æ€§ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"æä¾›å“ªäº›æœåŠ¡ï¼š\n",
      "\n",
      "Toolsï¼šæä¾›ç»™LLMåº”ç”¨ç‰¹åˆ«æ˜¯Agentä½¿ç”¨çš„å·¥å…·ã€‚\n",
      "\n",
      "Resouresï¼šæä¾›ç»™LLMåº”ç”¨ä¸€äº›é¢å¤–çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "Promptsï¼šæä¾›ç»™LLMåº”ç”¨çš„ä¸€äº›Promptæ¨¡æ¿ã€‚æ¯”å¦‚ä½ çš„åº”ç”¨æ˜¯ä¸€ä¸ªChatbotï¼Œå¯ä»¥ä»MCP Serverä¸­å–å‡ºè¿™äº›æ¨¡æ¿ï¼Œè®©ä½¿ç”¨è€…é€‰æ‹©ä½¿ç”¨ã€‚\n",
      "\n",
      "ä½¿ç”¨ä»€ä¹ˆè¯­è¨€åˆ›å»ºï¼Ÿ\n",
      "\n",
      "ç›®å‰æ”¯æŒTS/Python/Java SDKæ¥ç¼–å†™MCP Serverã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"å°±åƒæ˜¯ä¸€ä¸ª â€œé€šç”¨æ’å¤´â€ æˆ–è€… â€œUSB æ¥å£â€ï¼Œåˆ¶å®šäº†ç»Ÿä¸€çš„è§„èŒƒï¼Œä¸ç®¡æ˜¯è¿æ¥æ•°æ®åº“ã€ç¬¬ä¸‰æ–¹ APIï¼Œè¿˜æ˜¯æœ¬åœ°æ–‡ä»¶ç­‰å„ç§å¤–éƒ¨èµ„æºï¼Œéƒ½å¯ä»¥é€šè¿‡è¿™ä¸ª â€œé€šç”¨æ¥å£â€ æ¥å®Œæˆï¼Œè®© AI æ¨¡å‹ä¸å¤–éƒ¨å·¥å…·æˆ–æ•°æ®æºä¹‹é—´çš„äº¤äº’æ›´åŠ æ ‡å‡†åŒ–ã€å¯å¤ç”¨ã€‚\n",
      "\n",
      "å¸¦æ¥çš„ä¼˜åŠ¿ï¼š\n",
      "\n",
      "LLMåº”ç”¨çš„ç®€åŒ–ï¼šä¸ç”¨é€‚é…å„ç§ç§æœ‰åè®®ï¼Œä½ åªéœ€è¦çŸ¥é“æ€ä¹ˆè¿æ¥MCP server\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP (Model Context Protocol) is an open standard launched by Anthropic in late November 2024, designed to unify communication between large language models (LLMs) and external data sources and tools. Its primary goal is to address the limitations of data silos that hinder AI models' potential, enabling secure access and operation of local and remote data, thereby providing a comprehensive interface for AI applications.\n",
      "\n",
      "The document demonstrates how to use a tool to manage desktop files through dialogue, showcasing the powerful extensibility of MCP.\n",
      "\n",
      "The MCP (Model Context Protocol) is a framework designed to facilitate the interaction and communication between different models in a system, ensuring that they operate effectively within their respective contexts.\n",
      "\n",
      "The service offers tools for LLM applications, structured data resources, and prompt templates for applications like chatbots. It supports development in TypeScript, Python, and Java SDK for creating the MCP Server.\n",
      "\n",
      "MCP Clients are server sessions created and maintained by client LLM applications using a Client SDK. MCP Servers provide context, tools, and prompts to clients and can be deployed locally or remotely.\n",
      "\n",
      "The concept of a \"universal interface,\" akin to a \"universal plug\" or \"USB port,\" establishes standardized protocols for connecting various external resources like databases, APIs, and local files. This standardization simplifies interactions between AI models and external tools, enhancing reusability and reducing the need to adapt to different proprietary protocols, as users only need to know how to connect to the MCP server.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The Model Context Protocol (MCP), launched by Anthropic in November 2024, is an open standard aimed at improving communication between large language models (LLMs) and external data sources. It addresses data silos by enabling secure access to local and remote data, providing a unified interface for AI applications. MCP facilitates interaction among models, offering tools for LLM applications, structured data resources, and prompt templates, with support for TypeScript, Python, and Java. MCP Clients maintain server sessions, while MCP Servers provide context and tools, deployable locally or remotely. The protocol establishes a \"universal interface\" for connecting various external resources, simplifying interactions and enhancing reusability across AI models.\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»º Map-Reduce æ‘˜è¦é“¾\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# ä½¿ç”¨ OpenAI çš„ GPT-3.5 æ¨¡å‹å®ä¾‹åŒ–æ‘˜è¦é“¾ï¼Œé‡‡ç”¨ map_reduce ç­–ç•¥\n",
    "summ_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ¥æ”¶ç”¨æˆ·é—®é¢˜ï¼Œæ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”Ÿæˆæ‘˜è¦\n",
    "def qa_with_summary(question: str):\n",
    "    # ä½¿ç”¨å¤šæŸ¥è¯¢æ£€ç´¢å™¨è·å–ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n",
    "    docs = retriever_from_llm.get_relevant_documents(question)\n",
    "    # å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œæ‘˜è¦\n",
    "    summary = summ_chain.run(docs)\n",
    "    return summary\n",
    "\n",
    "# ç¤ºä¾‹ï¼šç”Ÿæˆå…³äº MCP æ ¸å¿ƒæ¦‚å¿µçš„æ‘˜è¦\n",
    "print(qa_with_summary(\"è¯·ç”¨ 100 å­—æ€»ç»“ MCP çš„æ ¸å¿ƒæ¦‚å¿µ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_88182/431357988.py:28: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥æ‰€éœ€çš„å·¥å…·ç±»å’ŒAgentåˆå§‹åŒ–æ–¹æ³•\n",
    "from langchain.tools import Tool\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé€šè¿‡Retrieval QAé“¾å›ç­”ç”¨æˆ·çš„é—®é¢˜\n",
    "def retriever_tool_func(q):\n",
    "    return qa_chain({\"question\": q})[\"answer\"]  # è°ƒç”¨qa_chainå¤„ç†é—®é¢˜ï¼Œå¹¶æå–ç­”æ¡ˆéƒ¨åˆ†\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æ¡£æ£€ç´¢é—®ç­”çš„å·¥å…·\n",
    "retrieval_tool = Tool(name=\"RAG_QA\", func=retriever_tool_func, description=\"å¯¹ä¸€èˆ¬çŸ¥è¯†ç±»é—®é¢˜ï¼Œå…ˆæ£€ç´¢æ–‡æ¡£å†å›ç­”\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æ¡£æ‘˜è¦çš„å·¥å…·\n",
    "summary_tool = Tool(name=\"RAG_Summary\", func=qa_with_summary, description=\"å½“ç”¨æˆ·è¦æ±‚æ¦‚æ‹¬/æ€»ç»“æ—¶ä½¿ç”¨\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºæ‰§è¡ŒPythonä»£ç çš„å·¥å…·\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# å°†ä¸Šè¿°å·¥å…·ç»„åˆæˆä¸€ä¸ªå·¥å…·åˆ—è¡¨ï¼Œä¾›Agentä½¿ç”¨\n",
    "tools = [retrieval_tool, summary_tool, python_tool]\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªæ™ºèƒ½ä½“ï¼ˆAgentï¼‰ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n",
    "# - ä½¿ç”¨OpenAIçš„GPT-3.5æ¨¡å‹ï¼ˆllmï¼‰\n",
    "# - æä¾›çš„å·¥å…·åˆ—è¡¨ï¼ˆtoolsï¼‰\n",
    "# - ä½¿ç”¨ZERO_SHOT_REACT_DESCRIPTIONç±»å‹çš„Agentï¼ˆåŸºäºReActæ¡†æ¶ï¼‰\n",
    "# - å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆverbose=Trueï¼‰\n",
    "# - ä½¿ç”¨ä¼šè¯è®°å¿†ï¼ˆmemoryï¼‰ä»¥ä¿æŒä¸Šä¸‹æ–‡\n",
    "agent = initialize_agent(\n",
    "    tools=tools,  # æä¾›çš„å·¥å…·åˆ—è¡¨\n",
    "    llm=llm,  # ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agentçš„ç±»å‹\n",
    "    verbose=True,  # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯\n",
    "    memory=memory  # ä¼šè¯è®°å¿†ï¼Œç”¨äºä¿æŒå¯¹è¯çš„ä¸Šä¸‹æ–‡\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mMCP å¯èƒ½æŒ‡ä»£å¤šç§äº‹ç‰©ï¼Œä½†é€šå¸¸åœ¨æŠ€æœ¯å’Œè®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå®ƒæŒ‡çš„æ˜¯â€œå¾®æ§åˆ¶å™¨å¹³å°â€ï¼ˆMicrocontroller Platformï¼‰æˆ–â€œå¤šé€šé“å¤„ç†â€ï¼ˆMulti-Channel Processingï¼‰ã€‚ä¸ºäº†å‡†ç¡®æ€»ç»“å…¶ä½œç”¨ï¼Œæˆ‘éœ€è¦ç¡®è®¤å…·ä½“çš„ä¸Šä¸‹æ–‡ã€‚ç”±äºé—®é¢˜æ²¡æœ‰æä¾›å…·ä½“çš„èƒŒæ™¯ï¼Œæˆ‘å°†è¿›è¡Œæ–‡æ¡£æ£€ç´¢ä»¥è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯ã€‚\n",
      "\n",
      "Action: RAG_QA\n",
      "Action Input: \"MCP çš„ä½œç”¨\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\n",
      "Assistant: MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚\n",
      "Human: å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "Assistant: MCPçš„ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨MCPæœåŠ¡å™¨ã€‚\n",
      "2. é€‰æ‹©æ”¯æŒMCPçš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\n",
      "3. å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼ \n",
      "\n",
      "é€šè¿‡è¿™äº›æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨MCPåŠŸèƒ½ä¸æ”¯æŒçš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚\n",
      "Follow Up Input: MCP çš„ä½œç”¨\"\n",
      "\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'MCP åœ¨ä»€ä¹ˆæ–¹é¢å‘æŒ¥ä½œç”¨ï¼Ÿ  ', 'MCP çš„ä¸»è¦ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\n",
      "\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\n",
      "\n",
      "Command å¯¹åº”å‘½ä»¤ï¼Œargs å¯¹åº”å‚æ•°ã€‚å¯¹äºå…¬å¼€çš„ MCP å·¥å…·ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è¿™æ ·æ‰¾åˆ°ç›¸åº”çš„è®¾ç½®æ–¹æ³•ï¼Œåé¢æˆ‘è¿˜ä¼šä»‹ç»å¦‚ä½•æŸ¥æ‰¾æ›´å¤šå…¬å¼€çš„ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "4.2.4 ä½¿ç”¨æ–¹æ³•\n",
      "\n",
      "å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "é€‰æ‹©æ”¯æŒ MCP çš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\n",
      "\n",
      "å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼\n",
      "\n",
      "\n",
      "\n",
      "ä½¿ç”¨mcpåŠŸèƒ½\n",
      "\n",
      "\n",
      "\n",
      "é€‰æ‹©æ”¯æŒmcpçš„æ¨¡å‹\n",
      "\n",
      "å¯¹è¯æ•ˆæœï¼š\n",
      "\n",
      "\n",
      "\n",
      "å¯¹è¯æ•ˆæœ\n",
      "\n",
      "è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è¦å‘å¼€ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œæ¯”å¦‚å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘ã€è¿”å›ç»“æ„ç­‰ç­‰ï¼Œè¿™ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„ã€‚\n",
      "\n",
      "é˜¶æ®µä¸‰ã€MCPï¼ˆ Anthropic  åœ¨ 2024 .11 æå‡ºï¼‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "æ ¸å¿ƒè´¡çŒ®ï¼šç»Ÿä¸€è§„èŒƒ\n",
      "\n",
      "æä¾›å“ªäº›æœåŠ¡ï¼š\n",
      "\n",
      "Toolsï¼šæä¾›ç»™LLMåº”ç”¨ç‰¹åˆ«æ˜¯Agentä½¿ç”¨çš„å·¥å…·ã€‚\n",
      "\n",
      "Resouresï¼šæä¾›ç»™LLMåº”ç”¨ä¸€äº›é¢å¤–çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "Promptsï¼šæä¾›ç»™LLMåº”ç”¨çš„ä¸€äº›Promptæ¨¡æ¿ã€‚æ¯”å¦‚ä½ çš„åº”ç”¨æ˜¯ä¸€ä¸ªChatbotï¼Œå¯ä»¥ä»MCP Serverä¸­å–å‡ºè¿™äº›æ¨¡æ¿ï¼Œè®©ä½¿ç”¨è€…é€‰æ‹©ä½¿ç”¨ã€‚\n",
      "\n",
      "ä½¿ç”¨ä»€ä¹ˆè¯­è¨€åˆ›å»ºï¼Ÿ\n",
      "\n",
      "ç›®å‰æ”¯æŒTS/Python/Java SDKæ¥ç¼–å†™MCP Serverã€‚\n",
      "Human: MCP çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mMCPï¼ˆModel Context Protocolï¼‰çš„ä½œç”¨æ˜¯ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å®ƒæ—¨åœ¨è§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚å…·ä½“æ¥è¯´ï¼ŒMCP æä¾›äº†å·¥å…·ã€ç»“æ„åŒ–æ•°æ®å’Œæç¤ºæ¨¡æ¿ç­‰æœåŠ¡ï¼Œå¸®åŠ© LLM åº”ç”¨æ›´å¥½åœ°è¿›è¡Œäº¤äº’å’Œå¤„ç†ä¿¡æ¯ã€‚\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“äº† MCP çš„ä½œç”¨ï¼Œå®ƒæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„é€šä¿¡åè®®ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„äº¤äº’èƒ½åŠ›ã€‚ \n",
      "\n",
      "Final Answer: MCPï¼ˆModel Context Protocolï¼‰çš„ä½œç”¨æ˜¯ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ï¼Œè§£å†³æ•°æ®å­¤å²›é—®é¢˜ï¼Œä½¿ AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCPï¼ˆModel Context Protocolï¼‰çš„ä½œç”¨æ˜¯ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ï¼Œè§£å†³æ•°æ®å­¤å²›é—®é¢˜ï¼Œä½¿ AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(\"MCP çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\"))    # âœ è§¦å‘ RAG_Summary å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦è®¡ç®—è¡¨è¾¾å¼ (27*13)+9 çš„å€¼ã€‚  \n",
      "Action: Python_REPL  \n",
      "Action Input: (27 * 13) + 9  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“äº†æœ€ç»ˆç­”æ¡ˆã€‚  \n",
      "Final Answer: 360\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(\"è®¡ç®— (27*13)+9 ç­‰äºå¤šå°‘ï¼Ÿ\"))   # âœ è§¦å‘ Python è§£é‡Šå™¨å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦æŸ¥æ‰¾æœ‰å…³ MCP å’Œ LangChain Output Parser çš„ä¿¡æ¯ï¼Œä»¥ä¾¿äº†è§£å®ƒä»¬ä¹‹é—´çš„åŒºåˆ«ã€‚  \n",
      "Action: RAG_QA  \n",
      "Action Input: \"MCP å’Œ LangChain Output Parser æœ‰ä½•åŒºåˆ«ï¼Ÿ\"  \u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\n",
      "Assistant: MCPæ˜¯Model Context Protocolï¼ˆæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰çš„ç®€ç§°ï¼Œæ˜¯ç”±Anthropicåœ¨2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å…¶ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰AIæ¨¡å‹ç”±äºæ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼Œä½¿å¾—AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸ºAIåº”ç”¨æä¾›è¿æ¥å„ç§èµ„æºçš„æ¥å£ã€‚\n",
      "Human: å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "Assistant: MCPçš„ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨MCPæœåŠ¡å™¨ã€‚\n",
      "2. é€‰æ‹©æ”¯æŒMCPçš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\n",
      "3. å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼ \n",
      "\n",
      "é€šè¿‡è¿™äº›æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨MCPåŠŸèƒ½ä¸æ”¯æŒçš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚\n",
      "Human: MCP çš„ä½œç”¨\"\n",
      "\n",
      "Assistant: MCPï¼ˆModel Context Protocolï¼‰çš„ä½œç”¨æ˜¯ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å®ƒæ—¨åœ¨è§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚å…·ä½“æ¥è¯´ï¼ŒMCP æä¾›äº†å·¥å…·ã€ç»“æ„åŒ–æ•°æ®å’Œæç¤ºæ¨¡æ¿ç­‰æœåŠ¡ï¼Œå¸®åŠ© LLM åº”ç”¨æ›´å¥½åœ°è¿›è¡Œäº¤äº’å’Œå¤„ç†ä¿¡æ¯ã€‚\n",
      "Human: MCP çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\n",
      "Assistant: MCPï¼ˆModel Context Protocolï¼‰çš„ä½œç”¨æ˜¯ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ï¼Œè§£å†³æ•°æ®å­¤å²›é—®é¢˜ï¼Œä½¿ AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ã€‚\n",
      "Human: è®¡ç®— (27*13)+9 ç­‰äºå¤šå°‘ï¼Ÿ\n",
      "Assistant: 360\n",
      "Follow Up Input: MCP å’Œ LangChain Output Parser æœ‰ä½•åŒºåˆ«ï¼Ÿ\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP å’Œ LangChain çš„è¾“å‡ºè§£æå™¨ä¹‹é—´æœ‰ä»€ä¹ˆä¸åŒä¹‹å¤„ï¼Ÿ  ', 'MCP ä¸ LangChain çš„è¾“å‡ºè§£æå™¨ç›¸æ¯”ï¼Œä¸»è¦çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'è¯·é—® MCP å’Œ LangChain çš„è¾“å‡ºè§£æå™¨å„è‡ªçš„ç‰¹ç‚¹å’Œå·®å¼‚æ˜¯ä»€ä¹ˆï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Modelè¿”å›çš„å†…å®¹é€šå¸¸éƒ½æ˜¯å­—ç¬¦ä¸²çš„æ¨¡å¼ï¼Œä½†åœ¨å®é™…å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€å¸Œæœ›modelå¯ä»¥è¿”å›æ›´ç›´è§‚çš„å†…å®¹ï¼ŒLangchainæä¾›çš„è¾“å‡ºè§£æå™¨åˆ™å°†æ´¾ä¸Šç”¨åœºã€‚\n",
      "\n",
      "åœ¨å®ç°ä¸€ä¸ªè¾“å‡ºè§£æå™¨çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å®ç°ä¸¤ç§æ–¹æ³•ï¼š\n",
      "\n",
      "è·å–æ ¼å¼æŒ‡ä»¤ï¼šè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²çš„æ–¹æ³•ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³å¦‚ä½•æ ¼å¼åŒ–è¯­è¨€æ¨¡å‹è¾“å‡ºçš„è¯´æ˜ã€‚\n",
      "\n",
      "Parseï¼šä¸€ç§æ¥æ”¶å­—ç¬¦ä¸²ï¼ˆå‡è®¾æ˜¯æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å“åº”ï¼‰å¹¶å°†å…¶è§£æä¸ºæŸç§ç»“æ„çš„æ–¹æ³•ã€‚\n",
      "\n",
      "è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è¦å‘å¼€ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œæ¯”å¦‚å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘ã€è¿”å›ç»“æ„ç­‰ç­‰ï¼Œè¿™ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„ã€‚\n",
      "\n",
      "é˜¶æ®µä¸‰ã€MCPï¼ˆ Anthropic  åœ¨ 2024 .11 æå‡ºï¼‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "æ ¸å¿ƒè´¡çŒ®ï¼šç»Ÿä¸€è§„èŒƒ\n",
      "\n",
      "æ–°çš„AIèƒ½åŠ›å…±äº«ç”Ÿæ€ï¼šé€šè¿‡MCP Serverçš„å…±äº«ï¼Œæ–°çš„LLMåº”ç”¨å¯ä»¥å¿«é€Ÿè·å¾—å„ç§å·¥å…·ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„åˆä½œä½“ç³»ï¼Œæé«˜æ•´ä½“æ•ˆç”¨ã€‚\n",
      "\n",
      "æ€»ç»“ï¼šFunctionCallå’ŒMCPçš„åŒºåˆ«\n",
      "\n",
      "\n",
      "\n",
      "3. æ ¸å¿ƒæ¶æ„\n",
      "\n",
      "3.1 æ¦‚å¿µè§£æï¼š\n",
      "\n",
      "\n",
      "\n",
      "MCP ä¸»æœºï¼ˆMCP Hostsï¼‰ï¼šå‘èµ·è¯·æ±‚çš„ LLM åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ Claude Desktopã€IDE æˆ– AI å·¥å…·ï¼‰ã€‚\n",
      "\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\n",
      "\n",
      "æä¾›å“ªäº›æœåŠ¡ï¼š\n",
      "\n",
      "Toolsï¼šæä¾›ç»™LLMåº”ç”¨ç‰¹åˆ«æ˜¯Agentä½¿ç”¨çš„å·¥å…·ã€‚\n",
      "\n",
      "Resouresï¼šæä¾›ç»™LLMåº”ç”¨ä¸€äº›é¢å¤–çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "Promptsï¼šæä¾›ç»™LLMåº”ç”¨çš„ä¸€äº›Promptæ¨¡æ¿ã€‚æ¯”å¦‚ä½ çš„åº”ç”¨æ˜¯ä¸€ä¸ªChatbotï¼Œå¯ä»¥ä»MCP Serverä¸­å–å‡ºè¿™äº›æ¨¡æ¿ï¼Œè®©ä½¿ç”¨è€…é€‰æ‹©ä½¿ç”¨ã€‚\n",
      "\n",
      "ä½¿ç”¨ä»€ä¹ˆè¯­è¨€åˆ›å»ºï¼Ÿ\n",
      "\n",
      "ç›®å‰æ”¯æŒTS/Python/Java SDKæ¥ç¼–å†™MCP Serverã€‚\n",
      "Human: MCP å’Œ LangChain Output Parser æœ‰ä½•åŒºåˆ«ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mMCPï¼ˆModel Context Protocolï¼‰å’ŒLangChain Output Parserçš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„æ ¸å¿ƒåŠŸèƒ½å’Œç›®çš„ã€‚\n",
      "\n",
      "1. **MCP**ï¼š\n",
      "   - æ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚\n",
      "   - ä¸»è¦ç›®çš„æ˜¯è§£å†³AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œæä¾›å®‰å…¨è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®çš„èƒ½åŠ›ã€‚\n",
      "   - æä¾›å·¥å…·ã€èµ„æºå’ŒPromptæ¨¡æ¿ï¼Œæ”¯æŒLLMåº”ç”¨çš„å¼€å‘ã€‚\n",
      "\n",
      "2. **LangChain Output Parser**ï¼š\n",
      "   - ä¸»è¦ç”¨äºè§£æå’Œæ ¼å¼åŒ–è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶æ›´ç›´è§‚å’Œæ˜“äºä½¿ç”¨ã€‚\n",
      "   - éœ€è¦å®ç°è·å–æ ¼å¼æŒ‡ä»¤å’Œè§£æå­—ç¬¦ä¸²çš„æ–¹æ³•ï¼Œä¸“æ³¨äºå¦‚ä½•å¤„ç†å’Œè½¬æ¢æ¨¡å‹è¿”å›çš„å†…å®¹ã€‚\n",
      "\n",
      "æ€»ç»“æ¥è¯´ï¼ŒMCPæ˜¯ä¸€ä¸ªæ›´å¹¿æ³›çš„åè®®ï¼Œæ—¨åœ¨ä¿ƒè¿›LLMä¸å¤–éƒ¨å·¥å…·å’Œæ•°æ®çš„äº¤äº’ï¼Œè€ŒLangChain Output Parseråˆ™æ˜¯ä¸€ä¸ªå…·ä½“çš„å·¥å…·ï¼Œç”¨äºå¤„ç†å’Œè§£ææ¨¡å‹çš„è¾“å‡ºã€‚\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“äº†æœ€ç»ˆç­”æ¡ˆã€‚  \n",
      "Final Answer: MCPï¼ˆModel Context Protocolï¼‰å’ŒLangChain Output Parserçš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„æ ¸å¿ƒåŠŸèƒ½å’Œç›®çš„ã€‚MCPæ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ï¼Œä¸»è¦è§£å†³AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ã€‚è€ŒLangChain Output Parseråˆ™ä¸»è¦ç”¨äºè§£æå’Œæ ¼å¼åŒ–è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶æ›´ç›´è§‚å’Œæ˜“äºä½¿ç”¨ã€‚æ€»ç»“æ¥è¯´ï¼ŒMCPæ˜¯ä¸€ä¸ªæ›´å¹¿æ³›çš„åè®®ï¼Œä¿ƒè¿›LLMä¸å¤–éƒ¨å·¥å…·å’Œæ•°æ®çš„äº¤äº’ï¼Œè€ŒLangChain Output Parseræ˜¯ä¸€ä¸ªå…·ä½“çš„å·¥å…·ï¼Œç”¨äºå¤„ç†å’Œè§£ææ¨¡å‹çš„è¾“å‡ºã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCPï¼ˆModel Context Protocolï¼‰å’ŒLangChain Output Parserçš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„æ ¸å¿ƒåŠŸèƒ½å’Œç›®çš„ã€‚MCPæ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ï¼Œä¸»è¦è§£å†³AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ã€‚è€ŒLangChain Output Parseråˆ™ä¸»è¦ç”¨äºè§£æå’Œæ ¼å¼åŒ–è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶æ›´ç›´è§‚å’Œæ˜“äºä½¿ç”¨ã€‚æ€»ç»“æ¥è¯´ï¼ŒMCPæ˜¯ä¸€ä¸ªæ›´å¹¿æ³›çš„åè®®ï¼Œä¿ƒè¿›LLMä¸å¤–éƒ¨å·¥å…·å’Œæ•°æ®çš„äº¤äº’ï¼Œè€ŒLangChain Output Parseræ˜¯ä¸€ä¸ªå…·ä½“çš„å·¥å…·ï¼Œç”¨äºå¤„ç†å’Œè§£ææ¨¡å‹çš„è¾“å‡ºã€‚\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(\"MCP å’Œ LangChain Output Parser æœ‰ä½•åŒºåˆ«ï¼Ÿ\"))   # âœ è§¦å‘ RAG_QA å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class SimpleCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"ä¸€ä¸ªæœ€å°å¯ç”¨çš„å›è°ƒç¤ºä¾‹ï¼šæ‰“å° LLM / Chain çš„å…³é”®é˜¶æ®µ\"\"\"\n",
    "\n",
    "    # LLM å¼€å§‹ç”Ÿæˆæ—¶è§¦å‘\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(\"\\nğŸ”µ LLM started\")  # serialized æ˜¯æ¨¡å‹ä¿¡æ¯ï¼›prompts æ˜¯æç¤ºè¯\n",
    "\n",
    "    # LLM æµå¼ç”Ÿæˆæ–° token æ—¶è§¦å‘ï¼ˆè‹¥æ¨¡å‹/é“¾æ”¯æŒæµå¼ï¼‰\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)  # å®æ—¶æ‰“å°æ¯ä¸ª tokenï¼Œå¯åšæ‰“å­—æœºæ•ˆæœ\n",
    "\n",
    "    # LLM ç»“æŸæ—¶è§¦å‘\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"\\nğŸŸ¢ LLM finished\")  # response åŒ…å«å®Œæ•´æ–‡æœ¬åŠ token ä½¿ç”¨é‡\n",
    "\n",
    "    # æ•´ä¸ªé“¾ï¼ˆChain / Agentï¼‰ç»“æŸæ—¶è§¦å‘\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        # outputs æ˜¯ä¸€ä¸ª dictï¼Œé”®å€¼ç”±å…·ä½“é“¾å†³å®š\n",
    "        print(f\"ğŸŸ¡ Chain outputs: {list(outputs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'StringPromptValue' object has no attribute 'keys'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mLangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—å·¥å…·å’Œç»„ä»¶ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å®¹æ˜“åœ°é›†æˆå’Œä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå¤šç§åŠŸèƒ½ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚ä¸ºäº†æ›´å¥½åœ°æ€»ç»“ï¼Œæˆ‘å°†ä½¿ç”¨ RAG_Summary å·¥å…·æ¥è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚\n",
      "\n",
      "Action: RAG_Summary\n",
      "Action Input: \"LangChain çš„ä½œç”¨\"\u001b[0m\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['LangChain çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'LangChain å¯ä»¥åšäº›ä»€ä¹ˆï¼Ÿ  ', 'LangChain çš„ä¸»è¦ç”¨é€”æœ‰å“ªäº›ï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆä¸€ï¼‰ï¼šå…¥é—¨\n",
      "\n",
      "ä»‹ç»\n",
      "\n",
      "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒä½¿å¾—åº”ç”¨ç¨‹åºèƒ½å¤Ÿï¼š\n",
      "\n",
      "å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼šå°†è¯­è¨€æ¨¡å‹è¿æ¥åˆ°ä¸Šä¸‹æ–‡æ¥æºï¼ˆæç¤ºæŒ‡ä»¤ï¼Œå°‘é‡çš„ç¤ºä¾‹ï¼Œéœ€è¦å›åº”çš„å†…å®¹ç­‰ï¼‰\n",
      "\n",
      "å…·æœ‰æ¨ç†èƒ½åŠ›ï¼šä¾èµ–è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å¦‚ä½•å›ç­”ï¼Œé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ç­‰ï¼‰\n",
      "\n",
      "è¿™ä¸ªæ¡†æ¶ç”±å‡ ä¸ªéƒ¨åˆ†ç»„æˆã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain åº“ï¼šPython å’Œ JavaScript åº“ã€‚åŒ…å«äº†å„ç§ç»„ä»¶çš„æ¥å£å’Œé›†æˆï¼Œä¸€ä¸ªåŸºæœ¬çš„è¿è¡Œæ—¶ï¼Œç”¨äºå°†è¿™äº›ç»„ä»¶ç»„åˆæˆé“¾å’Œä»£ç†ï¼Œä»¥åŠç°æˆçš„é“¾å’Œä»£ç†çš„å®ç°ã€‚\n",
      "\n",
      "LangChain æ¨¡æ¿ï¼šä¸€ç³»åˆ—æ˜“äºéƒ¨ç½²çš„å‚è€ƒæ¶æ„ï¼Œç”¨äºå„ç§ä»»åŠ¡ã€‚\n",
      "\n",
      "LangServeï¼šä¸€ä¸ªç”¨äºå°† LangChain é“¾éƒ¨ç½²ä¸º REST API çš„åº“ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆäºŒï¼‰ï¼šModel I/O\n",
      "\n",
      "ä»‹ç»\n",
      "\n",
      "LangChain æä¾›çš„ LangChain Expression Language(LCEL) è®©å¼€å‘å¯ä»¥å¾ˆæ–¹ä¾¿åœ°å°†å¤šä¸ªç»„ä»¶è¿æ¥æˆ AI å·¥ä½œæµã€‚å¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å·¥ä½œæµï¼š\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangSmithï¼šä¸€ä¸ªå¼€å‘è€…å¹³å°ï¼Œè®©ä½ å¯ä»¥è°ƒè¯•ã€æµ‹è¯•ã€è¯„ä¼°å’Œç›‘æ§åŸºäºä»»ä½• LLM æ¡†æ¶æ„å»ºçš„é“¾ï¼Œå¹¶ä¸”ä¸ LangChain æ— ç¼é›†æˆã€‚ \n",
      "\n",
      "æ ¸å¿ƒæ¨¡å—\n",
      "\n",
      "LangChainç”±6ä¸ªmoduleç»„æˆï¼Œåˆ†åˆ«æ˜¯Model IOã€Retrievalã€Chainsã€Memoryã€Agentså’ŒCallbacksã€‚\n",
      "\n",
      "Model I/Oï¼šAIåº”ç”¨çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬è¾“å…¥ã€Modelå’Œè¾“å‡ºã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Chainsï¼šè™½ç„¶é€šè¿‡Model IOã€Retrievalå’ŒMemoryè¿™ä¸‰å¤§æ¨¡å—å¯ä»¥åˆæ­¥å®Œæˆåº”ç”¨æ­å»ºï¼Œä½†æ˜¯è‹¥æƒ³å®ç°ä¸€ä¸ªå¼ºå¤§ä¸”å¤æ‚çš„åº”ç”¨ï¼Œè¿˜æ˜¯éœ€è¦å°†å„æ¨¡å—ç»„åˆèµ·æ¥ï¼Œè¿™æ—¶å°±å¯ä»¥åˆ©ç”¨Chainså°†å…¶è¿æ¥èµ·æ¥ï¼Œä»è€Œä¸°å¯ŒåŠŸèƒ½ã€‚\n",
      "\n",
      "Agentsï¼šå®ƒå¯ä»¥é€šè¿‡ç”¨æˆ·çš„è¾“å…¥ï¼Œç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œè¿”å›ä¸€ä¸ªç‰¹å®šçš„åŠ¨ä½œç±»å‹å’Œå‚æ•°ï¼Œä»è€Œè‡ªä¸»è°ƒç”¨ç›¸å…³çš„å·¥å…·æ¥æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œå°†åº”ç”¨æ›´åŠ æ™ºèƒ½åŒ–ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['outputs']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a framework designed for developing applications powered by language models. It enables applications to have context awareness by connecting language models to various context sources and to perform reasoning based on the provided context. The framework consists of several components.\n",
      "\n",
      "LangChain is a library for Python and JavaScript that provides interfaces and integrations for various components, a runtime for combining them into chains and agents, and ready-made implementations. It includes templates for easily deployable reference architectures for different tasks and LangServe, a library for deploying LangChain chains as REST APIs.\n",
      "\n",
      "The second tutorial in the LangChain series, titled \"Model I/O,\" introduces the LangChain Expression Language (LCEL), which enables developers to easily connect multiple components to create AI workflows. A simple workflow example is provided.\n",
      "\n",
      "LangSmith is a developer platform that enables debugging, testing, evaluation, and monitoring of chains built on any LLM framework, seamlessly integrating with LangChain. LangChain consists of six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being the central component that handles input, models, and output for AI applications.\n",
      "\n",
      "Chains enable the integration of Model IO, Retrieval, and Memory modules to create complex applications with enhanced functionality. Agents enhance this by interpreting user input to autonomously execute specific actions and utilize relevant tools, making applications more intelligent.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ğŸŸ¡ Chain outputs: ['output_text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for developing applications powered by language models, allowing for context awareness and reasoning through various components. It offers a library for Python and JavaScript, enabling integration and deployment of AI workflows via LangServe as REST APIs. The second tutorial introduces the LangChain Expression Language (LCEL) for connecting components. LangSmith is a platform for debugging and monitoring chains built on any LLM framework. LangChain comprises six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Chains integrating these modules to create complex applications, while Agents enhance functionality by autonomously executing actions based on user input.\u001b[0m\n",
      "Thought:\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘åŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œæä¾›äº†å¤šç§ç»„ä»¶å’Œå·¥å…·ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ¨ç†ã€‚å®ƒåŒ…æ‹¬å…­ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºã€æ£€ç´¢ã€é“¾ã€è®°å¿†ã€ä»£ç†å’Œå›è°ƒï¼Œå¸®åŠ©å¼€å‘è€…é›†æˆå’Œéƒ¨ç½² AI å·¥ä½œæµï¼Œå¹¶é€šè¿‡ LangServe æä¾› REST APIã€‚\u001b[0m\n",
      "ğŸŸ¡ Chain outputs: ['output']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘åŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œæä¾›äº†å¤šç§ç»„ä»¶å’Œå·¥å…·ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ¨ç†ã€‚å®ƒåŒ…æ‹¬å…­ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºã€æ£€ç´¢ã€é“¾ã€è®°å¿†ã€ä»£ç†å’Œå›è°ƒï¼Œå¸®åŠ©å¼€å‘è€…é›†æˆå’Œéƒ¨ç½² AI å·¥ä½œæµï¼Œå¹¶é€šè¿‡ LangServe æä¾› REST APIã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_handler = SimpleCallbackHandler()\n",
    "\n",
    "# callbacks å¯ä»¥åŒæ—¶ä¼ å…¥å¤šä¸ª handler çš„åˆ—è¡¨\n",
    "agent.run(\"LangChain çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\", callbacks=[callback_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
