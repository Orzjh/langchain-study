{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé…ç½® API åœ°å€ä¸å¯†é’¥ï¼ˆä½¿ç”¨ç¬¬ä¸‰æ–¹ OpenAI é•œåƒï¼‰\n",
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.chatanywhere.tech\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk-xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- æ–‡æ¡£åŠ è½½ä¸åˆ‡åˆ† --------------------\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "\n",
    "base_dir = 'Docs'  # æ–‡æ¡£æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„\n",
    "documents = []     # å­˜æ”¾åŠ è½½åçš„ Document å¯¹è±¡\n",
    "\n",
    "# éå†æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œæ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ä¸åŒçš„åŠ è½½å™¨\n",
    "for file in os.listdir(base_dir):\n",
    "    file_path = os.path.join(base_dir, file)\n",
    "    if file.endswith('.pdf'):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.docx'):\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith('.txt'):\n",
    "        loader = TextLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# æ–‡æœ¬åˆ‡åˆ†å™¨ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†ä¸ºå¤šä¸ªæ®µè½å—ï¼Œchunk_size ä¸ºæ¯æ®µæœ€å¤§é•¿åº¦\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "chunked_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- åµŒå…¥ + å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ --------------------\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# å°†åˆ‡åˆ†åçš„æ–‡æ¡£åµŒå…¥ä¸ºå‘é‡ï¼Œå¹¶ä¸´æ—¶å­˜å‚¨åœ¨ Qdrant çš„å†…å­˜æ•°æ®åº“ä¸­\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    location=\":memory:\",  # ä»…å­˜äºå†…å­˜ï¼Œé€‚åˆæ•™å­¦/æµ‹è¯•\n",
    "    collection_name=\"my_documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCPçš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'MCPæŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'MCPçš„å…¨ç§°å’Œè§£é‡Šæ˜¯ä»€ä¹ˆï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\n",
      "\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\n",
      "\n",
      "è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è¦å‘å¼€ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œæ¯”å¦‚å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘ã€è¿”å›ç»“æ„ç­‰ç­‰ï¼Œè¿™ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„ã€‚\n",
      "\n",
      "é˜¶æ®µä¸‰ã€MCPï¼ˆ Anthropic  åœ¨ 2024 .11 æå‡ºï¼‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "æ ¸å¿ƒè´¡çŒ®ï¼šç»Ÿä¸€è§„èŒƒ\n",
      "\n",
      "Command å¯¹åº”å‘½ä»¤ï¼Œargs å¯¹åº”å‚æ•°ã€‚å¯¹äºå…¬å¼€çš„ MCP å·¥å…·ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è¿™æ ·æ‰¾åˆ°ç›¸åº”çš„è®¾ç½®æ–¹æ³•ï¼Œåé¢æˆ‘è¿˜ä¼šä»‹ç»å¦‚ä½•æŸ¥æ‰¾æ›´å¤šå…¬å¼€çš„ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "4.2.4 ä½¿ç”¨æ–¹æ³•\n",
      "\n",
      "å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "é€‰æ‹©æ”¯æŒ MCP çš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\n",
      "\n",
      "å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼\n",
      "\n",
      "\n",
      "\n",
      "ä½¿ç”¨mcpåŠŸèƒ½\n",
      "\n",
      "\n",
      "\n",
      "é€‰æ‹©æ”¯æŒmcpçš„æ¨¡å‹\n",
      "\n",
      "å¯¹è¯æ•ˆæœï¼š\n",
      "\n",
      "\n",
      "\n",
      "å¯¹è¯æ•ˆæœ\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰æ˜¯ç”±Anthropicäº2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCPçš„ä¸»è¦ç›®çš„æ˜¯è§£å†³å½“å‰AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œä½¿AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œå¹¶ä¸ºAIåº”ç”¨æä¾›è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\n",
      "Assistant: MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰æ˜¯ç”±Anthropicäº2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCPçš„ä¸»è¦ç›®çš„æ˜¯è§£å†³å½“å‰AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œä½¿AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œå¹¶ä¸ºAIåº”ç”¨æä¾›è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "Follow Up Input: å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['å®ƒæ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼Ÿ', 'è¯·é—®è¿™ä¸ªäº§å“çš„å…·ä½“ä½¿ç”¨æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¦‚ä½•æ­£ç¡®ä½¿ç”¨å®ƒï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "åšä»€ä¹ˆã€‚è€äººç¬‘ç€å›ç­”è¯´ï¼Œä»–åœ¨åšä¸€åªå°ç†Šç©å…·ï¼Œå‡†å¤‡é€ç»™è‡ªå·±çš„å­™å­ã€‚\\n\\nå°å¥³å­©æ„ŸåŠ¨åœ°çœ‹ç€',\n",
      "\n",
      "å“ˆå“ˆï¼Œçœ‹æ¥å°æ˜çš„â€œæ•°å­¦â€é€»è¾‘çœŸæ˜¯ä¸ä¼—ä¸åŒå‘€ï¼\n",
      "Predict method took 2.9873 seconds to execute.\n",
      "æœ‰ä¸€å¤©ï¼Œå°æ˜å»ä¸Šå­¦ï¼Œè€å¸ˆé—®ä»–ï¼šâ€œå°æ˜ï¼Œå¦‚æœä½ æœ‰åä¸ªè‹¹æœï¼Œç»™äº†åˆ«äººå››ä¸ªï¼Œä½ è¿˜å‰©å‡ ä¸ªè‹¹æœï¼Ÿâ€  \n",
      "å°æ˜æƒ³äº†æƒ³ï¼Œå›ç­”ï¼šâ€œè€å¸ˆï¼Œæˆ‘è¿˜å‰©ä¸‹å…­ä¸ªï¼â€  \n",
      "è€å¸ˆè¯´ï¼šâ€œä¸å¯¹å•Šï¼Œåº”è¯¥æ˜¯å…­ä¸ªã€‚ä½ ä¸ºä»€ä¹ˆä¼šé‚£ä¹ˆç¡®å®šï¼Ÿâ€\n",
      "\n",
      "ä»£ç è°ƒç”¨\n",
      "\n",
      "æ™ºèƒ½ä½“å¹³å°ï¼ˆå¦‚difyã€é˜¿é‡Œç™¾ç‚¼ï¼‰\n",
      "\n",
      "4.2 ä½¿ç”¨ç¤ºä¾‹\n",
      "\n",
      "ä»¥CherryStudioä¸ºä¾‹å­\n",
      "\n",
      "4.2.1 ä¸ºä»€ä¹ˆé€‰æ‹© Cherry Studioï¼Ÿ\n",
      "\n",
      "Command å¯¹åº”å‘½ä»¤ï¼Œargs å¯¹åº”å‚æ•°ã€‚å¯¹äºå…¬å¼€çš„ MCP å·¥å…·ï¼Œæˆ‘ä»¬é€šå¸¸å¯ä»¥è¿™æ ·æ‰¾åˆ°ç›¸åº”çš„è®¾ç½®æ–¹æ³•ï¼Œåé¢æˆ‘è¿˜ä¼šä»‹ç»å¦‚ä½•æŸ¥æ‰¾æ›´å¤šå…¬å¼€çš„ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "4.2.4 ä½¿ç”¨æ–¹æ³•\n",
      "\n",
      "å®‰è£…å®ŒæœåŠ¡å™¨åï¼Œåœ¨å¯¹è¯ç•Œé¢ç‚¹å‡»å¯ç”¨ MCP æœåŠ¡å™¨ã€‚\n",
      "\n",
      "é€‰æ‹©æ”¯æŒ MCP çš„æ¨¡å‹ï¼ˆå¸¦æœ‰æ‰³æ‰‹ ğŸ”§ æ ‡å¿—ï¼‰ã€‚\n",
      "\n",
      "å°±å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼\n",
      "\n",
      "\n",
      "\n",
      "ä½¿ç”¨mcpåŠŸèƒ½\n",
      "\n",
      "\n",
      "\n",
      "é€‰æ‹©æ”¯æŒmcpçš„æ¨¡å‹\n",
      "\n",
      "å¯¹è¯æ•ˆæœï¼š\n",
      "\n",
      "\n",
      "\n",
      "å¯¹è¯æ•ˆæœ\n",
      "\n",
      "[1m> Finished chain.[0m\n",
      "We were just exchanging greetings and asking each other how we are. Is there something specific you would like to talk about?\n",
      "\n",
      "é›†æˆå¤–éƒ¨å·¥å…·\n",
      "Human: å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "æˆ‘ä¸çŸ¥é“ã€‚\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡æ¨¡å‹å’ŒRetrievalé“¾\n",
    "import logging # å¯¼å…¥Loggingå·¥å…·\n",
    "from langchain_openai import ChatOpenAI # ChatOpenAIæ¨¡å‹\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever # MultiQueryRetrieverå·¥å…·\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# è®¾ç½®Logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªå¤§æ¨¡å‹å·¥å…· - OpenAIçš„GPT-3.5\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªMultiQueryRetriever\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectorstore.as_retriever(), llm=llm)\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªmemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.clear()\n",
    "\n",
    "# å®ä¾‹åŒ–ä¸€ä¸ªConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever_from_llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['è¯·ç”¨ 100 å­—æè¿° MCP çš„ä¸»è¦æ€æƒ³å’ŒåŸåˆ™ã€‚  ', 'MCP çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿè¯·ç”¨ 100 å­—æ¦‚æ‹¬ä¸€ä¸‹ã€‚  ', 'èƒ½å¦ç”¨ 100 å­—æ€»ç»“ MCP çš„åŸºæœ¬æ¦‚å¿µå’Œå…³é”®ç‚¹ï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"æä¾›å“ªäº›æœåŠ¡ï¼š\n",
      "\n",
      "Toolsï¼šæä¾›ç»™LLMåº”ç”¨ç‰¹åˆ«æ˜¯Agentä½¿ç”¨çš„å·¥å…·ã€‚\n",
      "\n",
      "Resouresï¼šæä¾›ç»™LLMåº”ç”¨ä¸€äº›é¢å¤–çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "Promptsï¼šæä¾›ç»™LLMåº”ç”¨çš„ä¸€äº›Promptæ¨¡æ¿ã€‚æ¯”å¦‚ä½ çš„åº”ç”¨æ˜¯ä¸€ä¸ªChatbotï¼Œå¯ä»¥ä»MCP Serverä¸­å–å‡ºè¿™äº›æ¨¡æ¿ï¼Œè®©ä½¿ç”¨è€…é€‰æ‹©ä½¿ç”¨ã€‚\n",
      "\n",
      "ä½¿ç”¨ä»€ä¹ˆè¯­è¨€åˆ›å»ºï¼Ÿ\n",
      "\n",
      "ç›®å‰æ”¯æŒTS/Python/Java SDKæ¥ç¼–å†™MCP Serverã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è¦å‘å¼€ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œæ¯”å¦‚å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘ã€è¿”å›ç»“æ„ç­‰ç­‰ï¼Œè¿™ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„ã€‚\n",
      "\n",
      "é˜¶æ®µä¸‰ã€MCPï¼ˆ Anthropic  åœ¨ 2024 .11 æå‡ºï¼‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "æ ¸å¿ƒè´¡çŒ®ï¼šç»Ÿä¸€è§„èŒƒ\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"æ–°çš„AIèƒ½åŠ›å…±äº«ç”Ÿæ€ï¼šé€šè¿‡MCP Serverçš„å…±äº«ï¼Œæ–°çš„LLMåº”ç”¨å¯ä»¥å¿«é€Ÿè·å¾—å„ç§å·¥å…·ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„åˆä½œä½“ç³»ï¼Œæé«˜æ•´ä½“æ•ˆç”¨ã€‚\n",
      "\n",
      "æ€»ç»“ï¼šFunctionCallå’ŒMCPçš„åŒºåˆ«\n",
      "\n",
      "\n",
      "\n",
      "3. æ ¸å¿ƒæ¶æ„\n",
      "\n",
      "3.1 æ¦‚å¿µè§£æï¼š\n",
      "\n",
      "\n",
      "\n",
      "MCP ä¸»æœºï¼ˆMCP Hostsï¼‰ï¼šå‘èµ·è¯·æ±‚çš„ LLM åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ Claude Desktopã€IDE æˆ– AI å·¥å…·ï¼‰ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP (Model Context Protocol), launched by Anthropic in late November 2024, is an open standard designed to unify communication between large language models (LLMs) and external data sources and tools. Its primary goal is to address data silo issues that limit AI models' potential, enabling secure access and operation of local and remote data, thereby providing a comprehensive connectivity interface for AI applications.\n",
      "\n",
      "The MCP (Model Context Protocol) research report introduces the concept of MCP, outlining its purpose and significance in managing and interpreting contextual information within models.\n",
      "\n",
      "MCP Clients are server sessions created and maintained by client LLM applications using a Client SDK. MCP Servers provide context, tools, and prompts to clients and can be deployed either locally or remotely.\n",
      "\n",
      "The MCP Server offers tools for LLM applications, particularly for agents, along with structured data resources and prompt templates for customization, such as those used in chatbots. It supports development in TypeScript, Python, and Java SDKs.\n",
      "\n",
      "ä¸ºäº†å¼€å‘ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦é€‚é…ä¸åŒæ¨¡å‹çš„å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘å’Œè¿”å›ç»“æ„ï¼Œè¿™æ ·çš„æˆæœ¬éå¸¸é«˜ã€‚2024å¹´11æœˆï¼ŒAnthropic æå‡ºäº† MCPï¼Œå…¶æ ¸å¿ƒè´¡çŒ®æ˜¯æä¾›ç»Ÿä¸€è§„èŒƒã€‚\n",
      "\n",
      "The text discusses a new AI capability-sharing ecosystem facilitated by MCP Server, which allows LLM applications to quickly access various tools, thereby forming a collaborative system that enhances overall efficiency. It also outlines the distinction between FunctionCall and MCP, and introduces MCP Hosts as LLM applications (e.g., Claude Desktop, IDE, or AI tools) that initiate requests.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Launched by Anthropic in November 2024, the Model Context Protocol (MCP) is an open standard aimed at unifying communication between large language models (LLMs) and external data sources. It addresses data silo challenges, allowing secure access to local and remote data and enhancing AI applications' connectivity. The MCP framework comprises MCP Clients, which are server sessions managed by LLM applications, and MCP Servers, which provide necessary context, tools, and prompts for LLM applications. It supports development in TypeScript, Python, and Java SDKs, and facilitates a collaborative ecosystem for efficient AI capability sharing. MCP distinguishes itself from FunctionCall and introduces MCP Hosts, which are LLM applications that initiate requests.\n"
     ]
    }
   ],
   "source": [
    "# -------------------- æ–‡æ¡£æ‘˜è¦é“¾ï¼ˆMap-Reduceï¼‰ --------------------\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summ_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "def qa_with_summary(question: str):\n",
    "    docs = retriever_from_llm.get_relevant_documents(question)\n",
    "    summary = summ_chain.run(docs)\n",
    "    return summary\n",
    "\n",
    "print(qa_with_summary(\"è¯·ç”¨ 100 å­—æ€»ç»“ MCP çš„æ ¸å¿ƒæ¦‚å¿µ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦æ€»ç»“ MCPï¼ˆå¯èƒ½æŒ‡ä»£â€œä¸»æ§å¤„ç†å™¨â€æˆ–å…¶ä»–å«ä¹‰ï¼‰çš„ä½œç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘å°†æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥è·å– MCP çš„åŠŸèƒ½å’Œä½œç”¨æ–¹é¢çš„ä¿¡æ¯ã€‚  \n",
      "Action: RAG_Summary  \n",
      "Action Input: \"MCP çš„ä½œç”¨\"  \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['MCP åœ¨ä»€ä¹ˆé¢†åŸŸä¸­å‘æŒ¥ä½œç”¨ï¼Ÿ  ', 'MCP æ˜¯ä»€ä¹ˆï¼Œå…¶ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'MCP çš„å…·ä½“ç”¨é€”å’Œåº”ç”¨ç¤ºä¾‹æœ‰å“ªäº›ï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"MCP å®¢æˆ·ç«¯ï¼ˆMCP Clientsï¼‰ï¼šç”±å®¢æˆ·ç«¯LLMåº”ç”¨ä½¿ç”¨Client SDKåˆ›å»ºå¹¶ç»´æŠ¤çš„ä¸€ä¸ªServerä¼šè¯ã€‚\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰ï¼šä¸ºClient  æä¾›ä¸Šä¸‹æ–‡ã€å·¥å…·å’Œ prompt ç­‰ä¿¡æ¯çš„æœåŠ¡ã€‚å¯ä»¥éƒ¨ç½²åœ¨LLMåº”ç”¨æœ¬æœºï¼Œä¹Ÿå¯ä»¥è¿œç¨‹éƒ¨ç½²ï¼ˆRemoteï¼‰ã€‚\n",
      "\n",
      "\n",
      "\n",
      "3.2 å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "[è¯¥ç±»å‹çš„å†…å®¹æš‚ä¸æ”¯æŒä¸‹è½½]\n",
      "\n",
      "MCP æœåŠ¡å™¨ï¼ˆMCP Serversï¼‰\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"æä¾›å“ªäº›æœåŠ¡ï¼š\n",
      "\n",
      "Toolsï¼šæä¾›ç»™LLMåº”ç”¨ç‰¹åˆ«æ˜¯Agentä½¿ç”¨çš„å·¥å…·ã€‚\n",
      "\n",
      "Resouresï¼šæä¾›ç»™LLMåº”ç”¨ä¸€äº›é¢å¤–çš„ç»“æ„åŒ–æ•°æ®ã€‚\n",
      "\n",
      "Promptsï¼šæä¾›ç»™LLMåº”ç”¨çš„ä¸€äº›Promptæ¨¡æ¿ã€‚æ¯”å¦‚ä½ çš„åº”ç”¨æ˜¯ä¸€ä¸ªChatbotï¼Œå¯ä»¥ä»MCP Serverä¸­å–å‡ºè¿™äº›æ¨¡æ¿ï¼Œè®©ä½¿ç”¨è€…é€‰æ‹©ä½¿ç”¨ã€‚\n",
      "\n",
      "ä½¿ç”¨ä»€ä¹ˆè¯­è¨€åˆ›å»ºï¼Ÿ\n",
      "\n",
      "ç›®å‰æ”¯æŒTS/Python/Java SDKæ¥ç¼–å†™MCP Serverã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The MCP (Model Context Protocol) is a framework designed to standardize the way models are contextualized and managed within various systems. It facilitates effective communication and interaction between different models by defining protocols for context sharing and management.\n",
      "\n",
      "MCP (Model Context Protocol) is an open standard introduced by Anthropic in late November 2024, designed to unify communication between large language models (LLMs) and external data sources and tools. Its primary goal is to overcome data silos that limit AI model potential, enabling secure access and operation of local and remote data, thereby providing a comprehensive interface for AI applications.\n",
      "\n",
      "MCP Clients are server sessions created and maintained by client LLM applications using a Client SDK. MCP Servers provide essential context, tools, and prompts to these clients, which can be deployed locally or remotely.\n",
      "\n",
      "The MCP Server offers tools for LLM applications, particularly for agents, along with structured data resources and prompt templates for chatbot applications. It supports development in TypeScript, Python, and Java SDK.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe Model Context Protocol (MCP) is an open standard introduced by Anthropic in November 2024 to standardize the contextualization and management of models across systems. It enhances communication between large language models (LLMs) and external data sources, aiming to eliminate data silos and facilitate secure access to local and remote data. MCP consists of client-server architecture, where MCP Clients are created by LLM applications and MCP Servers provide essential tools, context, and prompts. It supports development in TypeScript, Python, and Java SDK.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨å·²ç»çŸ¥é“ MCP çš„ä½œç”¨ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨æ ‡å‡†åŒ–æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç®¡ç†å’Œç³»ç»Ÿé—´çš„äº¤äº’ã€‚  \n",
      "Final Answer: MCPï¼ˆModel Context Protocolï¼‰æ˜¯ç”± Anthropic åœ¨ 2024 å¹´ 11 æœˆæå‡ºçš„å¼€æ”¾æ ‡å‡†ï¼Œç”¨äºæ ‡å‡†åŒ–æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨æ•°æ®æºä¹‹é—´çš„é€šä¿¡ï¼Œæ¶ˆé™¤æ•°æ®å­¤å²›å¹¶ä¿ƒè¿›å¯¹æœ¬åœ°å’Œè¿œç¨‹æ•°æ®çš„å®‰å…¨è®¿é—®ã€‚MCP é‡‡ç”¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼ŒLLM åº”ç”¨ç”Ÿæˆ MCP å®¢æˆ·ç«¯ï¼Œè€Œ MCP æœåŠ¡å™¨æä¾›å¿…è¦çš„å·¥å…·ã€ä¸Šä¸‹æ–‡å’Œæç¤ºã€‚å®ƒæ”¯æŒ TypeScriptã€Python å’Œ Java SDK çš„å¼€å‘ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCPï¼ˆModel Context Protocolï¼‰æ˜¯ç”± Anthropic åœ¨ 2024 å¹´ 11 æœˆæå‡ºçš„å¼€æ”¾æ ‡å‡†ï¼Œç”¨äºæ ‡å‡†åŒ–æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨æ•°æ®æºä¹‹é—´çš„é€šä¿¡ï¼Œæ¶ˆé™¤æ•°æ®å­¤å²›å¹¶ä¿ƒè¿›å¯¹æœ¬åœ°å’Œè¿œç¨‹æ•°æ®çš„å®‰å…¨è®¿é—®ã€‚MCP é‡‡ç”¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼ŒLLM åº”ç”¨ç”Ÿæˆ MCP å®¢æˆ·ç«¯ï¼Œè€Œ MCP æœåŠ¡å™¨æä¾›å¿…è¦çš„å·¥å…·ã€ä¸Šä¸‹æ–‡å’Œæç¤ºã€‚å®ƒæ”¯æŒ TypeScriptã€Python å’Œ Java SDK çš„å¼€å‘ã€‚\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦è®¡ç®—è¡¨è¾¾å¼ (27*13)+9 çš„å€¼ã€‚  \n",
      "Action: Python_REPL  \n",
      "Action Input: (27*13)+9  \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“äº†è®¡ç®—çš„ç»“æœã€‚  \n",
      "Final Answer: 360  \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "360\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo address the question about the differences between MCP (Model Configuration Protocol) and LangChain Output Parser, I need to gather relevant information. I'll begin by searching for general knowledge on both MCP and LangChain Output Parser to identify their distinctions.\n",
      "\n",
      "Action: RAG_QA\n",
      "Action Input: \"MCP Model Configuration Protocol vs LangChain Output Parser difference\"\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: ä»€ä¹ˆæ˜¯MCPï¼Ÿ\n",
      "Assistant: MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰æ˜¯ç”±Anthropicäº2024å¹´11æœˆåº•æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCPçš„ä¸»è¦ç›®çš„æ˜¯è§£å†³å½“å‰AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œä½¿AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œå¹¶ä¸ºAIåº”ç”¨æä¾›è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "Human: å®ƒçš„ä½¿ç”¨æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "Assistant: æˆ‘ä¸çŸ¥é“ã€‚\n",
      "Human: MCP çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\n",
      "Assistant: MCPï¼ˆModel Context Protocolï¼‰æ˜¯ç”± Anthropic åœ¨ 2024 å¹´ 11 æœˆæå‡ºçš„å¼€æ”¾æ ‡å‡†ï¼Œç”¨äºæ ‡å‡†åŒ–æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤–éƒ¨æ•°æ®æºä¹‹é—´çš„é€šä¿¡ï¼Œæ¶ˆé™¤æ•°æ®å­¤å²›å¹¶ä¿ƒè¿›å¯¹æœ¬åœ°å’Œè¿œç¨‹æ•°æ®çš„å®‰å…¨è®¿é—®ã€‚MCP é‡‡ç”¨å®¢æˆ·ç«¯-æœåŠ¡å™¨æ¶æ„ï¼ŒLLM åº”ç”¨ç”Ÿæˆ MCP å®¢æˆ·ç«¯ï¼Œè€Œ MCP æœåŠ¡å™¨æä¾›å¿…è¦çš„å·¥å…·ã€ä¸Šä¸‹æ–‡å’Œæç¤ºã€‚å®ƒæ”¯æŒ TypeScriptã€Python å’Œ Java SDK çš„å¼€å‘ã€‚\n",
      "Human: è®¡ç®— (27*13)+9 ç­‰äºå¤šå°‘ï¼Ÿ\n",
      "Assistant: 360\n",
      "Follow Up Input: MCP Model Configuration Protocol vs LangChain Output Parser difference\"\n",
      "\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. MCP æ¨¡å‹é…ç½®åè®®ä¸ LangChain è¾“å‡ºè§£æå™¨ä¹‹é—´å­˜åœ¨å“ªäº›ä¸»è¦åŒºåˆ«ï¼Ÿ', '2. åœ¨ MCP æ¨¡å‹é…ç½®åè®®å’Œ LangChain è¾“å‡ºè§£æå™¨çš„åŠŸèƒ½ä¸Šï¼Œæœ‰å“ªäº›ä¸åŒä¹‹å¤„ï¼Ÿ', '3. å¯ä»¥è¯¦ç»†è¯´æ˜ä¸€ä¸‹ MCP æ¨¡å‹é…ç½®åè®®åŠ LangChain è¾“å‡ºè§£æå™¨çš„ä¸åŒä¹‹å¤„å—ï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "MCPè°ƒç ”æŠ¥å‘Š\n",
      "\n",
      "1. ä»€ä¹ˆæ˜¯MCPï¼ˆModel Context Protocolï¼‰\n",
      "\n",
      "Modelè¿”å›çš„å†…å®¹é€šå¸¸éƒ½æ˜¯å­—ç¬¦ä¸²çš„æ¨¡å¼ï¼Œä½†åœ¨å®é™…å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€å¸Œæœ›modelå¯ä»¥è¿”å›æ›´ç›´è§‚çš„å†…å®¹ï¼ŒLangchainæä¾›çš„è¾“å‡ºè§£æå™¨åˆ™å°†æ´¾ä¸Šç”¨åœºã€‚\n",
      "\n",
      "åœ¨å®ç°ä¸€ä¸ªè¾“å‡ºè§£æå™¨çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å®ç°ä¸¤ç§æ–¹æ³•ï¼š\n",
      "\n",
      "è·å–æ ¼å¼æŒ‡ä»¤ï¼šè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²çš„æ–¹æ³•ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³å¦‚ä½•æ ¼å¼åŒ–è¯­è¨€æ¨¡å‹è¾“å‡ºçš„è¯´æ˜ã€‚\n",
      "\n",
      "Parseï¼šä¸€ç§æ¥æ”¶å­—ç¬¦ä¸²ï¼ˆå‡è®¾æ˜¯æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å“åº”ï¼‰å¹¶å°†å…¶è§£æä¸ºæŸç§ç»“æ„çš„æ–¹æ³•ã€‚\n",
      "\n",
      "MCPï¼ˆModel Context Protocolï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼‰ ï¼Œ2024å¹´11æœˆåº•ï¼Œç”± Anthropic æ¨å‡ºçš„ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚MCP çš„ä¸»è¦ç›®çš„åœ¨äºè§£å†³å½“å‰ AI æ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å……åˆ†å‘æŒ¥æ½œåŠ›çš„éš¾é¢˜ï¼ŒMCP ä½¿å¾— AI åº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä¸º AI åº”ç”¨æä¾›äº†è¿æ¥ä¸‡ç‰©çš„æ¥å£ã€‚\n",
      "\n",
      "æ–°çš„AIèƒ½åŠ›å…±äº«ç”Ÿæ€ï¼šé€šè¿‡MCP Serverçš„å…±äº«ï¼Œæ–°çš„LLMåº”ç”¨å¯ä»¥å¿«é€Ÿè·å¾—å„ç§å·¥å…·ï¼Œå½¢æˆäº†ä¸€ç§æ–°çš„åˆä½œä½“ç³»ï¼Œæé«˜æ•´ä½“æ•ˆç”¨ã€‚\n",
      "\n",
      "æ€»ç»“ï¼šFunctionCallå’ŒMCPçš„åŒºåˆ«\n",
      "\n",
      "\n",
      "\n",
      "3. æ ¸å¿ƒæ¶æ„\n",
      "\n",
      "3.1 æ¦‚å¿µè§£æï¼š\n",
      "\n",
      "\n",
      "\n",
      "MCP ä¸»æœºï¼ˆMCP Hostsï¼‰ï¼šå‘èµ·è¯·æ±‚çš„ LLM åº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ Claude Desktopã€IDE æˆ– AI å·¥å…·ï¼‰ã€‚\n",
      "\n",
      "è¿™ä¹Ÿå°±æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬è¦å‘å¼€ä¸€ä¸ª Function Call å·¥å…·ï¼Œéœ€è¦å¯¹ä¸åŒçš„æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œæ¯”å¦‚å‚æ•°æ ¼å¼ã€è§¦å‘é€»è¾‘ã€è¿”å›ç»“æ„ç­‰ç­‰ï¼Œè¿™ä¸ªæˆæœ¬æ˜¯éå¸¸é«˜çš„ã€‚\n",
      "\n",
      "é˜¶æ®µä¸‰ã€MCPï¼ˆ Anthropic  åœ¨ 2024 .11 æå‡ºï¼‰\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MCP\n",
      "\n",
      "æ ¸å¿ƒè´¡çŒ®ï¼šç»Ÿä¸€è§„èŒƒ\n",
      "Human: MCP æ¨¡å‹é…ç½®åè®®ä¸ LangChain è¾“å‡ºè§£æå™¨æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mMCPï¼ˆModel Context Protocolï¼‰å’ŒLangChainè¾“å‡ºè§£æå™¨çš„åŒºåˆ«åœ¨äºå®ƒä»¬çš„ä¸»è¦ç›®çš„å’ŒåŠŸèƒ½ã€‚\n",
      "\n",
      "MCPæ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡åè®®ã€‚å®ƒçš„ä¸»è¦ç›®çš„æ˜¯è§£å†³AIæ¨¡å‹å› æ•°æ®å­¤å²›é™åˆ¶è€Œæ— æ³•å‘æŒ¥æ½œåŠ›çš„é—®é¢˜ï¼Œä½¿AIåº”ç”¨èƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å’Œæ“ä½œæœ¬åœ°åŠè¿œç¨‹æ•°æ®ï¼Œä»è€Œæé«˜æ•´ä½“æ•ˆç”¨ã€‚\n",
      "\n",
      "è€ŒLangChainè¾“å‡ºè§£æå™¨åˆ™å…³æ³¨äºå¦‚ä½•æ ¼å¼åŒ–å’Œè§£æè¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä¸»è¦æä¾›ä¸¤ç§åŠŸèƒ½ï¼šè·å–æ ¼å¼æŒ‡ä»¤å’Œè§£ææ¨¡å‹çš„å“åº”ã€‚å®ƒæ˜¯ä¸ºäº†åœ¨å®é™…å¼€å‘ä¸­ä½¿æ¨¡å‹çš„è¿”å›å†…å®¹æ›´åŠ ç›´è§‚å’Œç»“æ„åŒ–ã€‚\n",
      "\n",
      "æ€»ç»“è€Œè¨€ï¼ŒMCPä¾§é‡äºè§„èŒƒåŒ–é€šä¿¡åè®®ï¼Œè€ŒLangChainè¾“å‡ºè§£æå™¨ä¾§é‡äºå¤„ç†æ¨¡å‹è¾“å‡ºçš„æ ¼å¼å’Œè§£æã€‚\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: MCPï¼ˆModel Context Protocolï¼‰å’ŒLangChainè¾“å‡ºè§£æå™¨çš„ä¸»è¦åŒºåˆ«åœ¨äºåŠŸèƒ½å’Œç›®çš„ã€‚MCPæ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡ï¼Œè§£å†³æ•°æ®å­¤å²›é—®é¢˜ã€‚è€ŒLangChainè¾“å‡ºè§£æå™¨åˆ™ç”¨äºæ ¼å¼åŒ–å’Œè§£æè¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å¾—æ¨¡å‹çš„è¿”å›å†…å®¹æ›´åŠ ç›´è§‚å’Œç»“æ„åŒ–ã€‚æ€»ç»“è€Œè¨€ï¼ŒMCPä¾§é‡äºé€šä¿¡è§„èŒƒï¼Œè€ŒLangChainè¾“å‡ºè§£æå™¨åˆ™ä¸“æ³¨äºæ ¼å¼åŒ–å’Œè§£æè¾“å‡ºã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "MCPï¼ˆModel Context Protocolï¼‰å’ŒLangChainè¾“å‡ºè§£æå™¨çš„ä¸»è¦åŒºåˆ«åœ¨äºåŠŸèƒ½å’Œç›®çš„ã€‚MCPæ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œæ—¨åœ¨ç»Ÿä¸€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ä¹‹é—´çš„é€šä¿¡ï¼Œè§£å†³æ•°æ®å­¤å²›é—®é¢˜ã€‚è€ŒLangChainè¾“å‡ºè§£æå™¨åˆ™ç”¨äºæ ¼å¼åŒ–å’Œè§£æè¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å¾—æ¨¡å‹çš„è¿”å›å†…å®¹æ›´åŠ ç›´è§‚å’Œç»“æ„åŒ–ã€‚æ€»ç»“è€Œè¨€ï¼ŒMCPä¾§é‡äºé€šä¿¡è§„èŒƒï¼Œè€ŒLangChainè¾“å‡ºè§£æå™¨åˆ™ä¸“æ³¨äºæ ¼å¼åŒ–å’Œè§£æè¾“å‡ºã€‚\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Agent å¤šå·¥å…·ç»„åˆ --------------------\n",
    "from langchain.tools import Tool\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºé€šè¿‡Retrieval QAé“¾å›ç­”ç”¨æˆ·çš„é—®é¢˜\n",
    "def retriever_tool_func(q):\n",
    "    return qa_chain({\"question\": q})[\"answer\"]  # è°ƒç”¨qa_chainå¤„ç†é—®é¢˜ï¼Œå¹¶æå–ç­”æ¡ˆéƒ¨åˆ†\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æ¡£æ£€ç´¢é—®ç­”çš„å·¥å…·\n",
    "retrieval_tool = Tool(name=\"RAG_QA\", func=retriever_tool_func, description=\"å¯¹ä¸€èˆ¬çŸ¥è¯†ç±»é—®é¢˜ï¼Œå…ˆæ£€ç´¢æ–‡æ¡£å†å›ç­”\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºæ–‡æ¡£æ‘˜è¦çš„å·¥å…·\n",
    "summary_tool = Tool(name=\"RAG_Summary\", func=qa_with_summary, description=\"å½“ç”¨æˆ·è¦æ±‚æ¦‚æ‹¬/æ€»ç»“æ—¶ä½¿ç”¨\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç”¨äºæ‰§è¡ŒPythonä»£ç çš„å·¥å…·\n",
    "python_tool = PythonREPLTool()\n",
    "\n",
    "# å°†ä¸Šè¿°å·¥å…·ç»„åˆæˆä¸€ä¸ªå·¥å…·åˆ—è¡¨ï¼Œä¾›Agentä½¿ç”¨\n",
    "tools = [retrieval_tool, summary_tool, python_tool]\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªæ™ºèƒ½ä½“ï¼ˆAgentï¼‰ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n",
    "# - ä½¿ç”¨OpenAIçš„GPT-3.5æ¨¡å‹ï¼ˆllmï¼‰\n",
    "# - æä¾›çš„å·¥å…·åˆ—è¡¨ï¼ˆtoolsï¼‰\n",
    "# - ä½¿ç”¨ZERO_SHOT_REACT_DESCRIPTIONç±»å‹çš„Agentï¼ˆåŸºäºReActæ¡†æ¶ï¼‰\n",
    "# - å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆverbose=Trueï¼‰\n",
    "# - ä½¿ç”¨ä¼šè¯è®°å¿†ï¼ˆmemoryï¼‰ä»¥ä¿æŒä¸Šä¸‹æ–‡\n",
    "agent = initialize_agent(\n",
    "    tools=tools,  # æä¾›çš„å·¥å…·åˆ—è¡¨\n",
    "    llm=llm,  # ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Agentçš„ç±»å‹\n",
    "    verbose=True,  # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯\n",
    "    memory=memory  # ä¼šè¯è®°å¿†ï¼Œç”¨äºä¿æŒå¯¹è¯çš„ä¸Šä¸‹æ–‡\n",
    ")\n",
    "\n",
    "# ç¤ºä¾‹ï¼šè§¦å‘æ€»ç»“å·¥å…·\n",
    "print(agent.run(\"MCP çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\"))\n",
    "\n",
    "# ç¤ºä¾‹ï¼šè§¦å‘ Python è®¡ç®—å·¥å…·\n",
    "print(agent.run(\"è®¡ç®— (27*13)+9 ç­‰äºå¤šå°‘ï¼Ÿ\"))\n",
    "\n",
    "# ç¤ºä¾‹ï¼šè§¦å‘æ–‡æ¡£æ£€ç´¢é—®ç­”å·¥å…·\n",
    "print(agent.run(\"MCP å’Œ LangChain Output Parser æœ‰ä½•åŒºåˆ«ï¼Ÿ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'StringPromptValue' object has no attribute 'keys'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mLangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº¤äº’åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯ä½¿å¼€å‘è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°åˆ›å»ºæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ä»£ç†ã€æ€»ç»“å’Œé—®ç­”ç­‰åŠŸèƒ½ï¼Œé€šè¿‡å°†ä¸åŒçš„æ•°æ®æºå’Œå¤„ç†æ¨¡å—ç»“åˆèµ·æ¥ï¼Œå¸®åŠ©å¼€å‘è€…ç®¡ç†å¤æ‚çš„åº”ç”¨é€»è¾‘ã€‚\n",
      "\n",
      "Action: RAG_Summary  \n",
      "Action Input: \"LangChain çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\"  \u001b[0m\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['LangChain çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿè¯·ç®€è¦æ¦‚è¿°ä¸€ä¸‹ã€‚  ', 'èƒ½å¦å¸®æˆ‘æ¦‚æ‹¬ä¸€ä¸‹ LangChain çš„åº”ç”¨å’Œç›®çš„ï¼Ÿ  ', 'è¯·ç®€è¿°ä¸€ä¸‹ LangChain çš„å…³é”®åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆä¸€ï¼‰ï¼šå…¥é—¨\n",
      "\n",
      "ä»‹ç»\n",
      "\n",
      "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒä½¿å¾—åº”ç”¨ç¨‹åºèƒ½å¤Ÿï¼š\n",
      "\n",
      "å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼šå°†è¯­è¨€æ¨¡å‹è¿æ¥åˆ°ä¸Šä¸‹æ–‡æ¥æºï¼ˆæç¤ºæŒ‡ä»¤ï¼Œå°‘é‡çš„ç¤ºä¾‹ï¼Œéœ€è¦å›åº”çš„å†…å®¹ç­‰ï¼‰\n",
      "\n",
      "å…·æœ‰æ¨ç†èƒ½åŠ›ï¼šä¾èµ–è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å¦‚ä½•å›ç­”ï¼Œé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ç­‰ï¼‰\n",
      "\n",
      "è¿™ä¸ªæ¡†æ¶ç”±å‡ ä¸ªéƒ¨åˆ†ç»„æˆã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain åº“ï¼šPython å’Œ JavaScript åº“ã€‚åŒ…å«äº†å„ç§ç»„ä»¶çš„æ¥å£å’Œé›†æˆï¼Œä¸€ä¸ªåŸºæœ¬çš„è¿è¡Œæ—¶ï¼Œç”¨äºå°†è¿™äº›ç»„ä»¶ç»„åˆæˆé“¾å’Œä»£ç†ï¼Œä»¥åŠç°æˆçš„é“¾å’Œä»£ç†çš„å®ç°ã€‚\n",
      "\n",
      "LangChain æ¨¡æ¿ï¼šä¸€ç³»åˆ—æ˜“äºéƒ¨ç½²çš„å‚è€ƒæ¶æ„ï¼Œç”¨äºå„ç§ä»»åŠ¡ã€‚\n",
      "\n",
      "LangServeï¼šä¸€ä¸ªç”¨äºå°† LangChain é“¾éƒ¨ç½²ä¸º REST API çš„åº“ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆäºŒï¼‰ï¼šModel I/O\n",
      "\n",
      "ä»‹ç»\n",
      "\n",
      "LangChain æä¾›çš„ LangChain Expression Language(LCEL) è®©å¼€å‘å¯ä»¥å¾ˆæ–¹ä¾¿åœ°å°†å¤šä¸ªç»„ä»¶è¿æ¥æˆ AI å·¥ä½œæµã€‚å¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å·¥ä½œæµï¼š\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangSmithï¼šä¸€ä¸ªå¼€å‘è€…å¹³å°ï¼Œè®©ä½ å¯ä»¥è°ƒè¯•ã€æµ‹è¯•ã€è¯„ä¼°å’Œç›‘æ§åŸºäºä»»ä½• LLM æ¡†æ¶æ„å»ºçš„é“¾ï¼Œå¹¶ä¸”ä¸ LangChain æ— ç¼é›†æˆã€‚ \n",
      "\n",
      "æ ¸å¿ƒæ¨¡å—\n",
      "\n",
      "LangChainç”±6ä¸ªmoduleç»„æˆï¼Œåˆ†åˆ«æ˜¯Model IOã€Retrievalã€Chainsã€Memoryã€Agentså’ŒCallbacksã€‚\n",
      "\n",
      "Model I/Oï¼šAIåº”ç”¨çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬è¾“å…¥ã€Modelå’Œè¾“å‡ºã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¡ Chain outputs: ['outputs']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a framework for developing applications driven by language models, enabling context-aware capabilities and reasoning. It integrates language models with contextual sources and facilitates decision-making based on provided input. The framework consists of several components.\n",
      "\n",
      "LangChain is a library in Python and JavaScript that provides interfaces and integrations for various components, enabling users to create chains and agents. It includes ready-made implementations and templates for easy deployment across different tasks. LangServe is an additional library that allows users to deploy LangChain chains as REST APIs.\n",
      "\n",
      "The second tutorial in the LangChain series, titled \"Model I/O,\" introduces the LangChain Expression Language (LCEL), which enables developers to easily connect multiple components to create AI workflows. A simple workflow example is provided.\n",
      "\n",
      "LangSmith is a developer platform for debugging, testing, evaluating, and monitoring chains built on any LLM framework, seamlessly integrating with LangChain. LangChain consists of six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being central to AI applications, encompassing input, model, and output.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ğŸŸ¡ Chain outputs: ['output_text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for developing language model-driven applications with context-aware capabilities and reasoning. It supports Python and JavaScript, offering interfaces to create chains and agents, along with templates for easy deployment. LangServe extends LangChain by enabling the deployment of chains as REST APIs. The \"Model I/O\" tutorial introduces the LangChain Expression Language (LCEL) for connecting components in AI workflows. Additionally, LangSmith is a platform for debugging and evaluating chains. LangChain's architecture comprises six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being central to AI functionality.\u001b[0m\n",
      "Thought:\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº’åŠ¨çš„æ¡†æ¶ï¼Œæ”¯æŒåˆ›å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œæ¨ç†çš„åº”ç”¨ç¨‹åºã€‚å®ƒä¸º Python å’Œ JavaScript æä¾›æ¥å£ï¼Œå…è®¸ç”¨æˆ·åˆ›å»ºé“¾å’Œä»£ç†ï¼Œå¹¶å…·å¤‡ä¾¿äºéƒ¨ç½²çš„æ¨¡æ¿ã€‚LangServe å¯ä»¥å°†é“¾éƒ¨ç½²ä¸º REST APIã€‚LangChain çš„æ¶æ„åŒ…å«å…­ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºã€æ£€ç´¢ã€é“¾ã€å†…å­˜ã€ä»£ç†å’Œå›è°ƒï¼Œå…¶ä¸­å¿ƒæ¨¡å—æ˜¯æ¨¡å‹è¾“å…¥è¾“å‡ºï¼Œæ”¯æŒ AI åŠŸèƒ½çš„å®ç°ã€‚\u001b[0m\n",
      "\n",
      "ğŸŸ¡ Chain outputs: ['output']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰äº’åŠ¨çš„æ¡†æ¶ï¼Œæ”¯æŒåˆ›å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›å’Œæ¨ç†çš„åº”ç”¨ç¨‹åºã€‚å®ƒä¸º Python å’Œ JavaScript æä¾›æ¥å£ï¼Œå…è®¸ç”¨æˆ·åˆ›å»ºé“¾å’Œä»£ç†ï¼Œå¹¶å…·å¤‡ä¾¿äºéƒ¨ç½²çš„æ¨¡æ¿ã€‚LangServe å¯ä»¥å°†é“¾éƒ¨ç½²ä¸º REST APIã€‚LangChain çš„æ¶æ„åŒ…å«å…­ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºã€æ£€ç´¢ã€é“¾ã€å†…å­˜ã€ä»£ç†å’Œå›è°ƒï¼Œå…¶ä¸­å¿ƒæ¨¡å—æ˜¯æ¨¡å‹è¾“å…¥è¾“å‡ºï¼Œæ”¯æŒ AI åŠŸèƒ½çš„å®ç°ã€‚'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------- å›è°ƒæ‰“å°è¾“å‡ºäº‹ä»¶ï¼ˆå¯ç”¨äºè°ƒè¯•ã€æ‰“å­—æœºæ•ˆæœï¼‰ --------------------\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "class SimpleCallbackHandler(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        print(\"\\nğŸ”µ LLM started\")\n",
    "\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        print(\"\\nğŸŸ¢ LLM finished\")\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        print(f\"\\nğŸŸ¡ Chain outputs: {list(outputs.keys())}\")\n",
    "\n",
    "callback_handler = SimpleCallbackHandler()\n",
    "\n",
    "# Agent æ‰§è¡Œå¸¦å›è°ƒï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰\n",
    "agent.run(\"LangChain çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\", callbacks=[callback_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'StringPromptValue' object has no attribute 'keys'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mLangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—å·¥å…·å’Œç»„ä»¶ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å®¹æ˜“åœ°é›†æˆå’Œä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå¤šç§åŠŸèƒ½ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ã€å¯¹è¯ç³»ç»Ÿç­‰ã€‚ä¸ºäº†æ›´å¥½åœ°æ€»ç»“ï¼Œæˆ‘å°†ä½¿ç”¨ RAG_Summary å·¥å…·æ¥è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚\n",
      "\n",
      "Action: RAG_Summary\n",
      "Action Input: \"LangChain çš„ä½œç”¨\"\u001b[0m\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "WARNING:langchain_core.callbacks.manager:Error in SimpleCallbackHandler.on_chain_end callback: AttributeError(\"'list' object has no attribute 'keys'\")\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['LangChain çš„åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ  ', 'LangChain å¯ä»¥åšäº›ä»€ä¹ˆï¼Ÿ  ', 'LangChain çš„ä¸»è¦ç”¨é€”æœ‰å“ªäº›ï¼Ÿ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆä¸€ï¼‰ï¼šå…¥é—¨\n",
      "\n",
      "ä»‹ç»\n",
      "\n",
      "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒä½¿å¾—åº”ç”¨ç¨‹åºèƒ½å¤Ÿï¼š\n",
      "\n",
      "å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼šå°†è¯­è¨€æ¨¡å‹è¿æ¥åˆ°ä¸Šä¸‹æ–‡æ¥æºï¼ˆæç¤ºæŒ‡ä»¤ï¼Œå°‘é‡çš„ç¤ºä¾‹ï¼Œéœ€è¦å›åº”çš„å†…å®¹ç­‰ï¼‰\n",
      "\n",
      "å…·æœ‰æ¨ç†èƒ½åŠ›ï¼šä¾èµ–è¯­è¨€æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å¦‚ä½•å›ç­”ï¼Œé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ç­‰ï¼‰\n",
      "\n",
      "è¿™ä¸ªæ¡†æ¶ç”±å‡ ä¸ªéƒ¨åˆ†ç»„æˆã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain åº“ï¼šPython å’Œ JavaScript åº“ã€‚åŒ…å«äº†å„ç§ç»„ä»¶çš„æ¥å£å’Œé›†æˆï¼Œä¸€ä¸ªåŸºæœ¬çš„è¿è¡Œæ—¶ï¼Œç”¨äºå°†è¿™äº›ç»„ä»¶ç»„åˆæˆé“¾å’Œä»£ç†ï¼Œä»¥åŠç°æˆçš„é“¾å’Œä»£ç†çš„å®ç°ã€‚\n",
      "\n",
      "LangChain æ¨¡æ¿ï¼šä¸€ç³»åˆ—æ˜“äºéƒ¨ç½²çš„å‚è€ƒæ¶æ„ï¼Œç”¨äºå„ç§ä»»åŠ¡ã€‚\n",
      "\n",
      "LangServeï¼šä¸€ä¸ªç”¨äºå°† LangChain é“¾éƒ¨ç½²ä¸º REST API çš„åº“ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain ç³»åˆ—æ•™ç¨‹ï¼ˆäºŒï¼‰ï¼šModel I/O\n",
      "\n",
      "ä»‹ç»\n",
      "\n",
      "LangChain æä¾›çš„ LangChain Expression Language(LCEL) è®©å¼€å‘å¯ä»¥å¾ˆæ–¹ä¾¿åœ°å°†å¤šä¸ªç»„ä»¶è¿æ¥æˆ AI å·¥ä½œæµã€‚å¦‚ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„å·¥ä½œæµï¼š\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangSmithï¼šä¸€ä¸ªå¼€å‘è€…å¹³å°ï¼Œè®©ä½ å¯ä»¥è°ƒè¯•ã€æµ‹è¯•ã€è¯„ä¼°å’Œç›‘æ§åŸºäºä»»ä½• LLM æ¡†æ¶æ„å»ºçš„é“¾ï¼Œå¹¶ä¸”ä¸ LangChain æ— ç¼é›†æˆã€‚ \n",
      "\n",
      "æ ¸å¿ƒæ¨¡å—\n",
      "\n",
      "LangChainç”±6ä¸ªmoduleç»„æˆï¼Œåˆ†åˆ«æ˜¯Model IOã€Retrievalã€Chainsã€Memoryã€Agentså’ŒCallbacksã€‚\n",
      "\n",
      "Model I/Oï¼šAIåº”ç”¨çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå…¶ä¸­åŒ…æ‹¬è¾“å…¥ã€Modelå’Œè¾“å‡ºã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Chainsï¼šè™½ç„¶é€šè¿‡Model IOã€Retrievalå’ŒMemoryè¿™ä¸‰å¤§æ¨¡å—å¯ä»¥åˆæ­¥å®Œæˆåº”ç”¨æ­å»ºï¼Œä½†æ˜¯è‹¥æƒ³å®ç°ä¸€ä¸ªå¼ºå¤§ä¸”å¤æ‚çš„åº”ç”¨ï¼Œè¿˜æ˜¯éœ€è¦å°†å„æ¨¡å—ç»„åˆèµ·æ¥ï¼Œè¿™æ—¶å°±å¯ä»¥åˆ©ç”¨Chainså°†å…¶è¿æ¥èµ·æ¥ï¼Œä»è€Œä¸°å¯ŒåŠŸèƒ½ã€‚\n",
      "\n",
      "Agentsï¼šå®ƒå¯ä»¥é€šè¿‡ç”¨æˆ·çš„è¾“å…¥ï¼Œç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œè¿”å›ä¸€ä¸ªç‰¹å®šçš„åŠ¨ä½œç±»å‹å’Œå‚æ•°ï¼Œä»è€Œè‡ªä¸»è°ƒç”¨ç›¸å…³çš„å·¥å…·æ¥æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼Œå°†åº”ç”¨æ›´åŠ æ™ºèƒ½åŒ–ã€‚\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸ”µ LLM started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zjh/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:386: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['outputs']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a framework designed for developing applications powered by language models. It enables applications to have context awareness by connecting language models to various context sources and to perform reasoning based on the provided context. The framework consists of several components.\n",
      "\n",
      "LangChain is a library for Python and JavaScript that provides interfaces and integrations for various components, a runtime for combining them into chains and agents, and ready-made implementations. It includes templates for easily deployable reference architectures for different tasks and LangServe, a library for deploying LangChain chains as REST APIs.\n",
      "\n",
      "The second tutorial in the LangChain series, titled \"Model I/O,\" introduces the LangChain Expression Language (LCEL), which enables developers to easily connect multiple components to create AI workflows. A simple workflow example is provided.\n",
      "\n",
      "LangSmith is a developer platform that enables debugging, testing, evaluation, and monitoring of chains built on any LLM framework, seamlessly integrating with LangChain. LangChain consists of six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Model I/O being the central component that handles input, models, and output for AI applications.\n",
      "\n",
      "Chains enable the integration of Model IO, Retrieval, and Memory modules to create complex applications with enhanced functionality. Agents enhance this by interpreting user input to autonomously execute specific actions and utilize relevant tools, making applications more intelligent.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ğŸŸ¡ Chain outputs: ['output_text']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for developing applications powered by language models, allowing for context awareness and reasoning through various components. It offers a library for Python and JavaScript, enabling integration and deployment of AI workflows via LangServe as REST APIs. The second tutorial introduces the LangChain Expression Language (LCEL) for connecting components. LangSmith is a platform for debugging and monitoring chains built on any LLM framework. LangChain comprises six core modules: Model I/O, Retrieval, Chains, Memory, Agents, and Callbacks, with Chains integrating these modules to create complex applications, while Agents enhance functionality by autonomously executing actions based on user input.\u001b[0m\n",
      "Thought:\n",
      "ğŸ”µ LLM started\n",
      "\n",
      "ğŸŸ¢ LLM finished\n",
      "ğŸŸ¡ Chain outputs: ['text']\n",
      "\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘åŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œæä¾›äº†å¤šç§ç»„ä»¶å’Œå·¥å…·ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ¨ç†ã€‚å®ƒåŒ…æ‹¬å…­ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºã€æ£€ç´¢ã€é“¾ã€è®°å¿†ã€ä»£ç†å’Œå›è°ƒï¼Œå¸®åŠ©å¼€å‘è€…é›†æˆå’Œéƒ¨ç½² AI å·¥ä½œæµï¼Œå¹¶é€šè¿‡ LangServe æä¾› REST APIã€‚\u001b[0m\n",
      "ğŸŸ¡ Chain outputs: ['output']\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘åŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ï¼Œæä¾›äº†å¤šç§ç»„ä»¶å’Œå·¥å…·ï¼Œæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œæ¨ç†ã€‚å®ƒåŒ…æ‹¬å…­ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šæ¨¡å‹è¾“å…¥è¾“å‡ºã€æ£€ç´¢ã€é“¾ã€è®°å¿†ã€ä»£ç†å’Œå›è°ƒï¼Œå¸®åŠ©å¼€å‘è€…é›†æˆå’Œéƒ¨ç½² AI å·¥ä½œæµï¼Œå¹¶é€šè¿‡ LangServe æä¾› REST APIã€‚'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_handler = SimpleCallbackHandler()\n",
    "\n",
    "# callbacks å¯ä»¥åŒæ—¶ä¼ å…¥å¤šä¸ª handler çš„åˆ—è¡¨\n",
    "agent.run(\"LangChain çš„ä½œç”¨èƒ½ä¸€æ­¥æ€»ç»“ç»™æˆ‘å—ï¼Ÿ\", callbacks=[callback_handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
