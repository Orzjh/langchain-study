### é…ç½®ç¯å¢ƒ


```python
!pip install langchain langchain-openai tiktoken

import os
os.environ["OPENAI_API_BASE"] = "https://api.chatanywhere.tech"
os.environ["OPENAI_API_KEY"] = "sk-mvzdD655Ttsdn1C7IFKvdPN3XKtY5QI64ScWtiYmM4Le0K7i"
```

    Requirement already satisfied: langchain in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (0.3.23)
    Requirement already satisfied: langchain-openai in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (0.3.12)
    Requirement already satisfied: tiktoken in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (0.9.0)
    Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (0.3.51)
    Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (0.3.8)
    Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (0.3.30)
    Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (2.11.3)
    Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (2.0.40)
    Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (2.32.3)
    Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (6.0.2)
    Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain) (4.0.3)
    Requirement already satisfied: openai<2.0.0,>=1.68.2 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain-openai) (1.73.0)
    Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from tiktoken) (2024.11.6)
    Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)
    Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)
    Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.1)
    Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)
    Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)
    Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)
    Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)
    Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)
    Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)
    Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)
    Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)
    Requirement already satisfied: sniffio in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)
    Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.66.5)
    Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)
    Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)
    Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)
    Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)
    Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)
    Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)
    Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.2.2)
    Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)
    Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)
    Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/zhangjunhao/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)


### ç¤ºä¾‹æ•™ç¨‹

**ç»™å®šä¸€æ®µæ–‡æœ¬ï¼Œè¾“å‡º 40 å­—ä»¥å†…æ‘˜è¦åŠ 3 ä¸ªå…³é”®è¯ï¼ˆJSON æ ¼å¼ï¼‰**


```python
# â‘  å‡†å¤‡â€”â€”æ¨¡æ¿ã€è§£æå™¨ä¸æ¨¡å‹
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI

# æ„é€ è§£æå™¨å¹¶å–å‡ºâ€œè¾“å‡ºæ ¼å¼è¯´æ˜â€
parser = JsonOutputParser()
format_hint = parser.get_format_instructions()

# å®šä¹‰æç¤ºè¯æ¨¡æ¿ï¼š40 å­—æ‘˜è¦ + 3 å…³é”®è¯
prompt = PromptTemplate(
    template=(
        "è¯·åœ¨ä¸è¶…è¿‡40å­—å†…æ€»ç»“ä¸‹æ–‡å†…å®¹ï¼Œå¹¶ç»™å‡º3ä¸ªå…³é”®è¯ï¼Œ"
        "è¿”å›æ ¼å¼ï¼š{format}\n\næ–‡æœ¬ï¼š{text}"
    ),
    input_variables=["text", "format"],
)

# åˆå§‹åŒ– GPT-3.5ï¼ˆå¼€å¯æµå¼è¾“å‡ºï¼‰
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# â‘¡ ç»„è£…é“¾ï¼šPromptTemplate â†’ ChatModel â†’ OutputParser
chain = prompt | llm | parser

# â‘¢ è°ƒç”¨é“¾
doc = "LangChain æ˜¯ä¸€ä¸ªé¢å‘ LLM åº”ç”¨çš„æ¡†æ¶......"
result = chain.invoke({"text": doc, "format": format_hint})

print(result)  # {'summary': 'â€¦â€¦', 'keywords': ['LangChain', 'LLM', 'æ¡†æ¶']}
```

    {'summary': 'LangChain æ˜¯ç”¨äºæ„å»ºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚', 'keywords': ['LangChain', 'å¤§è¯­è¨€æ¨¡å‹', 'æ¡†æ¶']}


### PromptTemplate

PromptTemplateæ˜¯æŒ‡ç”Ÿæˆæç¤ºçš„å¯é‡å¤çš„æ–¹å¼ã€‚å®ƒåŒ…å«ä¸€ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆâ€œæ¨¡æ¿â€ï¼‰ï¼Œå¯ä»¥æ¥æ”¶æ¥è‡ªç”¨æˆ·çš„ä¸€ç»„å‚æ•°å¹¶ç”Ÿæˆæç¤ºã€‚

å¯ä»¥ä½¿ç”¨ PromptTemplate ç±»åˆ›å»ºç®€å•çš„ç¡¬ç¼–ç æç¤ºã€‚æç¤ºæ¨¡æ¿å¯ä»¥é‡‡ç”¨ä»»æ„æ•°é‡çš„è¾“å…¥å˜é‡ï¼Œå¹¶ä¸”å¯ä»¥æ ¼å¼åŒ–ä»¥ç”Ÿæˆæç¤ºã€‚


```python
from langchain import PromptTemplate

# æ²¡æœ‰è¾“å…¥å˜é‡çš„ç¤ºä¾‹æç¤º
no_input_prompt = PromptTemplate(input_variables=[], template="ç»™æˆ‘è®²ä¸ªç¬‘è¯ã€‚")
print(no_input_prompt.format())

# å¸¦æœ‰ä¸€ä¸ªè¾“å…¥å˜é‡çš„ç¤ºä¾‹æç¤º
one_input_prompt = PromptTemplate(input_variables=["adjective"], template="ç»™æˆ‘è®²ä¸€ä¸ª{adjective}ç¬‘è¯ã€‚")
print(one_input_prompt.format(adjective="æœ‰è¶£"))

# å…·æœ‰å¤šä¸ªè¾“å…¥å˜é‡çš„ç¤ºä¾‹æç¤º
multiple_input_prompt = PromptTemplate(
    input_variables=["adjective", "content"], 
    template="ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{content}çš„{adjective}ç¬‘è¯ã€‚"
)
print(multiple_input_prompt.format(adjective="æœ‰è¶£", content="é¸¡"))

# å®šä¹‰ä¸€ä¸ªåŒ…å«å ä½ç¬¦çš„æç¤ºæ¨¡æ¿ï¼ŒLangChain ä¼šè‡ªåŠ¨è§£æå¹¶è®°å½•æ‰€æœ‰å ä½ç¬¦å˜é‡
template = "ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{content}çš„{adjective}ç¬‘è¯ã€‚"
auto_prompt = PromptTemplate.from_template(template)
print(auto_prompt.input_variables)
print(auto_prompt.format(adjective="æœ‰è¶£", content="é¸¡"))
```

    ç»™æˆ‘è®²ä¸ªç¬‘è¯ã€‚
    ç»™æˆ‘è®²ä¸€ä¸ªæœ‰è¶£ç¬‘è¯ã€‚
    ç»™æˆ‘è®²ä¸€ä¸ªå…³äºé¸¡çš„æœ‰è¶£ç¬‘è¯ã€‚
    ['adjective', 'content']
    ç»™æˆ‘è®²ä¸€ä¸ªå…³äºé¸¡çš„æœ‰è¶£ç¬‘è¯ã€‚


#### Few-shot PromptTemplate

åœ¨å¤§è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹ä¸­ï¼Œ**Few-shot examples**æ˜¯æŒ‡åœ¨åŒä¸€ä¸ªæç¤ºé‡Œå…ˆæ”¾å…¥å°‘é‡ï¼ˆé€šå¸¸ 1â€“5 æ¡ï¼‰ç¤ºèŒƒè¾“å…¥â†’è¾“å‡ºé…å¯¹ï¼Œç”¨æ¥å‘æ¨¡å‹å±•ç¤ºæœŸæœ›çš„æ ¼å¼ã€è¯­æ°”æˆ–æ¨ç†è·¯å¾„ã€‚

è¿™ç§åšæ³•å±äº in-context learningï¼šæ¨¡å‹ä¸éœ€è¦é¢å¤–å¾®è°ƒï¼Œå³å¯ä»ç¤ºä¾‹ä¸­æŠ½å–æ¨¡å¼å¹¶æ³›åŒ–åˆ°ç”¨æˆ·çš„çœŸå®é—®é¢˜ã€‚ç›¸æ¯”äº zero-shotï¼ˆä¸ç»™ç¤ºä¾‹ï¼‰ä¸å…¨é‡ç›‘ç£å¾®è°ƒï¼Œfew-shot åœ¨æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ä¸èŠ‚çœæ ‡æ³¨æˆæœ¬ä¹‹é—´å–å¾—å¹³è¡¡ã€‚â€‹

ä¸‹é¢ä»£ç æ¼”ç¤ºå¦‚ä½•åœ¨ LangChain ä¸­ä½¿ç”¨ Few-shot PromptTemplate å°†ä¸‰æ¡ç¤ºä¾‹åµŒå…¥æç¤ºï¼ŒæŒ‡å¯¼ GPT-3.5 å¯¹å½±è¯„åšæƒ…æ„Ÿåˆ¤å®šã€‚


```python
from langchain.prompts import PromptTemplate, FewShotPromptTemplate
from langchain_openai import OpenAI
from langchain_core.output_parsers import StrOutputParser

# â‘  ç¤ºä¾‹å¯¹åˆ—è¡¨
examples = [
    {"review": "è¿™éƒ¨ç”µå½±è®©æˆ‘çƒ­æ³ªç›ˆçœ¶ï¼Œæ¼”å‘˜è¡¨ç°æƒŠè‰³ï¼", "label": "Positive"},
    {"review": "æ•…äº‹è€å¥—ï¼ŒèŠ‚å¥æ‹–æ²“ï¼Œçœ‹å¾—æˆ‘æƒ³ç¡è§‰ã€‚", "label": "Negative"},
    {"review": "è§†è§‰æ•ˆæœä¸é”™ï¼Œä½†å‰§æƒ…ä¸€èˆ¬ã€‚", "label": "Neutral"},
]

# â‘¡ æè¿°â€œç¤ºä¾‹é•¿ç›¸â€çš„æ¨¡æ¿
example_prompt = PromptTemplate(
    input_variables=["review", "label"],
    template="å½±è¯„ï¼š{review}\næƒ…æ„Ÿï¼š{label}\n",
)

# â‘¢ Few-shot PromptTemplateï¼šè‡ªåŠ¨æŠŠç¤ºä¾‹ + ç”¨æˆ·è¾“å…¥æ‹¼æ¥
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,                        # è¦æ’å…¥çš„ few-shot åˆ—è¡¨
    example_prompt=example_prompt,            # å¦‚ä½•æ¸²æŸ“æ¯æ¡ç¤ºä¾‹
    suffix="å½±è¯„ï¼š{input_review}\næƒ…æ„Ÿï¼š",      # ç”¨æˆ·è¾“å…¥éƒ¨åˆ†ï¼ŒçœŸæ­£è¦é—®æ¨¡å‹çš„é—®é¢˜ï¼Œå…¶ä¸­ {input_review} æ˜¯å ä½ç¬¦
    input_variables=["input_review"],         # è¿è¡Œæ—¶è¿˜éœ€è¦å“ªäº›å­—æ®µ
)

# â‘£ è°ƒç”¨ OpenAI GPT-3.5ï¼ˆéèŠå¤©æ¥å£ï¼‰
llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
parser = StrOutputParser()                                   # ä»…è¿”å›æ–‡æœ¬æ ‡ç­¾
chain = few_shot_prompt | llm | parser

print(chain.invoke({"input_review": "åŠ¨ä½œåœºé¢éœ‡æ’¼ï¼Œå‰§æƒ…ä¹Ÿæ‰£äººå¿ƒå¼¦ã€‚"}))
```

    Positive
    
    
    <|ipynb_marker|> Markdown
    
    ## ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åˆ†ç±»æƒ…æ„Ÿ
        
        1. BERT
        2. ERNIE
        
        ã€‚ã€‚ã€‚
    
    <|ipynb_marker|> Markdown
    
    ## æƒ…æ„Ÿåˆ†æå®é™…åº”ç”¨
    å…¬å¸äººå£«å¯ä»¥é€šè¿‡æƒ…æ„Ÿåˆ†æå·¥å…·æ´å¯Ÿå¸‚åœºæ½œåœ¨éœ€æ±‚ï¼Œæé«˜åº”å¯¹å¸‚åœºå˜åŒ–çš„æ°´å¹³ï¼ŒæŠ¢å å…ˆæœºã€‚
    <|ipynb_marker|> END OF DOC


prompt å¯ä»¥é€šè¿‡ json æˆ–è€… yaml è¿›è¡Œä¿å­˜è¯»å–


```python
'''
jsonæ–‡ä»¶å†…å®¹å¦‚ä¸‹ï¼š
{
  "_type": "prompt",
  "input_variables": ["adjective", "content"],
  "template": "Tell me a {adjective} joke about {content}."
}
'''

from langchain.prompts import load_prompt

prompt = load_prompt("simple_prompt.json")
print(prompt.format(adjective="funny", content="chickens"))
```

    Tell me a funny joke about chickens.


#### Selector

åœ¨ LangChain çš„ few-shot æç¤ºä¸­ï¼ŒSelector ç”¨äºåŠ¨æ€é€‰å–æœ€åˆé€‚çš„ç¤ºä¾‹ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½æŠŠå›ºå®šç¤ºä¾‹å…¨éƒ¨å¡è¿›æç¤ºé‡Œã€‚

è¿™æ ·å¯ä»¥ (1) ç¼©çŸ­ä¸Šä¸‹æ–‡ã€èŠ‚çœæˆæœ¬ï¼›(2) æé«˜ç¤ºä¾‹ä¸å½“å‰è¾“å…¥çš„ç›¸ä¼¼åº¦ï¼Œä»è€Œè·å¾—æ›´å‡†ç¡®çš„å›ç­”ã€‚


```python
# pip install "langchain-core>=0.2" langchain-openai faiss-cpu tiktoken

from langchain_core.example_selectors import SemanticSimilarityExampleSelector   # æ–°è·¯å¾„âœ…
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings, OpenAI
from langchain.vectorstores.faiss import FAISS
from langchain_core.output_parsers import StrOutputParser

# ---------- 1) å‡†å¤‡ few-shot ç¤ºä¾‹ ----------
examples = [
    {"review": "è¿™éƒ¨ç”µå½±è®©æˆ‘çƒ­æ³ªç›ˆçœ¶ï¼Œæ¼”å‘˜è¡¨ç°æƒŠè‰³ï¼", "label": "Positive"},
    {"review": "æ•…äº‹è€å¥—ï¼ŒèŠ‚å¥æ‹–æ²“ï¼Œçœ‹å¾—æˆ‘æƒ³ç¡è§‰ã€‚",   "label": "Negative"},
    {"review": "è§†è§‰æ•ˆæœä¸é”™ï¼Œä½†å‰§æƒ…ä¸€èˆ¬ã€‚",         "label": "Neutral"},
    {"review": "é…ä¹å‡ºå½©ï¼Œä½†äººç‰©å¡‘é€ å•è–„ã€‚",         "label": "Neutral"},
    {"review": "æƒ…èŠ‚ç´§å‡‘ï¼Œåè½¬æƒŠå–œï¼Œå…¨ç¨‹æ— å°¿ç‚¹!",     "label": "Positive"},
]

# ---------- 2) å®šä¹‰â€œæ¯æ¡ç¤ºä¾‹å¦‚ä½•å‘ˆç°â€ ----------
example_prompt = PromptTemplate(
    template="å½±è¯„ï¼š{review}\næƒ…æ„Ÿï¼š{label}\n",
    input_variables=["review", "label"],
)

# ---------- 3) åˆ›å»º Semantic Selectorï¼ˆé€‰ k=3 æ¡æœ€ç›¸ä¼¼ç¤ºä¾‹ï¼‰ ----------
selector = SemanticSimilarityExampleSelector.from_examples(
    examples           = examples,          # ç¤ºä¾‹åº“
    embeddings         = OpenAIEmbeddings(),# å‘é‡æ¨¡å‹
    vectorstore_cls    = FAISS,             # å†…å­˜å‘é‡åº“
    k                  = 3,                 # æ¯æ¬¡é€‰ 3 æ¡
)

# ---------- 4) Few-shot PromptTemplate ä¸ Selector ç»“åˆ ----------
few_shot_prompt = FewShotPromptTemplate(
    example_prompt   = example_prompt,      # ç¤ºä¾‹æ¸²æŸ“æ–¹å¼
    example_selector = selector,            # åŠ¨æ€é€‰æ‹©å™¨
    suffix           = "å½±è¯„ï¼š{input_review}\næƒ…æ„Ÿï¼š",  # ç”¨æˆ·è¾“å…¥éƒ¨åˆ†
    input_variables  = ["input_review"],
)

# ---------- 5) ç»„è£…é“¾å¹¶è°ƒç”¨ ----------
llm    = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0)
parser = StrOutputParser()
chain  = few_shot_prompt | llm | parser

query  = "èŠ‚å¥æ˜å¿«ï¼Œå‰ªè¾‘æµç•…ï¼Œå°±æ˜¯ç»“å°¾ç¨æ˜¾ä»“ä¿ƒã€‚"

# selector.select_examples() ä¼šï¼š
# 1) å°† query åµŒå…¥ä¸ºå‘é‡
# 2) ä¸å‘é‡åº“ä¸­çš„ç¤ºä¾‹åšä½™å¼¦ç›¸ä¼¼åº¦æ¯”å¯¹
# 3) å–ç›¸ä¼¼åº¦æœ€é«˜çš„ k æ¡ç¤ºä¾‹ï¼ˆk åœ¨åˆ›å»º selector æ—¶è®¾å®šï¼‰
# æ³¨æ„ï¼šè¾“å…¥å­—å…¸çš„ key å¿…é¡»å¯¹åº” selector åˆ›å»ºæ—¶ä½¿ç”¨çš„å ä½ç¬¦ï¼Œ
#       ä¾‹å¦‚å‰æ–‡ FewShotPromptTemplate çš„å ä½ç¬¦æ˜¯ {input_review}ï¼Œ
#       æ­¤å¤„å°±åº”è¯¥å†™ {"input_review": query}
selected_examples = selector.select_examples({"input_review": query})
for example in selected_examples:
    for k, v in example.items():
        print(f"{k}: {v}")

print(chain.invoke({"input_review": query}))   # â†’ Positive / Neutral / Negative
```

    review: æ•…äº‹è€å¥—ï¼ŒèŠ‚å¥æ‹–æ²“ï¼Œçœ‹å¾—æˆ‘æƒ³ç¡è§‰ã€‚
    label: Negative
    review: æƒ…èŠ‚ç´§å‡‘ï¼Œåè½¬æƒŠå–œï¼Œå…¨ç¨‹æ— å°¿ç‚¹!
    label: Positive
    review: è§†è§‰æ•ˆæœä¸é”™ï¼Œä½†å‰§æƒ…ä¸€èˆ¬ã€‚
    label: Neutral
    Neutral


### Model

Langchainä½œä¸ºä¸€ä¸ªâ€œå·¥å…·â€å®ƒå¹¶æ²¡æœ‰æä¾›è‡ªå·±çš„LLMï¼Œè€Œæ˜¯æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œç”¨äºä¸è®¸å¤šä¸åŒç±»å‹çš„LLMè¿›è¡Œäº¤äº’ï¼Œæ¯”å¦‚è€³ç†Ÿèƒ½è¯¦çš„openaiã€huggingfaceæˆ–è€…æ˜¯cohereç­‰ï¼Œéƒ½å¯ä»¥é€šè¿‡langchainå¿«é€Ÿè°ƒç”¨ã€‚


```python
from langchain.llms import OpenAI

llm = OpenAI()  # åˆ›å»º LLM å¯¹è±¡
print(llm('ä½ æ˜¯è°'))    # å•æ¡è°ƒç”¨
print(llm.generate(["ç»™æˆ‘èƒŒè¯µä¸€é¦–å¤è¯—", "ç»™æˆ‘è®²ä¸ª100å­—å°æ•…äº‹"]*10)) # æ‰¹é‡è°ƒç”¨
```

    ");
                return -1;
            }
        }
        
        if (strlen(str) > 1 && !strcmp(str, "æ— è¯­"))
        {
            if (query("weiwang") >= 100000)
            {
                message_vision(HIY "$Nå‘$nç¥äº†ä¸€çœ¼ï¼Œé“ï¼šä½ è¿™ä¸ªé—®é¢˜é—®å¾—å®åœ¨æ˜¯â€¦â€¦\n" NOR, this_object(), me);
                message_vision(HIY "$Næ¥ç€å¹äº†ä¸€å£æ°”ï¼Œé“ï¼šæ²¡äº‹ï¼Œæ²¡äº‹ï¼Œæˆ‘ä¹Ÿä¸æƒ³å¤šè¯´ä»€ä¹ˆã€‚\n" NOR, this_object(), me);
                return -1;
            }
            else
            {
                message_vision(HIY "$Nå‘$nç¥äº†ä¸€çœ¼ï¼Œé“ï¼šä½ é—®æˆ‘è¿™ä¸ªå¹²ä»€ä¹ˆï¼Ÿ\n" NOR, this_object(), me);
                return -1;
            }
        }
        
        if (strlen(str) > 1 && !strcmp(str, "ä»€ä¹ˆ"))
        {
            message_vision(HIY "$Nå‘$nç¥äº†ä¸€çœ¼ï¼Œé“ï¼šä½ æƒ³çŸ¥é“ä»€ä¹ˆï¼Ÿ\n" NOR, this
    generations=[[Generation(text='\næç™½çš„ã€Šé™å¤œæ€ã€‹ï¼š\n\nåºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\nä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰ï¼Œåœ¨ä¸€ä¸ªå°é•‡ä¸Šï¼Œæœ‰ä¸€ä½è€äººï¼Œä»–æ¯å¤©éƒ½ä¼šååœ¨æ‘å£çš„é•¿æ¤…ä¸Šï¼Œè§‚å¯Ÿç€è·¯è¿‡çš„äººä»¬ã€‚ä»–æ€»æ˜¯éå¸¸å’Œè”¼åœ°å‘æ¯ä¸ªäººæ‰“æ‹›å‘¼ï¼Œç»™ä»–ä»¬ç¥ç¦ã€‚å³ä½¿æœ‰äº›äººæ²¡æœ‰å›åº”ï¼Œä»–ä¹Ÿä¸ä¼šç”Ÿæ°”ï¼Œä¾ç„¶ä¿æŒç€å¾®ç¬‘ã€‚æ‘é‡Œçš„äººéƒ½å¾ˆå–œæ¬¢å’Œä»–èŠå¤©ï¼Œå› ä¸ºä»–æ€»èƒ½ç»™ä»–ä»¬å¸¦æ¥å¿«ä¹å’Œæ¸©æš–ã€‚\n\næœ‰ä¸€å¤©ï¼Œä¸€ä½å°å¥³å­©è·¯è¿‡ï¼Œå¥¹çœ‹åˆ°è€äººçš„æ‰‹é‡Œæ‹¿ç€ä¸€æŠŠå°åˆ€åœ¨é›•åˆ»ç€ä¸€å—æœ¨å¤´ã€‚å¥¹å¾ˆå¥½å¥‡ï¼Œä¾¿èµ°è¿‡å»é—®è€äººåœ¨åšä»€ä¹ˆã€‚è€äººç¬‘ç€å›ç­”è¯´ï¼Œä»–åœ¨åšä¸€åªå°ç†Šç©å…·ï¼Œå‡†å¤‡é€ç»™è‡ªå·±çš„å­™å­ã€‚\n\nå°å¥³å­©æ„ŸåŠ¨åœ°çœ‹ç€', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nã€Šé™å¤œæ€ã€‹-æç™½\nåºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\nä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰æœ‰ä¸€ä¸ªå°ç”·å­©ï¼Œä»–éå¸¸å–œæ¬¢åƒç³–æœã€‚æ¯å¤©æ”¾å­¦å›å®¶ï¼Œä»–éƒ½ä¼šå…ˆå»ä¹°ä¸€è¢‹ç³–æœï¼Œç„¶åä¸€è¾¹åƒä¸€è¾¹ç©è€ã€‚ä»–çš„å¦ˆå¦ˆæ€»æ˜¯æ‹…å¿ƒä»–åƒå¤ªå¤šç³–ä¼šä¼¤å®³èº«ä½“ï¼Œå¯æ˜¯ä»–æ€»æ˜¯ä¸å¬ã€‚ç›´åˆ°æœ‰ä¸€å¤©ï¼Œä»–å‘ç°è‡ªå·±çš„ç‰™é½¿å¼€å§‹ç–¼ç—›ï¼Œå¦ˆå¦ˆå¸¦ä»–å»çœ‹ç‰™åŒ»ï¼ŒåŒ»ç”Ÿå‘Šè¯‰ä»–ï¼Œå› ä¸ºåƒå¤ªå¤šç³–ï¼Œä»–çš„ç‰™é½¿éƒ½è›€äº†ã€‚å°ç”·å­©éå¸¸åæ‚”ï¼Œä»–å†³å®šä»æ­¤ä»¥åä¸å†åƒç³–äº†ã€‚ç»è¿‡ä¸€æ®µæ—¶é—´çš„æŠ¤ç†ï¼Œä»–çš„ç‰™é½¿æ…¢æ…¢å¥½è½¬ï¼Œå˜å¾—æ›´åŠ å¥åº·ã€‚ä»æ­¤ä»¥åï¼Œä»–ä¹Ÿå­¦ä¼šäº†æ§åˆ¶è‡ªå·±çš„é£Ÿ', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nã€Šé™å¤œæ€ã€‹\nåºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\nä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰æœ‰ä¸€ä¸ªå°ç”·å­©ï¼Œä»–éå¸¸å–œæ¬¢è·‘æ­¥ï¼Œæ¯å¤©éƒ½ä¼šè·‘å»å…¬å›­é‡Œç»ƒä¹ ã€‚æœ‰ä¸€å¤©ï¼Œä»–çªç„¶å‘ç°ä¸€åªå°é¸Ÿçš„è…¿å—ä¼¤äº†ï¼Œæ— æ³•é£è¡Œã€‚å°ç”·å­©å¿ƒé‡Œå¾ˆéš¾è¿‡ï¼Œäºæ˜¯ä»–å†³å®šæ¯å¤©å¸®åŠ©å°é¸Ÿæ‰¾é£Ÿç‰©ï¼Œç›´åˆ°å®ƒçš„è…¿å¥½äº†ä¸ºæ­¢ã€‚ç»è¿‡ä¸€æ®µæ—¶é—´çš„ç…§é¡¾ï¼Œå°é¸Ÿçš„è…¿ç»ˆäºå®Œå…¨åº·å¤äº†ï¼Œå®ƒä¹Ÿèƒ½å¤Ÿé£å›å¤©ç©ºäº†ã€‚å°ç”·å­©å¾ˆé«˜å…´ï¼Œä½†æ˜¯å°é¸Ÿå´ä¸æƒ³ç¦»å¼€ä»–ï¼Œå®ƒæ¯å¤©éƒ½ä¼šæ¥å…¬å›­å’Œå°ç”·å­©ä¸€èµ·è·‘æ­¥ã€‚ä»æ­¤ä»¥åï¼Œå°ç”·å­©å’Œå°é¸Ÿæˆä¸ºäº†æœ€å¥½çš„æœ‹å‹ï¼Œä¸€èµ·äº«å—ç€è·‘æ­¥å’Œè‡ªç”±çš„å¿«ä¹ã€‚å°ç”·å­©ä¹Ÿæ˜ç™½äº†', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nã€Šæ˜¥æ™“ã€‹- å­Ÿæµ©ç„¶\n\næ˜¥çœ ä¸è§‰æ™“ï¼Œ\nå¤„å¤„é—»å•¼é¸Ÿã€‚\nå¤œæ¥é£é›¨å£°ï¼Œ\nèŠ±è½çŸ¥å¤šå°‘ã€‚\n\nå¤©è¡—å°é›¨æ¶¦å¦‚é…¥ï¼Œ\nè‰è‰²é¥çœ‹è¿‘å´æ— ã€‚\næœ€æ˜¯ä¸€å¹´æ˜¥å¥½å¤„ï¼Œ\nç»èƒœçƒŸæŸ³æ»¡çš‡éƒ½ã€‚', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\nä»å‰æœ‰ä¸€ä½è€äººï¼Œæ¯å¤©éƒ½ä¼šåœ¨å…¬å›­é‡Œæ•£æ­¥ã€‚ä»–å–œæ¬¢è§‚å¯Ÿå‘¨å›´çš„ä¸€åˆ‡ï¼Œå°¤å…¶æ˜¯åŠ¨ç‰©ä»¬ã€‚æœ‰ä¸€å¤©ï¼Œä»–å‘ç°ä¸€åªå°é¸Ÿæ‘”æ–­äº†ç¿…è†€ï¼Œæ— æ³•é£è¡Œã€‚è€äººç«‹åˆ»å¿ƒç”Ÿæ€œæ‚¯ï¼Œå°†å°é¸Ÿå¸¦å›å®¶ï¼Œç»™å®ƒåŒ…æ‰ä¼¤å£ï¼Œå¹¶ç»†å¿ƒåœ°å–‚é£Ÿã€‚\n\nç»è¿‡å‡ å¤©çš„ç…§é¡¾ï¼Œå°é¸Ÿçš„ä¼¤å£æ¸æ¸æ„ˆåˆï¼Œå®ƒä¹Ÿæ¢å¤äº†é£è¡Œèƒ½åŠ›ã€‚ä½†æ˜¯ï¼Œå½“è€äººæ‰“å¼€çª—æˆ·ï¼Œè®©å°é¸Ÿè‡ªç”±é£å‡ºå»æ—¶ï¼Œå®ƒå´ç•™åœ¨äº†è€äººçš„å®¶ä¸­ï¼Œä¸è‚¯ç¦»å¼€ã€‚\n\nè€äººæ„Ÿåˆ°ååˆ†å›°æƒ‘ï¼Œäºæ˜¯ä»–å†³å®šå¸¦ç€å°é¸Ÿå»å…¬å›­ï¼Œå¸Œæœ›èƒ½è®©å®ƒé‡æ–°å›åˆ°è‡ªç„¶ä¸­ã€‚çœ‹ç€å°', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\nã€Šç™»é¹³é›€æ¥¼ã€‹\nä½œè€…ï¼šç‹ä¹‹æ¶£\n\nç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚\næ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰æœ‰ä¸€åªå°çŒ«ï¼Œå®ƒæ˜¯ä¸€åªéå¸¸è°ƒçš®çš„å°çŒ«ã€‚æ¯å¤©å®ƒéƒ½ä¼šè·‘æ¥è·‘å»ï¼Œæ£è›‹é—¹äº‹ï¼Œè®©ä¸»äººéå¸¸å¤´ç–¼ã€‚ä¸»äººæ˜¯ä¸€ä½è€äººï¼Œå¥¹å¯¹å°çŒ«éå¸¸å®½å®¹ï¼Œæ€»æ˜¯å® çˆ±å®ƒã€‚æœ‰ä¸€å¤©ï¼Œå°çŒ«å·å·æºœå‡ºå®¶é—¨ï¼Œæ¥åˆ°äº†ä¸€åº§èŠ±å›­ã€‚èŠ±å›­é‡Œæœ‰ä¸€åªç¾ä¸½çš„å°é¸Ÿï¼Œå°çŒ«éå¸¸æƒ³æŠ“åˆ°å®ƒã€‚å®ƒè¹²åœ¨èŠ±ä¸›ä¸­ï¼Œæ‚„æ‚„åœ°ç­‰å¾…ç€ã€‚å¯æ˜¯å°é¸Ÿå´é£åˆ°äº†æ ‘ä¸Šï¼Œå°çŒ«è¿½äº†ä¸Šå»ï¼Œå´ä¸å°å¿ƒæ‰è¿›äº†ä¸€å£äº•é‡Œã€‚å®ƒå®³æ€•åœ°å–µå–µå«ï¼Œä½†æ˜¯æ²¡æœ‰äººå¬è§ã€‚å°±åœ¨è¿™æ—¶ï¼Œä¸»äººæ¥åˆ°äº†èŠ±å›­ï¼Œå¥¹å¬åˆ°äº†', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nã€Šé™å¤œæ€ã€‹\nåºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\nä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nå°æ˜æ˜¯ä¸€ä½å¿«ä¹çš„å°ç”·å­©ï¼Œä»–ä½åœ¨ä¸€ä¸ªç¾ä¸½çš„å°æ‘åº„é‡Œã€‚æ¯å¤©ï¼Œä»–éƒ½ä¼šå’Œå°ä¼™ä¼´ä»¬ä¸€èµ·åœ¨æ‘å­é‡Œç©è€ï¼Œä¸€èµ·æ¢ç´¢å‘¨å›´çš„å¤§è‡ªç„¶ã€‚ä»–ä»¬å¸¸å¸¸ä¼šåœ¨ç”°é‡é‡Œæ‰è¿·è—ï¼Œæˆ–è€…åœ¨æ²³è¾¹æ‰é±¼è™¾ã€‚å°æ˜æœ€å–œæ¬¢çš„æ˜¯å’Œçˆ·çˆ·ä¸€èµ·å»å±±é‡Œé‡‡è‰è¯ã€‚çˆ·çˆ·æ€»æ˜¯ä¼šç»™ä»–è®²è®¸å¤šå…³äºæ¤ç‰©çš„çŸ¥è¯†ï¼Œå°æ˜ä¹Ÿå› æ­¤å¯¹è‡ªç„¶äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚\n\næœ‰ä¸€å¤©ï¼Œå°æ˜å’Œå°ä¼™ä¼´ä»¬åœ¨æ²³è¾¹ç©è€æ—¶ï¼Œå‘ç°äº†ä¸€åªå—ä¼¤çš„å°é¹¿ã€‚å°æ˜ç«‹åˆ»å¸¦ç€å°ä¼™ä¼´ä»¬ä¸€èµ·å»æ‰¾çˆ·çˆ·å¸®å¿™ã€‚çˆ·çˆ·çœ‹åˆ°å°é¹¿å—ä¼¤çš„', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nã€Šç™»é¹³é›€æ¥¼ã€‹\n\nç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚\næ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰ï¼Œæœ‰ä¸€ä½èªæ˜çš„å°å¥³å­©ï¼Œå¥¹éå¸¸å–œæ¬¢æ¢ç´¢å’Œå‘ç°æ–°äº‹ç‰©ã€‚ä¸€å¤©ï¼Œå¥¹å‘ç°äº†ä¸€åªå°é¸Ÿï¼Œå®ƒçš„ç¾½æ¯›éå¸¸æ¼‚äº®ï¼Œä½†æ˜¯å´å—äº†ä¼¤ï¼Œä¸èƒ½é£è¡Œã€‚å°å¥³å­©å¿ƒé‡Œå¾ˆéš¾è¿‡ï¼Œä¾¿å†³å®šå¸®åŠ©å®ƒã€‚å¥¹ç”¨è‡ªå·±çš„å°æ‰‹æŒ‡åšäº†ä¸€ä¸ªå°ç»·å¸¦ï¼ŒæŠŠå°é¸Ÿçš„ä¼¤å£åŒ…æ‰å¥½ï¼Œè¿˜ç»™å®ƒå–‚é£Ÿã€‚ç»è¿‡å‡ å¤©çš„ç…§é¡¾ï¼Œå°é¸Ÿçš„ä¼¤å£æ„ˆåˆäº†ï¼Œå®ƒä¹Ÿæ¢å¤äº†å¥åº·ï¼Œå¯ä»¥è‡ªç”±åœ°é£ç¿”äº†ã€‚å°å¥³å­©çœ‹ç€å°é¸Ÿé£èµ°ï¼Œå¿ƒé‡Œæ„Ÿåˆ°éå¸¸æ»¡è¶³å’Œå¿«ä¹ã€‚ä»æ­¤ä»¥åï¼Œå°å¥³å­©æ›´åŠ çƒ­çˆ±å¤§è‡ªç„¶ï¼Œä¹Ÿå­¦ä¼šäº†å…³çˆ±ç”Ÿå‘½ï¼Œç”¨', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nã€Šé™å¤œæ€ã€‹ - æç™½\nåºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚\nä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰æœ‰ä¸€åªå°é¸Ÿï¼Œå®ƒå«å°çº¢ã€‚å°çº¢æ˜¯ä¸€ä¸ªéå¸¸å‹‡æ•¢çš„å°é¸Ÿï¼Œå®ƒæœ‰ç€ç¾ä¸½çš„ç¾½æ¯›å’Œçµå·§çš„ç¿…è†€ã€‚ä¸€å¤©ï¼Œå°çº¢é£åˆ°äº†ä¸€ä¸ªé™Œç”Ÿçš„åœ°æ–¹ï¼Œå®ƒæƒ³è¦æ¢ç´¢ä¸€ä¸‹è¿™ä¸ªåœ°æ–¹ã€‚ä½†æ˜¯ï¼Œå°çº¢å´é‡åˆ°äº†ä¸€åªå‡¶æ¶çš„è€é¹°ï¼Œå®ƒæ­£ç›¯ç€å°çº¢å‡†å¤‡è¦åƒæ‰å®ƒã€‚å°çº¢éå¸¸å®³æ€•ï¼Œä½†æ˜¯å®ƒå¹¶æ²¡æœ‰æ”¾å¼ƒï¼Œå®ƒæƒ³è¦ç”¨è‡ªå·±çš„å‹‡æ°”å’Œæ™ºæ…§æ¥åº”å¯¹è€é¹°ã€‚\n\nå°çº¢æƒ³åˆ°äº†ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå®ƒå¼€å§‹å”±æ­Œï¼Œç”¨ç¾å¦™çš„æ­Œå£°æ¥æ‰“åŠ¨è€é¹°ã€‚è€é¹°è¢«å°çº¢çš„æ­Œå£°å¸å¼•ä½äº†ï¼Œåœæ­¢äº†æ”»å‡»ã€‚å°çº¢è¶', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='å§\n\nã€Šç™»é¹³é›€æ¥¼ã€‹- ç‹ä¹‹æ¶£\n\nç™½æ—¥ä¾å±±å°½ï¼Œé»„æ²³å…¥æµ·æµã€‚\næ¬²ç©·åƒé‡Œç›®ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚\n', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\nä»å‰æœ‰ä¸€ä¸ªå°å§‘å¨˜ï¼Œå¥¹éå¸¸å–œæ¬¢ç”»ç”»ï¼Œæ¯å¤©éƒ½ä¼šæ‹¿èµ·ç”»ç¬”å’Œé¢œæ–™ï¼Œç”»å‡ºå„ç§ç¾ä¸½çš„å›¾æ¡ˆã€‚å¥¹çš„çˆ¶æ¯éå¸¸æ”¯æŒå¥¹ï¼Œæ¯æ¬¡çœ‹åˆ°å¥¹çš„ä½œå“éƒ½ä¼šè¡¨æ‰¬å¥¹ã€‚å°å§‘å¨˜ä¹Ÿå¾ˆå–„è‰¯ï¼Œç»å¸¸ä¼šæŠŠè‡ªå·±çš„ä½œå“é€ç»™é‚»å±…å’Œæœ‹å‹ã€‚\n\nä¸€å¤©ï¼Œå°å§‘å¨˜å¬è¯´æ‘å­é‡Œæœ‰ä¸€ä½è€å¥¶å¥¶ç”Ÿç—…äº†ï¼Œå®¶é‡Œæ²¡æœ‰äººèƒ½ç…§é¡¾å¥¹ã€‚å¥¹å†³å®šç”¨è‡ªå·±çš„ç”»ä½œæ¢å–ä¸€äº›ç²¥æ°´ï¼Œå¸®åŠ©è€å¥¶å¥¶ç…§é¡¾å¥¹ã€‚è€å¥¶å¥¶éå¸¸æ„ŸåŠ¨ï¼Œå¥¹æŠŠå°å§‘å¨˜çš„ç”»ä½œæŒ‚åœ¨å¢™ä¸Šï¼Œæ¯å¤©éƒ½ä¼šçœ‹ç€å®ƒï¼Œå¿ƒé‡Œæš–æš–çš„ã€‚\n\nå°å§‘å¨˜ä¹Ÿç»å¸¸å»çœ‹æœ›è€', generation_info={'finish_reason': 'length', 'logprobs': None})], [Generation(text='\n\nå…³å±±æœˆ\n\næ˜æœˆå‡ºå¤©å±±ï¼Œè‹èŒ«äº‘æµ·é—´ã€‚\né•¿é£å‡ ä¸‡é‡Œï¼Œå¹åº¦ç‰é—¨å…³ã€‚\næ±‰ä¸‹ç™½ç™»é“ï¼Œèƒ¡çª¥é’æµ·æ¹¾ã€‚\nç”±æ¥å¾æˆ˜åœ°ï¼Œä¸è§æœ‰äººè¿˜ã€‚\næˆå®¢æœ›è¾¹è‰²ï¼Œæ€å½’å¤šè‹¦é¢œã€‚\né«˜æ¥¼å½“æ­¤å¤œï¼Œå¹æ¯æœªåº”é—²ã€‚\nå¤©é˜¶å¤œè‰²å¯’ï¼Œé“¶æ±‰é¸¦å•¼çœ ã€‚\nä½†æ„¿äººé•¿ä¹…ï¼Œåƒé‡Œå…±å©µå¨Ÿã€‚', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\n\næœ‰ä¸€åªå°æ¾é¼ ï¼Œå®ƒéå¸¸å–œæ¬¢æ”¶é›†æ ‘ä¸Šçš„æ¾æœã€‚æ¯å¤©æ—©ä¸Šï¼Œå®ƒéƒ½ä¼šè·³ä¸Šæ ‘æï¼Œå¼€å§‹æ”¶é›†æ¾æœã€‚å®ƒæŠŠæ¾æœæ”¾åœ¨ä¸€ä¸ªå°æ´é‡Œï¼Œä½œä¸ºè‡ªå·±çš„è´¢å¯Œã€‚\n\næœ‰ä¸€å¤©ï¼Œå°æ¾é¼ é‡åˆ°äº†ä¸€åªé¸Ÿå„¿ï¼Œå®ƒçš„ç¿…è†€å—ä¼¤æ— æ³•é£è¡Œã€‚å°æ¾é¼ çœ‹åˆ°å®ƒè½åœ¨åœ°ä¸Šï¼Œç«‹åˆ»è·‘è¿‡å»å¸®åŠ©å®ƒã€‚å®ƒç”¨çˆªå­æŠ“ä½ä¸€æ ¹æ ‘æï¼Œè®©é¸Ÿå„¿ç«™åœ¨ä¸Šé¢ï¼Œç„¶åå¸¦å®ƒé£åˆ°æ ‘ä¸Šã€‚é¸Ÿå„¿éå¸¸æ„Ÿæ¿€å°æ¾é¼ çš„å¸®åŠ©ï¼Œå®ƒå†³å®šç”¨è‡ªå·±çš„æ­Œå£°æ¥å›æŠ¥å°æ¾é¼ ã€‚\n\nä»æ­¤ï¼Œå°æ¾é¼ å’Œ', generation_info={'finish_reason': 'length', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 230, 'completion_tokens': 3187, 'total_tokens': 3417}, 'model_name': 'gpt-3.5-turbo-instruct'} run=[RunInfo(run_id=UUID('96b87007-d652-4760-ba2f-9c3051238a24')), RunInfo(run_id=UUID('57995714-f640-41cf-9815-5a156fb683dd')), RunInfo(run_id=UUID('1de577e8-156a-49f6-892a-7da44386f149')), RunInfo(run_id=UUID('8d37b464-19a0-4522-b058-1ad4ff89eac3')), RunInfo(run_id=UUID('c0185803-86fc-4140-a097-5ccf65a3ca78')), RunInfo(run_id=UUID('6efb722a-935f-4ec5-8bf7-e2505429e19a')), RunInfo(run_id=UUID('9debff07-bf16-46da-9a76-b057f2b2f98c')), RunInfo(run_id=UUID('56207be2-7dde-44d7-98ab-fbcebf66b96d')), RunInfo(run_id=UUID('3ad9e5b6-815b-4228-975f-31c6482450f3')), RunInfo(run_id=UUID('ec052a8a-da81-46ae-822e-21e0930cb782')), RunInfo(run_id=UUID('b62455f7-58e7-452d-b9de-5e1c7bd68389')), RunInfo(run_id=UUID('48b0afca-9ee5-4408-8b80-cf0f805b87d9')), RunInfo(run_id=UUID('bf36e293-71e8-46bf-86c8-3a7e60f15d7d')), RunInfo(run_id=UUID('c523593e-c1dd-419e-8668-c428eb5615f2')), RunInfo(run_id=UUID('f4518e12-d716-4f94-8b2d-9d20fea9b194')), RunInfo(run_id=UUID('ba9d9cab-6a73-40e7-8b10-8e23e8d5cc1e')), RunInfo(run_id=UUID('3711938d-5df1-4ebb-a8db-91f7c013a4d3')), RunInfo(run_id=UUID('a3d1c05f-6fc1-4df8-9dd3-da9ebf4f18d0')), RunInfo(run_id=UUID('d9ad44a4-a519-4916-be2a-d17dd8155f14')), RunInfo(run_id=UUID('361c9490-67b4-40d1-aba2-cb7880975429'))] type='LLMResult'



```python
import os, time, asyncio, openai
from langchain.llms import OpenAI

# ---------- åŒæ­¥è°ƒç”¨ ----------
def generate_sync(n: int = 10):
    llm = OpenAI(temperature=0.9)
    t0 = time.perf_counter()
    for _ in range(n):
        resp = llm.generate(["Hello, how are you?"])
        print(resp.generations[0][0].text.strip())
    dt = time.perf_counter() - t0
    print(f"\nåŒæ­¥æ¨¡å¼è€—æ—¶ï¼š{dt:.2f} ç§’")

# ---------- å¼‚æ­¥è°ƒç”¨ ----------
async def async_generate_once(llm):
    resp = await llm.agenerate(["Hello, how are you?"])
    print(resp.generations[0][0].text.strip())

async def generate_async(n: int = 10):
    llm = OpenAI(temperature=0.9)
    t0 = time.perf_counter()
    tasks = [async_generate_once(llm) for _ in range(n)]
    await asyncio.gather(*tasks)
    dt = time.perf_counter() - t0
    print(f"\nå¼‚æ­¥å¹¶å‘è€—æ—¶ï¼š{dt:.2f} ç§’")

# ---------- è¿è¡Œå¯¹æ¯” ----------
if __name__ == "__main__":
    print("=== åŒæ­¥è°ƒç”¨ ===")
    generate_sync()

    print("\n=== å¼‚æ­¥è°ƒç”¨ ===")
    await generate_async()
```

    === åŒæ­¥è°ƒç”¨ ===
    I am an AI and do not have emotions, but thank you for asking. How can I assist you?
    I'm an AI and I don't experience emotions, but thank you for asking. Is there something I can assist you with today?
    I'm an AI language model, so I don't have feelings or emotions. But I am functioning well, thank you for asking. How about you?
    I am an AI, I do not have physical or emotional capabilities, but thank you for asking. Is there something you would like to talk about?
    I'm an AI and have no emotions, but thank you for asking! How may I assist you today?
    I am an AI, so I do not have emotions. But thank you for asking. How may I assist you?
    I am an AI and do not have emotions, but thank you for asking. How can I assist you?
    I am an AI and do not have emotions, but thank you for asking. How can I assist you today?
    I am an AI and cannot feel emotions, but thank you for asking. How can I assist you?
    I am an AI and don't have the ability to feel emotions. But thank you for asking! How can I assist you today?
    
    åŒæ­¥æ¨¡å¼è€—æ—¶ï¼š16.55 ç§’
    
    === å¼‚æ­¥è°ƒç”¨ ===
    I am an AI so I do not have emotions, but thank you for asking. How can I assist you today?
    I am an AI and do not have the ability to feel emotions. How can I assist you?
    I am an AI, I do not have emotions like humans do, but thank you for asking. How can I assist you?
    I am a language model created by OpenAI and I do not have feelings. But thank you for asking, how can I assist you?
    I'm doing well, thank you. How about you?
    I'm an AI so I don't have feelings like humans, but thank you for asking! How can I assist you?
    I'm doing well, thank you for asking. How about you?
    I am an AI and do not have emotions, but thank you for asking. How can I assist you today?
    I am an AI language model. I do not have emotions but thank you for asking. Is there anything I can assist you with?
    I am an AI language model created by OpenAI. I do not have emotions, but thank you for asking. How can I assist you today?
    
    å¼‚æ­¥å¹¶å‘è€—æ—¶ï¼š4.21 ç§’


#### è‡ªå®šä¹‰LLM

åœ¨å¼€å‘è¿‡ç¨‹ä¸­å¦‚æœé‡åˆ°éœ€è¦è°ƒç”¨ä¸åŒçš„LLMæ—¶ï¼Œå¯ä»¥é€šè¿‡è‡ªå®šä¹‰LLMå®ç°æ•ˆç‡çš„æé«˜ã€‚

è‡ªå®šä¹‰LLMæ—¶ï¼Œå¿…é¡»è¦å®ç°çš„æ˜¯_callæ–¹æ³•ï¼Œé€šè¿‡è¿™ä¸ªæ–¹æ³•æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ã€ä¸€äº›å¯é€‰çš„ç´¢å¼•å­—ï¼Œå¹¶æœ€ç»ˆè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

é™¤äº†è¯¥æ–¹æ³•ä¹‹å¤–ï¼Œè¿˜å¯ä»¥é€‰æ‹©æ€§ç”Ÿæˆä¸€äº›æ–¹æ³•ç”¨äºä»¥å­—å…¸çš„æ¨¡å¼è¿”å›è¯¥è‡ªå®šä¹‰LLMç±»çš„å„å±æ€§ã€‚


```python
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from typing import Optional, List, Any, Mapping

class CustomLLM(LLM):
    """
    ä¸€ä¸ªæœ€å°ç¤ºèŒƒ LLMï¼šåªä¼šæŠŠè¾“å…¥ prompt æˆªæ–­åˆ°å‰ n ä¸ªå­—ç¬¦è¿”å›ã€‚
    ç»§æ‰¿è‡ª LangChain çš„ LLM æŠ½è±¡ç±»ï¼Œå› è€Œå¯ä¸é“¾ã€ä»£ç†ç­‰ç»„ä»¶æ— ç¼é…åˆã€‚
    """

    n: int  # æ§åˆ¶è¿”å›é•¿åº¦çš„å‚æ•°ï¼Œä¾‹å¦‚ n=10 å°±åªä¿ç•™å‰ 10 ä¸ªå­—ç¬¦

    # ---- å¿…éœ€æ¥å£ 1ï¼šå£°æ˜æ¨¡å‹ç±»å‹ ----
    @property
    def _llm_type(self) -> str:
        return "custom"

    # ---- å¿…éœ€æ¥å£ 2ï¼šçœŸæ­£æ‰§è¡Œæ¨ç†çš„å‡½æ•° ----
    def _call(
        self,
        prompt: str,                                # è¾“å…¥æ–‡æœ¬
        stop: Optional[List[str]] = None,           # ä¸æ”¯æŒ stopï¼Œå› æ­¤è‹¥ä¼ å…¥å°†æŠ¥é”™
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        if stop is not None:
            raise ValueError("stop kwargs are not permitted.")
        return prompt[: self.n]                     # ç®€å•åœ°æˆªæ–­å¹¶è¿”å›

    # ---- å¿…éœ€æ¥å£ 3ï¼šç”¨äºç¼“å­˜ä¸å¤ç°çš„â€œæ¨¡å‹å‚æ•°â€ ----
    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        # è®© LangChain çŸ¥é“ï¼šå‚æ•°åªæœ‰ n
        return {"n": self.n}
    

toy_llm = CustomLLM(n=8)
print(toy_llm("LangChain Makes LLM Apps Easy!"))
```

    LangChai


#### æµ‹è¯•LLM

ä¸ºäº†èŠ‚çœæˆ‘ä»¬çš„æˆæœ¬ï¼Œå½“å†™å¥½ä¸€ä¸²ä»£ç è¿›è¡Œæµ‹è¯•çš„æ—¶å€™ï¼Œé€šå¸¸æƒ…å†µä¸‹æˆ‘ä»¬æ˜¯ä¸å¸Œæœ›å»çœŸæ­£è°ƒç”¨LLMï¼Œå› ä¸ºè¿™ä¼šæ¶ˆè€—tokenã€‚

Langchainåˆ™æä¾›ç»™æˆ‘ä»¬ä¸€ä¸ªâ€œå‡çš„â€å¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥æ–¹ä¾¿æˆ‘ä»¬è¿›è¡Œæµ‹è¯•ã€‚


```python
# ä»langchain.llms.fakeæ¨¡å—å¯¼å…¥FakeListLLMç±»ï¼Œæ­¤ç±»å¯ç”¨äºæ¨¡æ‹Ÿæˆ–ä¼ªé€ æŸç§è¡Œä¸º
from langchain.llms.fake import FakeListLLM
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain_experimental.tools import PythonREPLTool

# åŠ è½½"python_repl"çš„å·¥å…·
tools = [PythonREPLTool()]

# å®šä¹‰ä¸€ä¸ªå“åº”åˆ—è¡¨ï¼Œè¿™äº›å“åº”æ˜¯æ¨¡æ‹ŸLLMçš„é¢„æœŸå“åº”
responses = ["Action: Python REPL\nAction Input: print(2 + 2)", "Final Answer: 4"]

# ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„responsesåˆå§‹åŒ–ä¸€ä¸ªFakeListLLMå¯¹è±¡
llm = FakeListLLM(responses=responses)

# è°ƒç”¨initialize_agentå‡½æ•°ï¼Œä½¿ç”¨ä¸Šé¢çš„toolså’Œllmï¼Œä»¥åŠæŒ‡å®šçš„ä»£ç†ç±»å‹å’Œverboseå‚æ•°æ¥åˆå§‹åŒ–ä¸€ä¸ªä»£ç†
agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

# è°ƒç”¨ä»£ç†çš„runæ–¹æ³•ï¼Œä¼ é€’å­—ç¬¦ä¸²"whats 2 + 2"ä½œä¸ºè¾“å…¥ï¼Œè¯¢é—®ä»£ç†2åŠ 2çš„ç»“æœ
agent.run("whats 2 + 2")
```

    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3mAction: Python REPL
    Action Input: print(2 + 2)[0m
    Observation: Python REPL is not a valid tool, try one of [Python_REPL].
    Thought:[32;1m[1;3mFinal Answer: 4[0m
    
    [1m> Finished chain.[0m


    /var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_71916/3086584864.py:15: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.
      agent = initialize_agent(
    /var/folders/z5/r_6tsnmx7bjcnqbgkmfp22540000gn/T/ipykernel_71916/3086584864.py:19: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.
      agent.run("whats 2 + 2")





    '4'



ä¸æ¨¡æ‹ŸllmåŒç†ï¼Œlangchainä¹Ÿæä¾›äº†ä¸€ä¸ªä¼ªç±»å»æ¨¡æ‹Ÿäººç±»å›å¤ï¼Œè¯¥åŠŸèƒ½ä¾èµ–äºwikipediaã€‚


```python
# ä»langchain.llms.humanæ¨¡å—å¯¼å…¥HumanInputLLMç±»ï¼Œæ­¤ç±»å¯èƒ½å…è®¸äººç±»è¾“å…¥æˆ–äº¤äº’æ¥æ¨¡æ‹ŸLLMçš„è¡Œä¸º
from langchain.llms.human import HumanInputLLM
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType

# è°ƒç”¨load_toolså‡½æ•°ï¼ŒåŠ è½½åä¸º"wikipedia"çš„å·¥å…·
tools = load_tools(["wikipedia"])

# åˆå§‹åŒ–ä¸€ä¸ªHumanInputLLMå¯¹è±¡ï¼Œå…¶ä¸­prompt_funcæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ‰“å°æç¤ºä¿¡æ¯
llm = HumanInputLLM(
    prompt_func=lambda prompt: print(f"\n===PROMPT====\n{prompt}\n=====END OF PROMPT======"))

# è°ƒç”¨initialize_agentå‡½æ•°ï¼Œä½¿ç”¨ä¸Šé¢çš„toolså’Œllmï¼Œä»¥åŠæŒ‡å®šçš„ä»£ç†ç±»å‹å’Œverboseå‚æ•°æ¥åˆå§‹åŒ–ä¸€ä¸ªä»£ç†
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# è°ƒç”¨ä»£ç†çš„runæ–¹æ³•ï¼Œä¼ é€’å­—ç¬¦ä¸²"What is 'Bocchi the Rock!'?"ä½œä¸ºè¾“å…¥ï¼Œè¯¢é—®ä»£ç†å…³äº'Bocchi the Rock!'çš„ä¿¡æ¯
agent.run("What is 'Bocchi the Rock!'?")
```

#### ç¼“å­˜LLM

å’Œæµ‹è¯•å¤§è¯­è¨€æ¨¡å‹å…·æœ‰ä¸€æ ·æ•ˆæœçš„æ˜¯ç¼“å­˜å¤§è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ç¼“å­˜å±‚å¯ä»¥å°½å¯èƒ½çš„å‡å°‘APIçš„è°ƒç”¨æ¬¡æ•°ï¼Œä»è€ŒèŠ‚çœè´¹ç”¨ã€‚

åœ¨Langchainä¸­è®¾ç½®ç¼“å­˜åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼šä¸€æ˜¯åœ¨å†…å­˜ä¸­è®¾ç½®ç¼“å­˜ï¼ŒäºŒæ˜¯åœ¨æ•°æ®ä¸­è®¾ç½®ç¼“å­˜ã€‚å­˜å‚¨åœ¨å†…å­˜ä¸­åŠ è½½é€Ÿåº¦è¾ƒå¿«ï¼Œä½†æ˜¯å ç”¨èµ„æºå¹¶ä¸”åœ¨å…³æœºä¹‹åå°†ä¸å†è¢«ç¼“å­˜ã€‚


```python
# ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼šPredict method took 3.4563 seconds to execute.
# ç¬¬äºŒæ¬¡è°ƒç”¨æ—¶ï¼šPredict method took 0.0007 seconds to execute.

from langchain.cache import InMemoryCache
import langchain
from langchain.llms import OpenAI
import time

langchain.llm_cache = InMemoryCache()

llm = ChatOpenAI(model_name="gpt-3.5-turbo")

start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
print(llm.predict("ç”¨ä¸­æ–‡è®²ä¸ªç¬‘è¯"))
end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
elapsed_time = end_time - start_time  # è®¡ç®—æ€»æ—¶é—´
print(f"Predict method took {elapsed_time:.4f} seconds to execute.")

start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
print(llm.predict("ç”¨ä¸­æ–‡è®²ä¸ªç¬‘è¯"))
end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
elapsed_time = end_time - start_time  # è®¡ç®—æ€»æ—¶é—´
print(f"Predict method took {elapsed_time:.4f} seconds to execute.")
```

    æœ‰ä¸€å¤©ï¼Œä¸€åªå°çŒªèµ°è¿›äº†ä¸€ä¸ªé¤å…ï¼Œåä¸‹åå¯¹æœåŠ¡å‘˜è¯´ï¼šâ€œæˆ‘æƒ³ç‚¹èœã€‚â€  
    æœåŠ¡å‘˜å¥‡æ€ªåœ°é—®ï¼šâ€œçŒªæ€ä¹ˆä¼šåœ¨è¿™é‡Œç‚¹èœï¼Ÿâ€  
    å°çŒªè‡ªä¿¡åœ°å›ç­”ï¼šâ€œå› ä¸ºæˆ‘æœ‰â€˜çŒªâ€™çš„å“å‘³ï¼â€  
    
    æœåŠ¡å‘˜å¿ä¸ä½ç¬‘äº†ï¼šâ€œé‚£ä½ æƒ³åƒä»€ä¹ˆï¼Ÿâ€  
    å°çŒªçœ¨äº†çœ¨çœ¼ç›ï¼Œè¯´ï¼šâ€œæˆ‘æƒ³è¦ä¸€ä¸ªâ€˜ä¸å€’ç¿â€™çš„æ²™æ‹‰ï¼â€  
    
    æœåŠ¡å‘˜æ„£äº†ä¸€ä¸‹ï¼šâ€œä¸ºä»€ä¹ˆè¦ç‚¹è¿™ä¸ªï¼Ÿâ€  
    å°çŒªè¯´ï¼šâ€œå› ä¸ºå®ƒæœ‰â€˜æ‘‡æ‘‡â€™çš„æ„Ÿè§‰ï¼Œå’Œæˆ‘ä¸€æ ·å¯çˆ±ï¼â€  
    
    æ„Ÿè§‰è¿™åªå°çŒªçœŸæ˜¯ä¸ªå¤§å¹½é»˜å®¶ï¼
    Predict method took 3.4563 seconds to execute.
    æœ‰ä¸€å¤©ï¼Œä¸€åªå°çŒªèµ°è¿›äº†ä¸€ä¸ªé¤å…ï¼Œåä¸‹åå¯¹æœåŠ¡å‘˜è¯´ï¼šâ€œæˆ‘æƒ³ç‚¹èœã€‚â€  
    æœåŠ¡å‘˜å¥‡æ€ªåœ°é—®ï¼šâ€œçŒªæ€ä¹ˆä¼šåœ¨è¿™é‡Œç‚¹èœï¼Ÿâ€  
    å°çŒªè‡ªä¿¡åœ°å›ç­”ï¼šâ€œå› ä¸ºæˆ‘æœ‰â€˜çŒªâ€™çš„å“å‘³ï¼â€  
    
    æœåŠ¡å‘˜å¿ä¸ä½ç¬‘äº†ï¼šâ€œé‚£ä½ æƒ³åƒä»€ä¹ˆï¼Ÿâ€  
    å°çŒªçœ¨äº†çœ¨çœ¼ç›ï¼Œè¯´ï¼šâ€œæˆ‘æƒ³è¦ä¸€ä¸ªâ€˜ä¸å€’ç¿â€™çš„æ²™æ‹‰ï¼â€  
    
    æœåŠ¡å‘˜æ„£äº†ä¸€ä¸‹ï¼šâ€œä¸ºä»€ä¹ˆè¦ç‚¹è¿™ä¸ªï¼Ÿâ€  
    å°çŒªè¯´ï¼šâ€œå› ä¸ºå®ƒæœ‰â€˜æ‘‡æ‘‡â€™çš„æ„Ÿè§‰ï¼Œå’Œæˆ‘ä¸€æ ·å¯çˆ±ï¼â€  
    
    æ„Ÿè§‰è¿™åªå°çŒªçœŸæ˜¯ä¸ªå¤§å¹½é»˜å®¶ï¼
    Predict method took 0.0007 seconds to execute.


é™¤äº†å­˜å‚¨åœ¨å†…å­˜ä¸­è¿›è¡Œç¼“å­˜ï¼Œä¹Ÿå¯ä»¥å­˜å‚¨åœ¨æ•°æ®åº“ä¸­è¿›è¡Œç¼“å­˜ï¼Œå½“å¼€å‘ä¼ä¸šçº§åº”ç”¨çš„æ—¶å€™é€šå¸¸éƒ½ä¼šé€‰æ‹©å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ã€‚

è¿™ç§æ–¹å¼çš„åŠ è½½é€Ÿåº¦ç›¸è¾ƒäºå°†ç¼“å­˜å­˜å‚¨åœ¨å†…å­˜ä¸­æ›´æ…¢ä¸€äº›ï¼Œå¥½å¤„æ˜¯ä¸å ç”µè„‘èµ„æºï¼Œå¹¶ä¸”å­˜å‚¨è®°å½•å¹¶ä¸ä¼šéšç€å…³æœºæ¶ˆå¤±ã€‚


```python
# ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼šPredict method took 2.9873 seconds to execute.
# ç¬¬äºŒæ¬¡è°ƒç”¨æ—¶ï¼šPredict method took 0.0028 seconds to execute.

from langchain.cache import SQLiteCache
import langchain
from langchain.llms import OpenAI
import time

langchain.llm_cache = SQLiteCache(database_path="langchain.db")

llm = ChatOpenAI(model_name="gpt-3.5-turbo")

start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
print(llm.predict("ç”¨ä¸­æ–‡è®²ä¸ªç¬‘è¯"))
end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
elapsed_time = end_time - start_time  # è®¡ç®—æ€»æ—¶é—´
print(f"Predict method took {elapsed_time:.4f} seconds to execute.")

start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
print(llm.predict("ç”¨ä¸­æ–‡è®²ä¸ªç¬‘è¯"))
end_time = time.time()  # è®°å½•ç»“æŸæ—¶é—´
elapsed_time = end_time - start_time  # è®¡ç®—æ€»æ—¶é—´
print(f"Predict method took {elapsed_time:.4f} seconds to execute.")
```

    æœ‰ä¸€å¤©ï¼Œå°æ˜å»ä¸Šå­¦ï¼Œè€å¸ˆé—®ä»–ï¼šâ€œå°æ˜ï¼Œå¦‚æœä½ æœ‰åä¸ªè‹¹æœï¼Œç»™äº†åˆ«äººå››ä¸ªï¼Œä½ è¿˜å‰©å‡ ä¸ªè‹¹æœï¼Ÿâ€  
    å°æ˜æƒ³äº†æƒ³ï¼Œå›ç­”ï¼šâ€œè€å¸ˆï¼Œæˆ‘è¿˜å‰©ä¸‹å…­ä¸ªï¼â€  
    è€å¸ˆè¯´ï¼šâ€œä¸å¯¹å•Šï¼Œåº”è¯¥æ˜¯å…­ä¸ªã€‚ä½ ä¸ºä»€ä¹ˆä¼šé‚£ä¹ˆç¡®å®šï¼Ÿâ€  
    å°æ˜è‡ªä¿¡åœ°è¯´ï¼šâ€œå› ä¸ºæˆ‘è‹¹æœå¤ªå¤šäº†ï¼Œæˆ‘ä»æ¥æ²¡è¯•è¿‡ç»™äººå®¶è‹¹æœï¼â€  
    
    å“ˆå“ˆï¼Œçœ‹æ¥å°æ˜çš„â€œæ•°å­¦â€é€»è¾‘çœŸæ˜¯ä¸ä¼—ä¸åŒå‘€ï¼
    Predict method took 2.9873 seconds to execute.
    æœ‰ä¸€å¤©ï¼Œå°æ˜å»ä¸Šå­¦ï¼Œè€å¸ˆé—®ä»–ï¼šâ€œå°æ˜ï¼Œå¦‚æœä½ æœ‰åä¸ªè‹¹æœï¼Œç»™äº†åˆ«äººå››ä¸ªï¼Œä½ è¿˜å‰©å‡ ä¸ªè‹¹æœï¼Ÿâ€  
    å°æ˜æƒ³äº†æƒ³ï¼Œå›ç­”ï¼šâ€œè€å¸ˆï¼Œæˆ‘è¿˜å‰©ä¸‹å…­ä¸ªï¼â€  
    è€å¸ˆè¯´ï¼šâ€œä¸å¯¹å•Šï¼Œåº”è¯¥æ˜¯å…­ä¸ªã€‚ä½ ä¸ºä»€ä¹ˆä¼šé‚£ä¹ˆç¡®å®šï¼Ÿâ€  
    å°æ˜è‡ªä¿¡åœ°è¯´ï¼šâ€œå› ä¸ºæˆ‘è‹¹æœå¤ªå¤šäº†ï¼Œæˆ‘ä»æ¥æ²¡è¯•è¿‡ç»™äººå®¶è‹¹æœï¼â€  
    
    å“ˆå“ˆï¼Œçœ‹æ¥å°æ˜çš„â€œæ•°å­¦â€é€»è¾‘çœŸæ˜¯ä¸ä¼—ä¸åŒå‘€ï¼
    Predict method took 0.0028 seconds to execute.


#### è·Ÿè¸ªtokenä½¿ç”¨æƒ…å†µ

åˆ©ç”¨get_openai_callbackå¯å®Œæˆå¯¹äºå•æ¡çš„æé—®æ—¶tokençš„è®°å½•ï¼Œæ­¤å¤–å¯¹äºæœ‰å¤šä¸ªæ­¥éª¤çš„é“¾æˆ–è€…agentï¼Œlangchainä¹Ÿå¯ä»¥è¿½è¸ªåˆ°å„æ­¥éª¤æ‰€è€—è´¹çš„tokenã€‚


```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import OpenAI
from langchain.callbacks import get_openai_callback

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

with get_openai_callback() as cb:
    result = llm("è®²ä¸ªç¬‘è¯")
    print(cb)

with get_openai_callback() as cb:
    response = agent.run("ç‹è²ç°åœ¨çš„å¹´é¾„æ˜¯å¤šå°‘ï¼Ÿ")
    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Prompt Tokens: {cb.prompt_tokens}")
    print(f"Completion Tokens: {cb.completion_tokens}")
    print(f"Total Cost (USD): ${cb.total_cost}")
```

    Tokens Used: 144
    	Prompt Tokens: 5
    		Prompt Tokens Cached: 0
    	Completion Tokens: 139
    		Reasoning Tokens: 0
    Successful Requests: 1
    Total Cost (USD): $0.00028550000000000005
    
    
    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m I need to find out how old Wang Fei is now.
    Action: Calculator
    Action Input: 2021 - 1973[0m
    Observation: [36;1m[1;3mAnswer: 48[0m
    Thought:[32;1m[1;3m I now know the final answer
    Final Answer: Wang Fei is 48 years old.[0m
    
    [1m> Finished chain.[0m
    Total Tokens: 724
    Prompt Tokens: 653
    Completion Tokens: 71
    Total Cost (USD): $0.0011215


#### åºåˆ—åŒ–é…ç½®LLM

Langchainä¹Ÿæä¾›ä¸€ç§èƒ½åŠ›ç”¨æ¥ä¿å­˜LLMåœ¨è®­ç»ƒæ—¶ä½¿ç”¨çš„å„ç±»ç³»æ•°ï¼Œæ¯”å¦‚templateã€ model_nameç­‰ã€‚

è¿™ç±»ç³»æ•°é€šå¸¸ä¼šè¢«ä¿å­˜åœ¨jsonæˆ–è€…yamlæ–‡ä»¶ä¸­ï¼Œä»¥jsonæ–‡ä»¶ä¸ºä¾‹ï¼Œé…ç½®å¦‚ä¸‹ç³»æ•°ï¼Œç„¶ååˆ©ç”¨load_llmæ–¹æ³•å³å¯å¯¼å…¥ï¼š


```python
'''
llm.jsonå†…å®¹ï¼š
{
  "model_name": "gpt-turbo-3.5",
  "temperature": 0.7,
  "_type": "openai"
}
'''
from langchain.llms.loading import load_llm

llm = load_llm("llm.json")
```

### æµå¼å¤„ç†LLMçš„å“åº”ï¼š

æµå¼å¤„ç†æ„å‘³ç€**åœ¨æ¥æ”¶åˆ°ç¬¬ä¸€ä¸ªæ•°æ®å—åå°±ç«‹å³å¼€å§‹å¤„ç†ï¼Œè€Œä¸éœ€è¦ç­‰å¾…æ•´ä¸ªæ•°æ®åŒ…ä¼ è¾“å®Œæ¯•**ã€‚

è¿™ç§æ¦‚å¿µåº”ç”¨åœ¨LLMä¸­åˆ™å¯è¾¾åˆ°**ç”Ÿæˆå“åº”æ—¶å°±ç«‹åˆ»å‘ç”¨æˆ·å±•ç¤ºæ­¤ä¸‹çš„å“åº”ï¼Œæˆ–è€…åœ¨ç”Ÿæˆå“åº”æ—¶å¤„ç†å“åº”**ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬ç°åœ¨çœ‹åˆ°çš„**å’Œaiå¯¹è¯æ—¶é€å­—è¾“å‡ºçš„æ•ˆæœ**

å¯ä»¥çœ‹åˆ°å®ç°è¿˜æ˜¯è¾ƒä¸ºæ–¹ä¾¿çš„åªéœ€è¦ç›´æ¥è°ƒç”¨StreamingStdOutCallbackHandlerä½œä¸ºcallbackå³å¯ã€‚


```python
from langchain.llms import OpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)
resp = llm("Write me a song about sparkling water.")
```

    
    
    Verse 1:
    Bubbles dancing in my glass
    Clear and crisp, it's such a blast
    Refreshing taste, it's like a dream
    Sparkling water, you make me beam
    
    Chorus:
    Oh sparkling water, you're my delight
    With every sip, you make me feel so right
    You're like a party in my mouth
    I can't get enough, I'm hooked no doubt
    
    Verse 2:
    No sugar, no calories, just pure bliss
    You're the perfect drink, I must confess
    From lemon to lime, so many flavors to choose
    Sparkling water, you never fail to amuse
    
    Chorus:
    Oh sparkling water, you're my delight
    With every sip, you make me feel so right
    You're like a party in my mouth
    I can't get enough, I'm hooked no doubt
    
    Bridge:
    Some may say you're just plain water
    But to me, you're so much more
    You bring a sparkle to my day
    In every single way
    
    Chorus:
    Oh sparkling water, you're my delight
    With every sip, you make me feel so right
    You're like a party in my mouth
    I can't get enough, I'm hooked no doubt
    
    Outro:
    So

### Output Parser

Modelè¿”å›çš„å†…å®¹é€šå¸¸éƒ½æ˜¯å­—ç¬¦ä¸²çš„æ¨¡å¼ï¼Œä½†åœ¨å®é™…å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¾€å¾€å¸Œæœ›modelå¯ä»¥è¿”å›æ›´ç›´è§‚çš„å†…å®¹ï¼ŒLangchainæä¾›çš„è¾“å‡ºè§£æå™¨åˆ™å°†æ´¾ä¸Šç”¨åœºã€‚

åœ¨å®ç°ä¸€ä¸ªè¾“å‡ºè§£æå™¨çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å®ç°ä¸¤ç§æ–¹æ³•ï¼š
- è·å–æ ¼å¼æŒ‡ä»¤ï¼šè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²çš„æ–¹æ³•ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³å¦‚ä½•æ ¼å¼åŒ–è¯­è¨€æ¨¡å‹è¾“å‡ºçš„è¯´æ˜ã€‚
- Parseï¼šä¸€ç§æ¥æ”¶å­—ç¬¦ä¸²ï¼ˆå‡è®¾æ˜¯æ¥è‡ªè¯­è¨€æ¨¡å‹çš„å“åº”ï¼‰å¹¶å°†å…¶è§£æä¸ºæŸç§ç»“æ„çš„æ–¹æ³•ã€‚


```python
# ===== åˆ—è¡¨è§£æå™¨ç¤ºä¾‹ï¼šè®© LLM è¿”å›é€—å·åˆ†éš”çš„åˆ—è¡¨ =====
# éœ€æ±‚ï¼šè®©æ¨¡å‹è¾“å‡º 5 ç§å†°æ·‡æ·‹å£å‘³ï¼Œå¹¶ç›´æ¥è§£ææˆ Python åˆ—è¡¨

# 1) å¯¼å…¥å·¥å…· --------------------
from langchain.output_parsers import CommaSeparatedListOutputParser   # ç°æˆçš„â€œé€—å·åˆ—è¡¨â€è§£æå™¨
from langchain.prompts import PromptTemplate                         # ç”¨äºæ„é€ æç¤ºè¯
from langchain.llms import OpenAI                                    # OpenAI æ–‡æœ¬æ¨¡å‹

# 2) åˆå§‹åŒ–è§£æå™¨ -----------------
output_parser = CommaSeparatedListOutputParser()                     # å®ä¾‹åŒ–
format_instructions = output_parser.get_format_instructions()        # ç”Ÿæˆâ€œè¯·ç”¨é€—å·åˆ†éš”â€è¯´æ˜

# 3) æ„é€  Prompt -----------------
prompt = PromptTemplate(
    template="List five {subject}.\n{format_instructions}",          # æŒ‡ä»¤ + æ ¼å¼è¦æ±‚
    input_variables=["subject"],                                     # è¿è¡Œæ—¶éœ€æä¾› subject
    partial_variables={"format_instructions": format_instructions},  # é¢„å¡«æ ¼å¼æŒ‡ä»¤
)

# 4) è°ƒç”¨ LLM --------------------
llm = OpenAI(temperature=0)                                          # æ¸©åº¦ 0ï¼šè¾“å‡ºæ›´ç¡®å®š
_input  = prompt.format(subject="å†°æ·‡æ·‹å£å‘³")                        # ç”Ÿæˆæœ€ç»ˆ prompt
raw_out = llm(_input)                                                # è·å¾—åŸå§‹å­—ç¬¦ä¸²è¾“å‡º

# 5) è§£æä¸º Python åˆ—è¡¨ -----------
flavors = output_parser.parse(raw_out)
print(flavors)
```

    ['å·§å…‹åŠ›', 'é¦™è‰', 'è‰è“', 'æŠ¹èŒ¶', 'èŠ’æœ']



```python
# ========= æ—¥æœŸè§£æå™¨ï¼šDatetimeOutputParser =========
# ä½œç”¨ï¼šè®©æ¨¡å‹è¾“å‡ºçš„æ—¥æœŸ/æ—¶é—´ç›´æ¥å˜æˆ datetime å¯¹è±¡ï¼Œå…å»æ‰‹åŠ¨è§£æ

from langchain.prompts import PromptTemplate
from langchain.output_parsers import DatetimeOutputParser
from langchain.chains import LLMChain
from langchain.llms import OpenAI

# 1) å®ä¾‹åŒ–è§£æå™¨
date_parser = DatetimeOutputParser()                       

# 2) å‡†å¤‡ Promptï¼Œå¹¶æ³¨å…¥â€œæ ¼å¼æŒ‡ä»¤â€ï¼ˆè¦æ±‚æ¨¡å‹è¾“å‡ºå¯è§£æçš„æ—¥æœŸæ ¼å¼ï¼‰
template = """å›ç­”ç”¨æˆ·çš„é—®é¢˜:
{question}
{format_instructions}"""                                  # {format_instructions} ä¼šè¢«æ›¿æ¢

prompt = PromptTemplate.from_template(
    template,
    partial_variables={
        "format_instructions": date_parser.get_format_instructions()
    },
)

# 3) ç»„è£…ä¸ºé“¾
chain = LLMChain(prompt=prompt, llm=OpenAI())

# 4) è¿è¡Œå¹¶è§£æ
raw = chain.run("bitcoinæ˜¯ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿç”¨è‹±æ–‡æ ¼å¼è¾“å‡ºæ—¶é—´")   # æ¨¡å‹è¿”å›æ—¥æœŸå­—ç¬¦ä¸²
dt  = date_parser.parse(raw)                               # è½¬æˆ datetime.datetime
print(dt.date())                                           # 2009-01-03 ä¹‹ç±»
```

    2009-01-03



```python
# ========= æšä¸¾è§£æå™¨ï¼šEnumOutputParser =========
# ä½œç”¨ï¼šå¼ºåˆ¶æ¨¡å‹è¾“å‡ºæšä¸¾æˆå‘˜ä¹‹ä¸€ï¼Œè§£æåå¾—åˆ° Enum å¯¹è±¡

from langchain.output_parsers.enum import EnumOutputParser
from enum import Enum

class Colors(Enum):
    RED   = "red"
    GREEN = "green"
    BLUE  = "blue"

enum_parser = EnumOutputParser(enum=Colors)                # è‹¥è¾“å‡ºé red/green/blue å°†æŠ¥é”™
```


```python
# ========= PydanticOutputParser ç¤ºä¾‹ =========
# ç›®æ ‡ï¼šæŠŠ LLM è¿”å›çš„ JSON å­—ç¬¦ä¸²è§£ææˆ Actor æ•°æ®ç±»
# åŒæ—¶æ¼”ç¤ºå½“æ ¼å¼é”™è¯¯æ—¶ï¼ŒPydantic è§£æå™¨ä¼šæŠ›å‡ºå¼‚å¸¸

from langchain_core.prompt_values import StringPromptValue
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from langchain.output_parsers import PydanticOutputParser, RetryWithErrorOutputParser
from pydantic import BaseModel, Field
from typing import List

# 1) å®šä¹‰æ•°æ®ç»“æ„ -----------------------------------
class Actor(BaseModel):
    """æè¿°æ¼”å‘˜åŠå…¶ä½œå“åˆ—è¡¨çš„ Pydantic æ¨¡å‹"""
    name: str = Field(description="name of an actor")
    film_names: List[str] = Field(description="films the actor starred in")

# 2) ç”¨ Actor åˆå§‹åŒ–è§£æå™¨ ----------------------------
parser = PydanticOutputParser(pydantic_object=Actor)

# 3) æ¨¡æ‹Ÿä¸€ä¸ªæ ¼å¼é”™è¯¯çš„å“åº”ï¼ˆå•å¼•å· âœ JSON éœ€åŒå¼•å·ï¼‰----
misformatted = "{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}"
formatted = "{\"name\": \"Tom Hanks\", \"film_names\": [\"Forrest Gump\"]}"

# 4) å°è¯•è§£æï¼›è‹¥æ ¼å¼ä¸åˆæ³•å°†æŠ›å‡º ValidationError -------
try:
    actor_obj = parser.parse(misformatted)
    print(actor_obj)
except Exception as e:
    print("è§£æå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š", e)
    # å®é™…é¡¹ç›®ä¸­ï¼Œå¯åœ¨æ­¤æ•è·åç”¨ RetryWithErrorOutputParser è¿›è¡Œè‡ªåŠ¨ä¿®å¤

try:
    actor_obj = parser.parse(formatted)
    print(actor_obj)
except Exception as e:
    print("è§£æå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š", e)
    # å®é™…é¡¹ç›®ä¸­ï¼Œå¯åœ¨æ­¤æ•è·åç”¨ RetryWithErrorOutputParser è¿›è¡Œè‡ªåŠ¨ä¿®å¤
```

    è§£æå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š Invalid json output: {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}
    For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE 
    name='Tom Hanks' film_names=['Forrest Gump']



```python

```
